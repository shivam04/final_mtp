{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "import time\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Alk. Phosphate</th>\n",
       "      <th>ALT</th>\n",
       "      <th>AST</th>\n",
       "      <th>Total Bili</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>...</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>Arterial BP [Systolic]</th>\n",
       "      <th>Temperature C</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>TroponinT</th>\n",
       "      <th>Urine</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Previous WeightF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 00:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103116</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.445489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 03:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 04:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.379699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 05:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 06:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 07:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.349624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 08:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.158416</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.349624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 09:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 10:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 11:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 13:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 14:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 15:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 16:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 17:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.301980</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 18:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.402256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 19:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.411654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 20:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 21:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 22:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.336634</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 23:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17</td>\n",
       "      <td>2 days 00:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 02:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 01:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 12:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.383459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 23:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17</td>\n",
       "      <td>1 days 00:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.227723</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.409774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 02:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.128713</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 03:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.379699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 04:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 05:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 06:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 07:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 08:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 09:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.402256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 10:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.402256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 11:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 01:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.445489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 22:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.372180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 13:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 14:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 15:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.217822</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 16:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 17:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 18:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 19:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.383459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 20:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.334586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 21:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>17</td>\n",
       "      <td>0 days 12:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188427</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>21</td>\n",
       "      <td>1 days 09:00:00.000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>0.026735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167656</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.970297</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035539</td>\n",
       "      <td>0.228866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SUBJECT_ID                  TimeStamp  Albumin  Alk. Phosphate       ALT  \\\n",
       "0           17  0 days 00:00:00.000000000      0.0        0.000000  0.000000   \n",
       "1           17  1 days 03:00:00.000000000      0.0        0.000000  0.000000   \n",
       "2           17  1 days 04:00:00.000000000      0.0        0.000000  0.000000   \n",
       "3           17  1 days 05:00:00.000000000      0.0        0.000000  0.000000   \n",
       "4           17  1 days 06:00:00.000000000      0.0        0.000000  0.000000   \n",
       "5           17  1 days 07:00:00.000000000      0.0        0.000000  0.000000   \n",
       "6           17  1 days 08:00:00.000000000      0.0        0.000000  0.000000   \n",
       "7           17  1 days 09:00:00.000000000      0.0        0.000000  0.000000   \n",
       "8           17  1 days 10:00:00.000000000      0.0        0.000000  0.000000   \n",
       "9           17  1 days 11:00:00.000000000      0.0        0.000000  0.000000   \n",
       "10          17  1 days 13:00:00.000000000      0.0        0.000000  0.000000   \n",
       "11          17  1 days 14:00:00.000000000      0.0        0.000000  0.000000   \n",
       "12          17  1 days 15:00:00.000000000      0.0        0.000000  0.000000   \n",
       "13          17  1 days 16:00:00.000000000      0.0        0.000000  0.000000   \n",
       "14          17  1 days 17:00:00.000000000      0.0        0.000000  0.000000   \n",
       "15          17  1 days 18:00:00.000000000      0.0        0.000000  0.000000   \n",
       "16          17  1 days 19:00:00.000000000      0.0        0.000000  0.000000   \n",
       "17          17  1 days 20:00:00.000000000      0.0        0.000000  0.000000   \n",
       "18          17  1 days 21:00:00.000000000      0.0        0.000000  0.000000   \n",
       "19          17  1 days 22:00:00.000000000      0.0        0.000000  0.000000   \n",
       "20          17  1 days 23:00:00.000000000      0.0        0.000000  0.000000   \n",
       "21          17  2 days 00:00:00.000000000      0.0        0.000000  0.000000   \n",
       "22          17  1 days 02:00:00.000000000      0.0        0.000000  0.000000   \n",
       "23          17  1 days 01:00:00.000000000      0.0        0.000000  0.000000   \n",
       "24          17  1 days 12:00:00.000000000      0.0        0.000000  0.000000   \n",
       "25          17  0 days 23:00:00.000000000      0.0        0.000000  0.000000   \n",
       "26          17  1 days 00:00:00.000000000      0.0        0.000000  0.000000   \n",
       "27          17  0 days 02:00:00.000000000      0.0        0.000000  0.000000   \n",
       "28          17  0 days 03:00:00.000000000      0.0        0.000000  0.000000   \n",
       "29          17  0 days 04:00:00.000000000      0.0        0.000000  0.000000   \n",
       "30          17  0 days 05:00:00.000000000      0.0        0.000000  0.000000   \n",
       "31          17  0 days 06:00:00.000000000      0.0        0.000000  0.000000   \n",
       "32          17  0 days 07:00:00.000000000      0.0        0.000000  0.000000   \n",
       "33          17  0 days 08:00:00.000000000      0.0        0.000000  0.000000   \n",
       "34          17  0 days 09:00:00.000000000      0.0        0.000000  0.000000   \n",
       "35          17  0 days 10:00:00.000000000      0.0        0.000000  0.000000   \n",
       "36          17  0 days 11:00:00.000000000      0.0        0.000000  0.000000   \n",
       "37          17  0 days 01:00:00.000000000      0.0        0.000000  0.000000   \n",
       "38          17  0 days 22:00:00.000000000      0.0        0.000000  0.000000   \n",
       "39          17  0 days 13:00:00.000000000      0.0        0.000000  0.000000   \n",
       "40          17  0 days 14:00:00.000000000      0.0        0.000000  0.000000   \n",
       "41          17  0 days 15:00:00.000000000      0.0        0.000000  0.000000   \n",
       "42          17  0 days 16:00:00.000000000      0.0        0.000000  0.000000   \n",
       "43          17  0 days 17:00:00.000000000      0.0        0.000000  0.000000   \n",
       "44          17  0 days 18:00:00.000000000      0.0        0.000000  0.000000   \n",
       "45          17  0 days 19:00:00.000000000      0.0        0.000000  0.000000   \n",
       "46          17  0 days 20:00:00.000000000      0.0        0.000000  0.000000   \n",
       "47          17  0 days 21:00:00.000000000      0.0        0.000000  0.000000   \n",
       "48          17  0 days 12:00:00.000000000      0.0        0.000000  0.000000   \n",
       "49          21  1 days 09:00:00.000000000      0.0        0.017775  0.012671   \n",
       "\n",
       "         AST  Total Bili       BUN  Cholesterol  Creatinine        ...         \\\n",
       "0   0.000000         0.0  0.060606          0.0    0.009424        ...          \n",
       "1   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "2   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "3   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "4   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "5   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "6   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "7   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "8   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "9   0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "10  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "11  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "12  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "13  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "14  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "15  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "16  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "17  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "18  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "19  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "20  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "21  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "22  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "23  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "24  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "25  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "26  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "27  0.000000         0.0  0.060606          0.0    0.009424        ...          \n",
       "28  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "29  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "30  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "31  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "32  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "33  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "34  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "35  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "36  0.000000         0.0  0.055556          0.0    0.008901        ...          \n",
       "37  0.000000         0.0  0.060606          0.0    0.009424        ...          \n",
       "38  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "39  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "40  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "41  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "42  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "43  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "44  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "45  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "46  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "47  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "48  0.000000         0.0  0.060606          0.0    0.008377        ...          \n",
       "49  0.026735         0.0  0.303030          0.0    0.019895        ...          \n",
       "\n",
       "    Platelets  Respiratory Rate      SaO2  Arterial BP [Systolic]  \\\n",
       "0    0.103116          0.143564  0.990099                0.445489   \n",
       "1    0.188427          0.267327  0.940594                0.406015   \n",
       "2    0.188427          0.207921  0.940594                0.379699   \n",
       "3    0.188427          0.217822  0.980198                0.413534   \n",
       "4    0.188427          0.178218  0.980198                0.406015   \n",
       "5    0.188427          0.158416  0.980198                0.349624   \n",
       "6    0.188427          0.158416  0.980198                0.349624   \n",
       "7    0.188427          0.178218  0.980198                0.357143   \n",
       "8    0.188427          0.178218  0.980198                0.357143   \n",
       "9    0.188427          0.188119  0.980198                0.375940   \n",
       "10   0.188427          0.198020  0.980198                0.406015   \n",
       "11   0.188427          0.217822  0.980198                0.406015   \n",
       "12   0.188427          0.301980  0.980198                0.428571   \n",
       "13   0.188427          0.301980  0.980198                0.368421   \n",
       "14   0.188427          0.301980  0.980198                0.368421   \n",
       "15   0.188427          0.356436  0.980198                0.402256   \n",
       "16   0.188427          0.306931  0.980198                0.411654   \n",
       "17   0.188427          0.306931  0.980198                0.406015   \n",
       "18   0.188427          0.297030  0.980198                0.421053   \n",
       "19   0.188427          0.336634  0.980198                0.421053   \n",
       "20   0.188427          0.316832  0.980198                0.421053   \n",
       "21   0.188427          0.316832  0.980198                0.421053   \n",
       "22   0.188427          0.267327  0.940594                0.406015   \n",
       "23   0.188427          0.306931  0.940594                0.375940   \n",
       "24   0.188427          0.247525  0.980198                0.383459   \n",
       "25   0.188427          0.247525  0.940594                0.387218   \n",
       "26   0.188427          0.227723  0.940594                0.409774   \n",
       "27   0.177300          0.128713  0.990099                0.458647   \n",
       "28   0.177300          0.188119  0.990099                0.379699   \n",
       "29   0.177300          0.237624  0.990099                0.439850   \n",
       "30   0.177300          0.217822  0.940594                0.439850   \n",
       "31   0.177300          0.217822  0.940594                0.439850   \n",
       "32   0.177300          0.168317  0.940594                0.428571   \n",
       "33   0.177300          0.168317  0.940594                0.375940   \n",
       "34   0.177300          0.198020  0.940594                0.402256   \n",
       "35   0.177300          0.198020  0.940594                0.402256   \n",
       "36   0.188427          0.237624  0.940594                0.368421   \n",
       "37   0.177300          0.143564  0.990099                0.445489   \n",
       "38   0.188427          0.287129  0.940594                0.372180   \n",
       "39   0.188427          0.277228  0.940594                0.390977   \n",
       "40   0.188427          0.277228  0.940594                0.390977   \n",
       "41   0.188427          0.217822  0.940594                0.421053   \n",
       "42   0.188427          0.198020  0.940594                0.327068   \n",
       "43   0.188427          0.262376  0.940594                0.300752   \n",
       "44   0.188427          0.247525  0.940594                0.300752   \n",
       "45   0.188427          0.237624  0.940594                0.383459   \n",
       "46   0.188427          0.316832  0.940594                0.334586   \n",
       "47   0.188427          0.267327  0.940594                0.357143   \n",
       "48   0.188427          0.237624  0.940594                0.368421   \n",
       "49   0.167656          0.168317  0.970297                0.447368   \n",
       "\n",
       "    Temperature C  TroponinI  TroponinT  Urine       WBC  Previous WeightF  \n",
       "0             0.0        0.0        0.0    0.0  0.021739          0.000000  \n",
       "1             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "2             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "3             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "4             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "5             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "6             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "7             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "8             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "9             0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "10            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "11            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "12            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "13            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "14            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "15            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "16            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "17            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "18            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "19            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "20            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "21            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "22            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "23            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "24            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "25            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "26            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "27            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "28            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "29            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "30            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "31            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "32            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "33            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "34            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "35            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "36            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "37            0.0        0.0        0.0    0.0  0.047259          0.000000  \n",
       "38            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "39            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "40            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "41            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "42            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "43            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "44            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "45            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "46            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "47            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "48            0.0        0.0        0.0    0.0  0.040076          0.000000  \n",
       "49            0.0        0.0        0.0    0.0  0.035539          0.228866  \n",
       "\n",
       "[50 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../finalset/patient_data_48_hours.csv')\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = df.groupby('SUBJECT_ID')\n",
    "p = []\n",
    "for s,g in ff:\n",
    "    p.append((s,len(g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "21\n",
      "23\n",
      "34\n",
      "36\n",
      "61\n",
      "68\n",
      "85\n",
      "94\n",
      "103\n",
      "105\n",
      "107\n",
      "109\n",
      "111\n",
      "112\n",
      "117\n",
      "124\n",
      "130\n",
      "138\n",
      "145\n",
      "154\n",
      "156\n",
      "157\n",
      "175\n",
      "177\n",
      "184\n",
      "188\n",
      "191\n",
      "199\n",
      "203\n",
      "209\n",
      "211\n",
      "222\n",
      "231\n",
      "234\n",
      "236\n",
      "249\n",
      "252\n",
      "256\n",
      "283\n",
      "287\n",
      "291\n",
      "303\n",
      "305\n",
      "307\n",
      "321\n",
      "323\n",
      "328\n",
      "339\n",
      "346\n",
      "353\n",
      "356\n",
      "357\n",
      "361\n",
      "362\n",
      "368\n",
      "376\n",
      "384\n",
      "394\n",
      "395\n",
      "402\n",
      "406\n",
      "417\n",
      "419\n",
      "423\n",
      "426\n",
      "433\n",
      "434\n",
      "450\n",
      "452\n",
      "474\n",
      "494\n",
      "502\n",
      "505\n",
      "507\n",
      "518\n",
      "523\n",
      "530\n",
      "546\n",
      "554\n",
      "580\n",
      "605\n",
      "618\n",
      "629\n",
      "631\n",
      "634\n",
      "638\n",
      "653\n",
      "665\n",
      "674\n",
      "679\n",
      "689\n",
      "690\n",
      "695\n",
      "698\n",
      "703\n",
      "711\n",
      "720\n",
      "721\n",
      "725\n",
      "728\n",
      "731\n",
      "735\n",
      "745\n",
      "747\n",
      "757\n",
      "771\n",
      "773\n",
      "776\n",
      "781\n",
      "782\n",
      "787\n",
      "794\n",
      "796\n",
      "801\n",
      "804\n",
      "805\n",
      "808\n",
      "820\n",
      "822\n",
      "824\n",
      "835\n",
      "843\n",
      "849\n",
      "852\n",
      "854\n",
      "858\n",
      "875\n",
      "878\n",
      "885\n",
      "890\n",
      "894\n",
      "899\n",
      "902\n",
      "904\n",
      "907\n",
      "914\n",
      "923\n",
      "924\n",
      "937\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "952\n",
      "959\n",
      "963\n",
      "969\n",
      "975\n",
      "984\n",
      "995\n",
      "998\n",
      "1004\n",
      "1006\n",
      "1018\n",
      "1029\n",
      "1042\n",
      "1044\n",
      "1050\n",
      "1062\n",
      "1076\n",
      "1080\n",
      "1084\n",
      "1086\n",
      "1093\n",
      "1104\n",
      "1106\n",
      "1121\n",
      "1124\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1141\n",
      "1148\n",
      "1158\n",
      "1163\n",
      "1180\n",
      "1183\n",
      "1186\n",
      "1197\n",
      "1203\n",
      "1207\n",
      "1226\n",
      "1241\n",
      "1257\n",
      "1271\n",
      "1276\n",
      "1283\n",
      "1286\n",
      "1292\n",
      "1301\n",
      "1313\n",
      "1319\n",
      "1331\n",
      "1332\n",
      "1339\n",
      "1344\n",
      "1347\n",
      "1351\n",
      "1354\n",
      "1357\n",
      "1386\n",
      "1397\n",
      "1401\n",
      "1419\n",
      "1423\n",
      "1441\n",
      "1442\n",
      "1453\n",
      "1459\n",
      "1474\n",
      "1478\n",
      "1485\n",
      "1488\n",
      "1493\n",
      "1498\n",
      "1516\n",
      "1517\n",
      "1528\n",
      "1530\n",
      "1535\n",
      "1544\n",
      "1550\n",
      "1552\n",
      "1563\n",
      "1569\n",
      "1571\n",
      "1578\n",
      "1590\n",
      "1594\n",
      "1604\n",
      "1606\n",
      "1610\n",
      "1611\n",
      "1616\n",
      "1620\n",
      "1621\n",
      "1636\n",
      "1673\n",
      "1679\n",
      "1699\n",
      "1709\n",
      "1727\n",
      "1757\n",
      "1764\n",
      "1767\n",
      "1778\n",
      "1784\n",
      "1787\n",
      "1802\n",
      "1804\n",
      "1816\n",
      "1819\n",
      "1840\n",
      "1855\n",
      "1860\n",
      "1872\n",
      "1893\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1915\n",
      "1931\n",
      "1935\n",
      "1936\n",
      "1944\n",
      "1949\n",
      "1967\n",
      "1972\n",
      "1973\n",
      "1982\n",
      "1985\n",
      "1986\n",
      "1988\n",
      "1991\n",
      "1999\n",
      "2005\n",
      "2015\n",
      "2018\n",
      "2021\n",
      "2025\n",
      "2029\n",
      "2034\n",
      "2040\n",
      "2045\n",
      "2049\n",
      "2078\n",
      "2081\n",
      "2090\n",
      "2092\n",
      "2093\n",
      "2100\n",
      "2113\n",
      "2116\n",
      "2124\n",
      "2135\n",
      "2136\n",
      "2150\n",
      "2185\n",
      "2187\n",
      "2208\n",
      "2215\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2240\n",
      "2245\n",
      "2258\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2268\n",
      "2269\n",
      "2280\n",
      "2284\n",
      "2298\n",
      "2301\n",
      "2303\n",
      "2310\n",
      "2322\n",
      "2333\n",
      "2338\n",
      "2341\n",
      "2343\n",
      "2349\n",
      "2361\n",
      "2365\n",
      "2369\n",
      "2371\n",
      "2378\n",
      "2387\n",
      "2394\n",
      "2397\n",
      "2413\n",
      "2415\n",
      "2420\n",
      "2423\n",
      "2425\n",
      "2427\n",
      "2442\n",
      "2452\n",
      "2460\n",
      "2464\n",
      "2467\n",
      "2473\n",
      "2479\n",
      "2482\n",
      "2486\n",
      "2512\n",
      "2513\n",
      "2530\n",
      "2539\n",
      "2550\n",
      "2558\n",
      "2561\n",
      "2577\n",
      "2582\n",
      "2586\n",
      "2589\n",
      "2590\n",
      "2592\n",
      "2598\n",
      "2611\n",
      "2619\n",
      "2622\n",
      "2625\n",
      "2636\n",
      "2639\n",
      "2649\n",
      "2652\n",
      "2658\n",
      "2664\n",
      "2667\n",
      "2682\n",
      "2710\n",
      "2712\n",
      "2718\n",
      "2733\n",
      "2743\n",
      "2747\n",
      "2759\n",
      "2763\n",
      "2775\n",
      "2784\n",
      "2786\n",
      "2791\n",
      "2809\n",
      "2813\n",
      "2825\n",
      "2828\n",
      "2830\n",
      "2846\n",
      "2867\n",
      "2876\n",
      "2885\n",
      "2905\n",
      "2919\n",
      "2921\n",
      "2953\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2984\n",
      "2990\n",
      "2997\n",
      "3011\n",
      "3019\n",
      "3024\n",
      "3026\n",
      "3036\n",
      "3042\n",
      "3045\n",
      "3052\n",
      "3065\n",
      "3070\n",
      "3078\n",
      "3084\n",
      "3100\n",
      "3123\n",
      "3127\n",
      "3132\n",
      "3133\n",
      "3143\n",
      "3145\n",
      "3154\n",
      "3155\n",
      "3178\n",
      "3180\n",
      "3181\n",
      "3184\n",
      "3191\n",
      "3210\n",
      "3214\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3222\n",
      "3225\n",
      "3234\n",
      "3242\n",
      "3250\n",
      "3267\n",
      "3272\n",
      "3278\n",
      "3280\n",
      "3286\n",
      "3306\n",
      "3318\n",
      "3322\n",
      "3333\n",
      "3347\n",
      "3362\n",
      "3365\n",
      "3369\n",
      "3377\n",
      "3386\n",
      "3392\n",
      "3393\n",
      "3403\n",
      "3404\n",
      "3417\n",
      "3425\n",
      "3426\n",
      "3433\n",
      "3436\n",
      "3437\n",
      "3460\n",
      "3466\n",
      "3481\n",
      "3482\n",
      "3490\n",
      "3491\n",
      "3497\n",
      "3506\n",
      "3513\n",
      "3516\n",
      "3523\n",
      "3533\n",
      "3543\n",
      "3566\n",
      "3575\n",
      "3588\n",
      "3593\n",
      "3598\n",
      "3600\n",
      "3602\n",
      "3623\n",
      "3632\n",
      "3633\n",
      "3639\n",
      "3644\n",
      "3649\n",
      "3674\n",
      "3676\n",
      "3678\n",
      "3685\n",
      "3700\n",
      "3703\n",
      "3705\n",
      "3722\n",
      "3728\n",
      "3734\n",
      "3742\n",
      "3748\n",
      "3765\n",
      "3768\n",
      "3771\n",
      "3788\n",
      "3790\n",
      "3792\n",
      "3793\n",
      "3796\n",
      "3800\n",
      "3830\n",
      "3851\n",
      "3860\n",
      "3862\n",
      "3866\n",
      "3868\n",
      "3874\n",
      "3883\n",
      "3895\n",
      "3903\n",
      "3911\n",
      "3914\n",
      "3917\n",
      "3929\n",
      "3935\n",
      "3939\n",
      "3940\n",
      "3949\n",
      "3952\n",
      "3956\n",
      "3969\n",
      "3974\n",
      "3977\n",
      "3980\n",
      "3987\n",
      "3995\n",
      "3999\n",
      "4002\n",
      "4004\n",
      "4016\n",
      "4023\n",
      "4026\n",
      "4030\n",
      "4058\n",
      "4059\n",
      "4064\n",
      "4074\n",
      "4076\n",
      "4084\n",
      "4094\n",
      "4096\n",
      "4103\n",
      "4104\n",
      "4109\n",
      "4113\n",
      "4115\n",
      "4126\n",
      "4137\n",
      "4155\n",
      "4159\n",
      "4166\n",
      "4180\n",
      "4187\n",
      "4211\n",
      "4239\n",
      "4240\n",
      "4267\n",
      "4268\n",
      "4271\n",
      "4290\n",
      "4292\n",
      "4308\n",
      "4313\n",
      "4316\n",
      "4317\n",
      "4328\n",
      "4329\n",
      "4334\n",
      "4346\n",
      "4351\n",
      "4367\n",
      "4390\n",
      "4392\n",
      "4394\n",
      "4397\n",
      "4401\n",
      "4405\n",
      "4406\n",
      "4410\n",
      "4423\n",
      "4448\n",
      "4454\n",
      "4456\n",
      "4458\n",
      "4467\n",
      "4506\n",
      "4520\n",
      "4521\n",
      "4527\n",
      "4535\n",
      "4555\n",
      "4566\n",
      "4571\n",
      "4574\n",
      "4577\n",
      "4587\n",
      "4588\n",
      "4599\n",
      "4602\n",
      "4609\n",
      "4618\n",
      "4635\n",
      "4641\n",
      "4644\n",
      "4655\n",
      "4664\n",
      "4675\n",
      "4676\n",
      "4679\n",
      "4685\n",
      "4690\n",
      "4696\n",
      "4700\n",
      "4708\n",
      "4713\n",
      "4714\n",
      "4718\n",
      "4730\n",
      "4741\n",
      "4749\n",
      "4765\n",
      "4766\n",
      "4770\n",
      "4784\n",
      "4785\n",
      "4787\n",
      "4788\n",
      "4816\n",
      "4826\n",
      "4827\n",
      "4831\n",
      "4833\n",
      "4843\n",
      "4871\n",
      "4873\n",
      "4883\n",
      "4893\n",
      "4900\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4910\n",
      "4915\n",
      "4916\n",
      "4924\n",
      "4929\n",
      "4949\n",
      "4951\n",
      "4954\n",
      "4958\n",
      "4962\n",
      "4966\n",
      "4968\n",
      "4985\n",
      "5018\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5048\n",
      "5050\n",
      "5058\n",
      "5060\n",
      "5071\n",
      "5072\n",
      "5074\n",
      "5077\n",
      "5078\n",
      "5096\n",
      "5099\n",
      "5107\n",
      "5124\n",
      "5138\n",
      "5139\n",
      "5142\n",
      "5145\n",
      "5150\n",
      "5161\n",
      "5163\n",
      "5170\n",
      "5171\n",
      "5183\n",
      "5193\n",
      "5195\n",
      "5196\n",
      "5199\n",
      "5205\n",
      "5215\n",
      "5231\n",
      "5239\n",
      "5242\n",
      "5244\n",
      "5247\n",
      "5278\n",
      "5282\n",
      "5285\n",
      "5289\n",
      "5290\n",
      "5319\n",
      "5321\n",
      "5328\n",
      "5330\n",
      "5343\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5362\n",
      "5369\n",
      "5370\n",
      "5374\n",
      "5382\n",
      "5384\n",
      "5389\n",
      "5392\n",
      "5393\n",
      "5397\n",
      "5398\n",
      "5400\n",
      "5403\n",
      "5406\n",
      "5448\n",
      "5453\n",
      "5460\n",
      "5476\n",
      "5481\n",
      "5491\n",
      "5494\n",
      "5495\n",
      "5506\n",
      "5517\n",
      "5525\n",
      "5527\n",
      "5528\n",
      "5535\n",
      "5542\n",
      "5544\n",
      "5564\n",
      "5570\n",
      "5573\n",
      "5581\n",
      "5596\n",
      "5598\n",
      "5604\n",
      "5611\n",
      "5619\n",
      "5620\n",
      "5622\n",
      "5643\n",
      "5645\n",
      "5658\n",
      "5662\n",
      "5665\n",
      "5666\n",
      "5675\n",
      "5679\n",
      "5685\n",
      "5689\n",
      "5691\n",
      "5692\n",
      "5696\n",
      "5709\n",
      "5710\n",
      "5712\n",
      "5727\n",
      "5739\n",
      "5752\n",
      "5760\n",
      "5761\n",
      "5766\n",
      "5768\n",
      "5771\n",
      "5772\n",
      "5774\n",
      "5782\n",
      "5783\n",
      "5786\n",
      "5791\n",
      "5794\n",
      "5795\n",
      "5806\n",
      "5816\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5824\n",
      "5830\n",
      "5832\n",
      "5839\n",
      "5841\n",
      "5850\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5874\n",
      "5882\n",
      "5885\n",
      "5897\n",
      "5901\n",
      "5904\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5913\n",
      "5928\n",
      "5935\n",
      "5937\n",
      "5950\n",
      "5954\n",
      "5958\n",
      "5962\n",
      "5978\n",
      "5979\n",
      "6001\n",
      "6005\n",
      "6008\n",
      "6019\n",
      "6020\n",
      "6024\n",
      "6038\n",
      "6061\n",
      "6063\n",
      "6069\n",
      "6079\n",
      "6082\n",
      "6085\n",
      "6086\n",
      "6090\n",
      "6098\n",
      "6116\n",
      "6124\n",
      "6131\n",
      "6145\n",
      "6147\n",
      "6155\n",
      "6156\n",
      "6158\n",
      "6160\n",
      "6171\n",
      "6174\n",
      "6176\n",
      "6179\n",
      "6202\n",
      "6206\n",
      "6212\n",
      "6214\n",
      "6228\n",
      "6234\n",
      "6255\n",
      "6258\n",
      "6262\n",
      "6272\n",
      "6273\n",
      "6279\n",
      "6283\n",
      "6317\n",
      "6321\n",
      "6331\n",
      "6349\n",
      "6353\n",
      "6358\n",
      "6359\n",
      "6365\n",
      "6378\n",
      "6395\n",
      "6398\n",
      "6428\n",
      "6432\n",
      "6437\n",
      "6440\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6451\n",
      "6464\n",
      "6466\n",
      "6470\n",
      "6481\n",
      "6488\n",
      "6497\n",
      "6510\n",
      "6526\n",
      "6533\n",
      "6534\n",
      "6540\n",
      "6541\n",
      "6542\n",
      "6543\n",
      "6545\n",
      "6552\n",
      "6566\n",
      "6571\n",
      "6575\n",
      "6579\n",
      "6598\n",
      "6603\n",
      "6604\n",
      "6621\n",
      "6630\n",
      "6632\n",
      "6638\n",
      "6653\n",
      "6667\n",
      "6675\n",
      "6685\n",
      "6692\n",
      "6697\n",
      "6700\n",
      "6702\n",
      "6706\n",
      "6707\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6718\n",
      "6736\n",
      "6749\n",
      "6756\n",
      "6783\n",
      "6787\n",
      "6805\n",
      "6809\n",
      "6824\n",
      "6828\n",
      "6829\n",
      "6833\n",
      "6838\n",
      "6841\n",
      "6850\n",
      "6863\n",
      "6878\n",
      "6884\n",
      "6891\n",
      "6892\n",
      "6894\n",
      "6901\n",
      "6908\n",
      "6912\n",
      "6914\n",
      "6917\n",
      "6920\n",
      "6930\n",
      "6939\n",
      "6940\n",
      "6950\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6958\n",
      "6960\n",
      "6962\n",
      "6973\n",
      "6976\n",
      "6978\n",
      "6981\n",
      "6983\n",
      "6988\n",
      "7009\n",
      "7013\n",
      "7029\n",
      "7051\n",
      "7059\n",
      "7066\n",
      "7095\n",
      "7101\n",
      "7102\n",
      "7105\n",
      "7107\n",
      "7118\n",
      "7125\n",
      "7138\n",
      "7142\n",
      "7161\n",
      "7169\n",
      "7175\n",
      "7180\n",
      "7181\n",
      "7184\n",
      "7187\n",
      "7188\n",
      "7192\n",
      "7210\n",
      "7211\n",
      "7216\n",
      "7223\n",
      "7224\n",
      "7225\n",
      "7229\n",
      "7230\n",
      "7232\n",
      "7235\n",
      "7241\n",
      "7246\n",
      "7247\n",
      "7251\n",
      "7253\n",
      "7254\n",
      "7262\n",
      "7275\n",
      "7289\n",
      "7299\n",
      "7323\n",
      "7326\n",
      "7327\n",
      "7338\n",
      "7339\n",
      "7354\n",
      "7357\n",
      "7359\n",
      "7363\n",
      "7371\n",
      "7373\n",
      "7389\n",
      "7391\n",
      "7393\n",
      "7402\n",
      "7412\n",
      "7413\n",
      "7427\n",
      "7429\n",
      "7437\n",
      "7445\n",
      "7449\n",
      "7452\n",
      "7475\n",
      "7476\n",
      "7477\n",
      "7478\n",
      "7482\n",
      "7487\n",
      "7492\n",
      "7494\n",
      "7511\n",
      "7513\n",
      "7514\n",
      "7517\n",
      "7522\n",
      "7529\n",
      "7533\n",
      "7539\n",
      "7546\n",
      "7552\n",
      "7554\n",
      "7558\n",
      "7562\n",
      "7574\n",
      "7585\n",
      "7589\n",
      "7607\n",
      "7612\n",
      "7613\n",
      "7614\n",
      "7616\n",
      "7618\n",
      "7621\n",
      "7627\n",
      "7629\n",
      "7632\n",
      "7650\n",
      "7654\n",
      "7666\n",
      "7668\n",
      "7671\n",
      "7676\n",
      "7679\n",
      "7681\n",
      "7686\n",
      "7695\n",
      "7698\n",
      "7702\n",
      "7709\n",
      "7718\n",
      "7726\n",
      "7731\n",
      "7752\n",
      "7755\n",
      "7756\n",
      "7778\n",
      "7784\n",
      "7786\n",
      "7787\n",
      "7798\n",
      "7804\n",
      "7805\n",
      "7809\n",
      "7815\n",
      "7830\n",
      "7853\n",
      "7880\n",
      "7883\n",
      "7884\n",
      "7900\n",
      "7908\n",
      "7930\n",
      "7936\n",
      "7946\n",
      "7955\n",
      "7958\n",
      "7965\n",
      "7968\n",
      "7973\n",
      "7979\n",
      "7985\n",
      "7996\n",
      "8018\n",
      "8037\n",
      "8057\n",
      "8060\n",
      "8068\n",
      "8070\n",
      "8071\n",
      "8073\n",
      "8081\n",
      "8086\n",
      "8103\n",
      "8105\n",
      "8109\n",
      "8110\n",
      "8114\n",
      "8115\n",
      "8120\n",
      "8145\n",
      "8154\n",
      "8156\n",
      "8163\n",
      "8167\n",
      "8188\n",
      "8209\n",
      "8217\n",
      "8222\n",
      "8224\n",
      "8228\n",
      "8231\n",
      "8238\n",
      "8251\n",
      "8258\n",
      "8273\n",
      "8283\n",
      "8296\n",
      "8309\n",
      "8311\n",
      "8314\n",
      "8323\n",
      "8337\n",
      "8344\n",
      "8360\n",
      "8364\n",
      "8370\n",
      "8374\n",
      "8375\n",
      "8389\n",
      "8393\n",
      "8406\n",
      "8411\n",
      "8414\n",
      "8426\n",
      "8427\n",
      "8431\n",
      "8432\n",
      "8445\n",
      "8447\n",
      "8452\n",
      "8463\n",
      "8464\n",
      "8467\n",
      "8471\n",
      "8472\n",
      "8481\n",
      "8489\n",
      "8492\n",
      "8493\n",
      "8498\n",
      "8501\n",
      "8505\n",
      "8512\n",
      "8513\n",
      "8519\n",
      "8532\n",
      "8533\n",
      "8543\n",
      "8546\n",
      "8548\n",
      "8551\n",
      "8556\n",
      "8559\n",
      "8569\n",
      "8581\n",
      "8608\n",
      "8619\n",
      "8654\n",
      "8665\n",
      "8668\n",
      "8670\n",
      "8674\n",
      "8675\n",
      "8678\n",
      "8682\n",
      "8686\n",
      "8695\n",
      "8697\n",
      "8698\n",
      "8714\n",
      "8731\n",
      "8734\n",
      "8735\n",
      "8780\n",
      "8786\n",
      "8789\n",
      "8795\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8803\n",
      "8818\n",
      "8822\n",
      "8845\n",
      "8850\n",
      "8870\n",
      "8876\n",
      "8887\n",
      "8896\n",
      "8900\n",
      "8913\n",
      "8914\n",
      "8917\n",
      "8921\n",
      "8936\n",
      "8947\n",
      "8948\n",
      "8978\n",
      "8984\n",
      "8985\n",
      "8989\n",
      "8990\n",
      "8992\n",
      "8995\n",
      "8996\n",
      "9002\n",
      "9003\n",
      "9013\n",
      "9016\n",
      "9030\n",
      "9036\n",
      "9044\n",
      "9054\n",
      "9056\n",
      "9058\n",
      "9061\n",
      "9064\n",
      "9070\n",
      "9096\n",
      "9101\n",
      "9102\n",
      "9128\n",
      "9132\n",
      "9137\n",
      "9141\n",
      "9144\n",
      "9161\n",
      "9177\n",
      "9186\n",
      "9206\n",
      "9211\n",
      "9216\n",
      "9227\n",
      "9233\n",
      "9248\n",
      "9249\n",
      "9251\n",
      "9253\n",
      "9256\n",
      "9261\n",
      "9263\n",
      "9266\n",
      "9267\n",
      "9271\n",
      "9272\n",
      "9274\n",
      "9278\n",
      "9302\n",
      "9304\n",
      "9307\n",
      "9311\n",
      "9324\n",
      "9330\n",
      "9331\n",
      "9339\n",
      "9341\n",
      "9356\n",
      "9363\n",
      "9364\n",
      "9377\n",
      "9385\n",
      "9393\n",
      "9395\n",
      "9402\n",
      "9403\n",
      "9407\n",
      "9428\n",
      "9434\n",
      "9443\n",
      "9453\n",
      "9454\n",
      "9457\n",
      "9473\n",
      "9480\n",
      "9481\n",
      "9484\n",
      "9486\n",
      "9498\n",
      "9505\n",
      "9518\n",
      "9539\n",
      "9544\n",
      "9545\n",
      "9547\n",
      "9550\n",
      "9555\n",
      "9561\n",
      "9571\n",
      "9575\n",
      "9576\n",
      "9602\n",
      "9622\n",
      "9635\n",
      "9656\n",
      "9667\n",
      "9672\n",
      "9673\n",
      "9676\n",
      "9686\n",
      "9688\n",
      "9702\n",
      "9706\n",
      "9710\n",
      "9714\n",
      "9718\n",
      "9725\n",
      "9727\n",
      "9736\n",
      "9753\n",
      "9763\n",
      "9768\n",
      "9778\n",
      "9782\n",
      "9787\n",
      "9791\n",
      "9805\n",
      "9810\n",
      "9818\n",
      "9828\n",
      "9855\n",
      "9856\n",
      "9863\n",
      "9865\n",
      "9871\n",
      "9872\n",
      "9873\n",
      "9882\n",
      "9887\n",
      "9889\n",
      "9895\n",
      "9896\n",
      "9897\n",
      "9900\n",
      "9902\n",
      "9906\n",
      "9911\n",
      "9920\n",
      "9923\n",
      "9933\n",
      "9954\n",
      "9957\n",
      "9962\n",
      "9965\n",
      "9966\n",
      "9967\n",
      "9968\n",
      "9969\n",
      "9973\n",
      "9976\n",
      "9981\n",
      "9982\n",
      "9984\n",
      "9998\n",
      "10004\n",
      "10059\n",
      "10071\n",
      "10088\n",
      "10094\n",
      "10117\n",
      "10119\n",
      "10124\n",
      "10125\n",
      "10134\n",
      "10144\n",
      "10149\n",
      "10152\n",
      "10160\n",
      "10168\n",
      "10174\n",
      "10188\n",
      "10197\n",
      "10206\n",
      "10207\n",
      "10220\n",
      "10222\n",
      "10224\n",
      "10236\n",
      "10246\n",
      "10247\n",
      "10248\n",
      "10249\n",
      "10254\n",
      "10257\n",
      "10272\n",
      "10277\n",
      "10301\n",
      "10302\n",
      "10305\n",
      "10315\n",
      "10328\n",
      "10369\n",
      "10375\n",
      "10377\n",
      "10386\n",
      "10394\n",
      "10395\n",
      "10414\n",
      "10416\n",
      "10417\n",
      "10422\n",
      "10425\n",
      "10428\n",
      "10434\n",
      "10471\n",
      "10478\n",
      "10487\n",
      "10502\n",
      "10510\n",
      "10515\n",
      "10531\n",
      "10532\n",
      "10569\n",
      "10581\n",
      "10594\n",
      "10595\n",
      "10597\n",
      "10604\n",
      "10612\n",
      "10623\n",
      "10624\n",
      "10634\n",
      "10635\n",
      "10637\n",
      "10642\n",
      "10653\n",
      "10655\n",
      "10668\n",
      "10675\n",
      "10676\n",
      "10677\n",
      "10679\n",
      "10686\n",
      "10687\n",
      "10689\n",
      "10694\n",
      "10699\n",
      "10704\n",
      "10721\n",
      "10725\n",
      "10736\n",
      "10742\n",
      "10751\n",
      "10753\n",
      "10757\n",
      "10774\n",
      "10799\n",
      "10806\n",
      "10809\n",
      "10811\n",
      "10814\n",
      "10820\n",
      "10832\n",
      "10835\n",
      "10852\n",
      "10854\n",
      "10859\n",
      "10861\n",
      "10886\n",
      "10906\n",
      "10916\n",
      "10924\n",
      "10925\n",
      "10928\n",
      "10932\n",
      "10939\n",
      "10947\n",
      "10948\n",
      "10950\n",
      "10954\n",
      "10957\n",
      "10972\n",
      "10973\n",
      "10976\n",
      "10992\n",
      "11003\n",
      "11007\n",
      "11018\n",
      "11019\n",
      "11021\n",
      "11024\n",
      "11032\n",
      "11043\n",
      "11049\n",
      "11050\n",
      "11061\n",
      "11066\n",
      "11076\n",
      "11085\n",
      "11090\n",
      "11098\n",
      "11099\n",
      "11108\n",
      "11109\n",
      "11123\n",
      "11135\n",
      "11138\n",
      "11146\n",
      "11147\n",
      "11162\n",
      "11165\n",
      "11171\n",
      "11195\n",
      "11202\n",
      "11205\n",
      "11212\n",
      "11229\n",
      "11234\n",
      "11235\n",
      "11236\n",
      "11242\n",
      "11245\n",
      "11255\n",
      "11280\n",
      "11285\n",
      "11287\n",
      "11288\n",
      "11295\n",
      "11318\n",
      "11321\n",
      "11323\n",
      "11328\n",
      "11335\n",
      "11339\n",
      "11341\n",
      "11342\n",
      "11343\n",
      "11346\n",
      "11348\n",
      "11362\n",
      "11369\n",
      "11372\n",
      "11382\n",
      "11395\n",
      "11421\n",
      "11427\n",
      "11432\n",
      "11438\n",
      "11442\n",
      "11446\n",
      "11448\n",
      "11460\n",
      "11464\n",
      "11470\n",
      "11473\n",
      "11474\n",
      "11477\n",
      "11486\n",
      "11505\n",
      "11512\n",
      "11537\n",
      "11554\n",
      "11559\n",
      "11563\n",
      "11567\n",
      "11587\n",
      "11588\n",
      "11590\n",
      "11595\n",
      "11623\n",
      "11634\n",
      "11638\n",
      "11643\n",
      "11667\n",
      "11671\n",
      "11674\n",
      "11700\n",
      "11702\n",
      "11708\n",
      "11710\n",
      "11716\n",
      "11722\n",
      "11723\n",
      "11728\n",
      "11735\n",
      "11740\n",
      "11745\n",
      "11755\n",
      "11756\n",
      "11760\n",
      "11762\n",
      "11763\n",
      "11764\n",
      "11766\n",
      "11767\n",
      "11787\n",
      "11795\n",
      "11809\n",
      "11815\n",
      "11818\n",
      "11825\n",
      "11826\n",
      "11830\n",
      "11838\n",
      "11842\n",
      "11844\n",
      "11850\n",
      "11860\n",
      "11861\n",
      "11862\n",
      "11870\n",
      "11876\n",
      "11877\n",
      "11880\n",
      "11892\n",
      "11897\n",
      "11901\n",
      "11908\n",
      "11912\n",
      "11920\n",
      "11923\n",
      "11932\n",
      "11944\n",
      "11950\n",
      "11952\n",
      "11957\n",
      "11973\n",
      "11981\n",
      "11993\n",
      "12000\n",
      "12003\n",
      "12008\n",
      "12013\n",
      "12014\n",
      "12020\n",
      "12026\n",
      "12028\n",
      "12039\n",
      "12048\n",
      "12065\n",
      "12076\n",
      "12077\n",
      "12081\n",
      "12087\n",
      "12091\n",
      "12104\n",
      "12108\n",
      "12110\n",
      "12113\n",
      "12122\n",
      "12125\n",
      "12136\n",
      "12140\n",
      "12142\n",
      "12149\n",
      "12150\n",
      "12154\n",
      "12157\n",
      "12164\n",
      "12167\n",
      "12183\n",
      "12188\n",
      "12192\n",
      "12194\n",
      "12198\n",
      "12203\n",
      "12233\n",
      "12234\n",
      "12272\n",
      "12274\n",
      "12281\n",
      "12289\n",
      "12292\n",
      "12302\n",
      "12306\n",
      "12310\n",
      "12312\n",
      "12330\n",
      "12337\n",
      "12344\n",
      "12346\n",
      "12351\n",
      "12365\n",
      "12367\n",
      "12387\n",
      "12403\n",
      "12408\n",
      "12410\n",
      "12411\n",
      "12412\n",
      "12413\n",
      "12450\n",
      "12467\n",
      "12501\n",
      "12508\n",
      "12530\n",
      "12532\n",
      "12540\n",
      "12566\n",
      "12567\n",
      "12573\n",
      "12581\n",
      "12582\n",
      "12589\n",
      "12599\n",
      "12607\n",
      "12613\n",
      "12623\n",
      "12631\n",
      "12634\n",
      "12659\n",
      "12660\n",
      "12690\n",
      "12693\n",
      "12697\n",
      "12706\n",
      "12709\n",
      "12712\n",
      "12713\n",
      "12720\n",
      "12730\n",
      "12733\n",
      "12736\n",
      "12739\n",
      "12741\n",
      "12746\n",
      "12769\n",
      "12770\n",
      "12773\n",
      "12776\n",
      "12779\n",
      "12788\n",
      "12795\n",
      "12797\n",
      "12798\n",
      "12799\n",
      "12810\n",
      "12819\n",
      "12829\n",
      "12830\n",
      "12831\n",
      "12834\n",
      "12849\n",
      "12856\n",
      "12878\n",
      "12896\n",
      "12902\n",
      "12905\n",
      "12914\n",
      "12920\n",
      "12927\n",
      "12937\n",
      "12938\n",
      "12940\n",
      "12945\n",
      "12954\n",
      "12982\n",
      "12984\n",
      "12987\n",
      "13033\n",
      "13036\n",
      "13045\n",
      "13052\n",
      "13055\n",
      "13072\n",
      "13074\n",
      "13080\n",
      "13086\n",
      "13101\n",
      "13110\n",
      "13123\n",
      "13144\n",
      "13174\n",
      "13179\n",
      "13181\n",
      "13183\n",
      "13185\n",
      "13192\n",
      "13203\n",
      "13208\n",
      "13212\n",
      "13214\n",
      "13217\n",
      "13223\n",
      "13248\n",
      "13252\n",
      "13253\n",
      "13258\n",
      "13259\n",
      "13262\n",
      "13265\n",
      "13267\n",
      "13289\n",
      "13293\n",
      "13305\n",
      "13306\n",
      "13308\n",
      "13316\n",
      "13318\n",
      "13329\n",
      "13330\n",
      "13355\n",
      "13373\n",
      "13401\n",
      "13406\n",
      "13422\n",
      "13428\n",
      "13431\n",
      "13436\n",
      "13440\n",
      "13451\n",
      "13455\n",
      "13463\n",
      "13476\n",
      "13477\n",
      "13486\n",
      "13509\n",
      "13514\n",
      "13528\n",
      "13535\n",
      "13536\n",
      "13542\n",
      "13543\n",
      "13552\n",
      "13559\n",
      "13564\n",
      "13579\n",
      "13593\n",
      "13599\n",
      "13615\n",
      "13618\n",
      "13622\n",
      "13625\n",
      "13627\n",
      "13628\n",
      "13633\n",
      "13634\n",
      "13636\n",
      "13641\n",
      "13644\n",
      "13648\n",
      "13657\n",
      "13664\n",
      "13666\n",
      "13686\n",
      "13699\n",
      "13702\n",
      "13705\n",
      "13714\n",
      "13719\n",
      "13725\n",
      "13733\n",
      "13735\n",
      "13740\n",
      "13761\n",
      "13778\n",
      "13794\n",
      "13796\n",
      "13806\n",
      "13807\n",
      "13816\n",
      "13830\n",
      "13835\n",
      "13837\n",
      "13839\n",
      "13852\n",
      "13855\n",
      "13856\n",
      "13864\n",
      "13867\n",
      "13881\n",
      "13882\n",
      "13890\n",
      "13902\n",
      "13906\n",
      "13920\n",
      "13922\n",
      "13930\n",
      "13935\n",
      "13948\n",
      "13960\n",
      "13965\n",
      "13978\n",
      "13993\n",
      "13998\n",
      "14002\n",
      "14008\n",
      "14031\n",
      "14033\n",
      "14038\n",
      "14041\n",
      "14048\n",
      "14060\n",
      "14061\n",
      "14078\n",
      "14082\n",
      "14085\n",
      "14087\n",
      "14093\n",
      "14094\n",
      "14098\n",
      "14101\n",
      "14104\n",
      "14106\n",
      "14109\n",
      "14114\n",
      "14116\n",
      "14131\n",
      "14176\n",
      "14183\n",
      "14190\n",
      "14193\n",
      "14197\n",
      "14200\n",
      "14205\n",
      "14217\n",
      "14219\n",
      "14230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14240\n",
      "14245\n",
      "14256\n",
      "14263\n",
      "14269\n",
      "14279\n",
      "14291\n",
      "14297\n",
      "14305\n",
      "14313\n",
      "14316\n",
      "14321\n",
      "14328\n",
      "14329\n",
      "14330\n",
      "14332\n",
      "14333\n",
      "14334\n",
      "14342\n",
      "14348\n",
      "14363\n",
      "14381\n",
      "14393\n",
      "14405\n",
      "14423\n",
      "14427\n",
      "14432\n",
      "14458\n",
      "14467\n",
      "14472\n",
      "14475\n",
      "14478\n",
      "14498\n",
      "14507\n",
      "14510\n",
      "14516\n",
      "14517\n",
      "14520\n",
      "14565\n",
      "14567\n",
      "14568\n",
      "14579\n",
      "14582\n",
      "14585\n",
      "14597\n",
      "14603\n",
      "14606\n",
      "14611\n",
      "14617\n",
      "14621\n",
      "14622\n",
      "14634\n",
      "14646\n",
      "14651\n",
      "14667\n",
      "14669\n",
      "14701\n",
      "14703\n",
      "14705\n",
      "14711\n",
      "14716\n",
      "14738\n",
      "14741\n",
      "14749\n",
      "14755\n",
      "14757\n",
      "14768\n",
      "14822\n",
      "14824\n",
      "14828\n",
      "14835\n",
      "14836\n",
      "14837\n",
      "14841\n",
      "14855\n",
      "14858\n",
      "14862\n",
      "14873\n",
      "14887\n",
      "14898\n",
      "14909\n",
      "14914\n",
      "14922\n",
      "14928\n",
      "14946\n",
      "14950\n",
      "14953\n",
      "14975\n",
      "14987\n",
      "14990\n",
      "14996\n",
      "15003\n",
      "15006\n",
      "15012\n",
      "15017\n",
      "15025\n",
      "15041\n",
      "15046\n",
      "15052\n",
      "15054\n",
      "15055\n",
      "15057\n",
      "15061\n",
      "15066\n",
      "15091\n",
      "15093\n",
      "15122\n",
      "15128\n",
      "15153\n",
      "15160\n",
      "15175\n",
      "15185\n",
      "15188\n",
      "15191\n",
      "15208\n",
      "15209\n",
      "15219\n",
      "15226\n",
      "15227\n",
      "15228\n",
      "15232\n",
      "15243\n",
      "15254\n",
      "15283\n",
      "15284\n",
      "15285\n",
      "15301\n",
      "15311\n",
      "15337\n",
      "15343\n",
      "15348\n",
      "15357\n",
      "15361\n",
      "15370\n",
      "15380\n",
      "15381\n",
      "15387\n",
      "15401\n",
      "15407\n",
      "15411\n",
      "15418\n",
      "15428\n",
      "15439\n",
      "15441\n",
      "15450\n",
      "15453\n",
      "15454\n",
      "15465\n",
      "15467\n",
      "15472\n",
      "15485\n",
      "15486\n",
      "15503\n",
      "15508\n",
      "15509\n",
      "15538\n",
      "15545\n",
      "15546\n",
      "15547\n",
      "15548\n",
      "15558\n",
      "15563\n",
      "15566\n",
      "15570\n",
      "15573\n",
      "15593\n",
      "15610\n",
      "15624\n",
      "15632\n",
      "15637\n",
      "15641\n",
      "15644\n",
      "15645\n",
      "15654\n",
      "15672\n",
      "15679\n",
      "15686\n",
      "15687\n",
      "15697\n",
      "15704\n",
      "15713\n",
      "15716\n",
      "15722\n",
      "15726\n",
      "15727\n",
      "15728\n",
      "15733\n",
      "15749\n",
      "15754\n",
      "15763\n",
      "15764\n",
      "15769\n",
      "15770\n",
      "15778\n",
      "15784\n",
      "15793\n",
      "15809\n",
      "15812\n",
      "15832\n",
      "15841\n",
      "15843\n",
      "15845\n",
      "15847\n",
      "15850\n",
      "15852\n",
      "15853\n",
      "15859\n",
      "15865\n",
      "15868\n",
      "15883\n",
      "15894\n",
      "15904\n",
      "15914\n",
      "15916\n",
      "15918\n",
      "15919\n",
      "15922\n",
      "15930\n",
      "15933\n",
      "15935\n",
      "15952\n",
      "15964\n",
      "15967\n",
      "15977\n",
      "15996\n",
      "16013\n",
      "16020\n",
      "16022\n",
      "16025\n",
      "16032\n",
      "16044\n",
      "16053\n",
      "16055\n",
      "16072\n",
      "16074\n",
      "16076\n",
      "16088\n",
      "16112\n",
      "16118\n",
      "16122\n",
      "16129\n",
      "16139\n",
      "16156\n",
      "16164\n",
      "16166\n",
      "16172\n",
      "16186\n",
      "16194\n",
      "16200\n",
      "16204\n",
      "16216\n",
      "16236\n",
      "16256\n",
      "16258\n",
      "16259\n",
      "16260\n",
      "16265\n",
      "16271\n",
      "16275\n",
      "16278\n",
      "16279\n",
      "16281\n",
      "16286\n",
      "16296\n",
      "16298\n",
      "16312\n",
      "16320\n",
      "16342\n",
      "16345\n",
      "16351\n",
      "16373\n",
      "16375\n",
      "16381\n",
      "16382\n",
      "16391\n",
      "16411\n",
      "16412\n",
      "16429\n",
      "16446\n",
      "16454\n",
      "16455\n",
      "16463\n",
      "16468\n",
      "16483\n",
      "16490\n",
      "16492\n",
      "16504\n",
      "16516\n",
      "16518\n",
      "16531\n",
      "16542\n",
      "16544\n",
      "16549\n",
      "16550\n",
      "16551\n",
      "16553\n",
      "16554\n",
      "16560\n",
      "16577\n",
      "16579\n",
      "16590\n",
      "16592\n",
      "16605\n",
      "16618\n",
      "16621\n",
      "16633\n",
      "16634\n",
      "16636\n",
      "16644\n",
      "16646\n",
      "16655\n",
      "16666\n",
      "16674\n",
      "16675\n",
      "16676\n",
      "16680\n",
      "16684\n",
      "16685\n",
      "16687\n",
      "16695\n",
      "16696\n",
      "16698\n",
      "16709\n",
      "16721\n",
      "16723\n",
      "16724\n",
      "16726\n",
      "16727\n",
      "16733\n",
      "16738\n",
      "16740\n",
      "16745\n",
      "16748\n",
      "16752\n",
      "16755\n",
      "16774\n",
      "16775\n",
      "16776\n",
      "16784\n",
      "16786\n",
      "16787\n",
      "16791\n",
      "16792\n",
      "16798\n",
      "16800\n",
      "16802\n",
      "16805\n",
      "16821\n",
      "16832\n",
      "16839\n",
      "16847\n",
      "16849\n",
      "16856\n",
      "16860\n",
      "16865\n",
      "16879\n",
      "16881\n",
      "16884\n",
      "16888\n",
      "16897\n",
      "16911\n",
      "16914\n",
      "16917\n",
      "16934\n",
      "16943\n",
      "16947\n",
      "16949\n",
      "16961\n",
      "16975\n",
      "16976\n",
      "16988\n",
      "16989\n",
      "16992\n",
      "16994\n",
      "16996\n",
      "17002\n",
      "17008\n",
      "17021\n",
      "17024\n",
      "17026\n",
      "17041\n",
      "17047\n",
      "17050\n",
      "17052\n",
      "17065\n",
      "17068\n",
      "17072\n",
      "17079\n",
      "17083\n",
      "17086\n",
      "17089\n",
      "17112\n",
      "17122\n",
      "17125\n",
      "17130\n",
      "17132\n",
      "17133\n",
      "17143\n",
      "17145\n",
      "17149\n",
      "17155\n",
      "17181\n",
      "17182\n",
      "17190\n",
      "17191\n",
      "17196\n",
      "17205\n",
      "17206\n",
      "17236\n",
      "17239\n",
      "17244\n",
      "17249\n",
      "17260\n",
      "17261\n",
      "17269\n",
      "17278\n",
      "17279\n",
      "17282\n",
      "17299\n",
      "17328\n",
      "17330\n",
      "17339\n",
      "17341\n",
      "17367\n",
      "17384\n",
      "17399\n",
      "17412\n",
      "17419\n",
      "17421\n",
      "17423\n",
      "17472\n",
      "17496\n",
      "17497\n",
      "17505\n",
      "17513\n",
      "17521\n",
      "17530\n",
      "17531\n",
      "17539\n",
      "17546\n",
      "17564\n",
      "17570\n",
      "17574\n",
      "17580\n",
      "17581\n",
      "17582\n",
      "17586\n",
      "17589\n",
      "17606\n",
      "17610\n",
      "17613\n",
      "17617\n",
      "17651\n",
      "17655\n",
      "17656\n",
      "17658\n",
      "17659\n",
      "17664\n",
      "17667\n",
      "17683\n",
      "17692\n",
      "17696\n",
      "17708\n",
      "17712\n",
      "17717\n",
      "17721\n",
      "17722\n",
      "17726\n",
      "17728\n",
      "17735\n",
      "17742\n",
      "17754\n",
      "17758\n",
      "17764\n",
      "17775\n",
      "17778\n",
      "17782\n",
      "17786\n",
      "17789\n",
      "17791\n",
      "17794\n",
      "17795\n",
      "17796\n",
      "17798\n",
      "17800\n",
      "17802\n",
      "17807\n",
      "17820\n",
      "17827\n",
      "17833\n",
      "17837\n",
      "17843\n",
      "17857\n",
      "17858\n",
      "17859\n",
      "17863\n",
      "17868\n",
      "17882\n",
      "17891\n",
      "17894\n",
      "17899\n",
      "17901\n",
      "17904\n",
      "17906\n",
      "17912\n",
      "17946\n",
      "17949\n",
      "17960\n",
      "17964\n",
      "17977\n",
      "17978\n",
      "17997\n",
      "18031\n",
      "18032\n",
      "18035\n",
      "18036\n",
      "18054\n",
      "18063\n",
      "18067\n",
      "18068\n",
      "18071\n",
      "18076\n",
      "18082\n",
      "18089\n",
      "18093\n",
      "18094\n",
      "18101\n",
      "18103\n",
      "18104\n",
      "18105\n",
      "18113\n",
      "18114\n",
      "18119\n",
      "18123\n",
      "18126\n",
      "18148\n",
      "18166\n",
      "18168\n",
      "18184\n",
      "18186\n",
      "18187\n",
      "18200\n",
      "18225\n",
      "18233\n",
      "18250\n",
      "18254\n",
      "18258\n",
      "18281\n",
      "18288\n",
      "18294\n",
      "18320\n",
      "18321\n",
      "18322\n",
      "18333\n",
      "18353\n",
      "18357\n",
      "18363\n",
      "18368\n",
      "18376\n",
      "18402\n",
      "18408\n",
      "18418\n",
      "18419\n",
      "18430\n",
      "18436\n",
      "18456\n",
      "18459\n",
      "18460\n",
      "18461\n",
      "18473\n",
      "18477\n",
      "18484\n",
      "18510\n",
      "18516\n",
      "18546\n",
      "18556\n",
      "18557\n",
      "18570\n",
      "18572\n",
      "18586\n",
      "18590\n",
      "18596\n",
      "18605\n",
      "18614\n",
      "18622\n",
      "18623\n",
      "18629\n",
      "18637\n",
      "18648\n",
      "18653\n",
      "18657\n",
      "18673\n",
      "18677\n",
      "18681\n",
      "18689\n",
      "18699\n",
      "18700\n",
      "18704\n",
      "18719\n",
      "18725\n",
      "18737\n",
      "18739\n",
      "18740\n",
      "18756\n",
      "18771\n",
      "18792\n",
      "18815\n",
      "18825\n",
      "18833\n",
      "18839\n",
      "18846\n",
      "18847\n",
      "18848\n",
      "18860\n",
      "18868\n",
      "18892\n",
      "18897\n",
      "18902\n",
      "18908\n",
      "18910\n",
      "18921\n",
      "18928\n",
      "18936\n",
      "18942\n",
      "18952\n",
      "18953\n",
      "18954\n",
      "18958\n",
      "18962\n",
      "18967\n",
      "18968\n",
      "18971\n",
      "18982\n",
      "18996\n",
      "19002\n",
      "19012\n",
      "19016\n",
      "19029\n",
      "19038\n",
      "19039\n",
      "19040\n",
      "19048\n",
      "19053\n",
      "19056\n",
      "19059\n",
      "19060\n",
      "19069\n",
      "19074\n",
      "19079\n",
      "19080\n",
      "19087\n",
      "19095\n",
      "19098\n",
      "19102\n",
      "19104\n",
      "19117\n",
      "19119\n",
      "19142\n",
      "19148\n",
      "19155\n",
      "19164\n",
      "19169\n",
      "19184\n",
      "19201\n",
      "19208\n",
      "19213\n",
      "19216\n",
      "19223\n",
      "19241\n",
      "19246\n",
      "19270\n",
      "19289\n",
      "19293\n",
      "19296\n",
      "19303\n",
      "19308\n",
      "19310\n",
      "19322\n",
      "19326\n",
      "19330\n",
      "19333\n",
      "19334\n",
      "19338\n",
      "19339\n",
      "19344\n",
      "19346\n",
      "19351\n",
      "19354\n",
      "19356\n",
      "19361\n",
      "19370\n",
      "19371\n",
      "19372\n",
      "19375\n",
      "19398\n",
      "19410\n",
      "19411\n",
      "19412\n",
      "19418\n",
      "19450\n",
      "19463\n",
      "19470\n",
      "19472\n",
      "19493\n",
      "19507\n",
      "19512\n",
      "19522\n",
      "19529\n",
      "19533\n",
      "19543\n",
      "19547\n",
      "19568\n",
      "19569\n",
      "19570\n",
      "19583\n",
      "19588\n",
      "19590\n",
      "19592\n",
      "19596\n",
      "19597\n",
      "19604\n",
      "19616\n",
      "19617\n",
      "19620\n",
      "19622\n",
      "19627\n",
      "19632\n",
      "19641\n",
      "19666\n",
      "19690\n",
      "19698\n",
      "19701\n",
      "19728\n",
      "19731\n",
      "19739\n",
      "19754\n",
      "19757\n",
      "19771\n",
      "19772\n",
      "19793\n",
      "19805\n",
      "19811\n",
      "19815\n",
      "19823\n",
      "19827\n",
      "19833\n",
      "19842\n",
      "19851\n",
      "19858\n",
      "19859\n",
      "19872\n",
      "19891\n",
      "19894\n",
      "19896\n",
      "19897\n",
      "19911\n",
      "19913\n",
      "19937\n",
      "19940\n",
      "19952\n",
      "19953\n",
      "19960\n",
      "19976\n",
      "19981\n",
      "19984\n",
      "19986\n",
      "20007\n",
      "20009\n",
      "20032\n",
      "20060\n",
      "20061\n",
      "20062\n",
      "20064\n",
      "20066\n",
      "20071\n",
      "20075\n",
      "20082\n",
      "20084\n",
      "20086\n",
      "20088\n",
      "20101\n",
      "20116\n",
      "20124\n",
      "20125\n",
      "20130\n",
      "20132\n",
      "20133\n",
      "20140\n",
      "20144\n",
      "20150\n",
      "20162\n",
      "20169\n",
      "20173\n",
      "20181\n",
      "20187\n",
      "20190\n",
      "20199\n",
      "20204\n",
      "20214\n",
      "20217\n",
      "20226\n",
      "20230\n",
      "20236\n",
      "20240\n",
      "20242\n",
      "20248\n",
      "20249\n",
      "20256\n",
      "20263\n",
      "20298\n",
      "20309\n",
      "20312\n",
      "20318\n",
      "20320\n",
      "20326\n",
      "20339\n",
      "20346\n",
      "20356\n",
      "20372\n",
      "20375\n",
      "20381\n",
      "20383\n",
      "20389\n",
      "20397\n",
      "20400\n",
      "20410\n",
      "20421\n",
      "20423\n",
      "20424\n",
      "20425\n",
      "20426\n",
      "20460\n",
      "20471\n",
      "20473\n",
      "20479\n",
      "20528\n",
      "20536\n",
      "20542\n",
      "20545\n",
      "20546\n",
      "20548\n",
      "20551\n",
      "20560\n",
      "20561\n",
      "20573\n",
      "20577\n",
      "20578\n",
      "20598\n",
      "20600\n",
      "20606\n",
      "20620\n",
      "20624\n",
      "20626\n",
      "20643\n",
      "20644\n",
      "20651\n",
      "20656\n",
      "20678\n",
      "20698\n",
      "20704\n",
      "20711\n",
      "20719\n",
      "20742\n",
      "20743\n",
      "20745\n",
      "20747\n",
      "20754\n",
      "20759\n",
      "20769\n",
      "20776\n",
      "20778\n",
      "20796\n",
      "20801\n",
      "20804\n",
      "20816\n",
      "20824\n",
      "20836\n",
      "20843\n",
      "20856\n",
      "20862\n",
      "20865\n",
      "20883\n",
      "20887\n",
      "20891\n",
      "20900\n",
      "20913\n",
      "20920\n",
      "20922\n",
      "20925\n",
      "20929\n",
      "20931\n",
      "20938\n",
      "20944\n",
      "20947\n",
      "20957\n",
      "20966\n",
      "20968\n",
      "20969\n",
      "20981\n",
      "21003\n",
      "21015\n",
      "21021\n",
      "21042\n",
      "21051\n",
      "21071\n",
      "21072\n",
      "21090\n",
      "21093\n",
      "21102\n",
      "21107\n",
      "21108\n",
      "21115\n",
      "21123\n",
      "21124\n",
      "21138\n",
      "21139\n",
      "21144\n",
      "21150\n",
      "21151\n",
      "21156\n",
      "21162\n",
      "21166\n",
      "21168\n",
      "21185\n",
      "21195\n",
      "21202\n",
      "21204\n",
      "21220\n",
      "21223\n",
      "21235\n",
      "21255\n",
      "21256\n",
      "21273\n",
      "21277\n",
      "21280\n",
      "21282\n",
      "21284\n",
      "21293\n",
      "21301\n",
      "21306\n",
      "21312\n",
      "21318\n",
      "21321\n",
      "21323\n",
      "21334\n",
      "21347\n",
      "21350\n",
      "21362\n",
      "21369\n",
      "21399\n",
      "21401\n",
      "21405\n",
      "21413\n",
      "21423\n",
      "21431\n",
      "21447\n",
      "21448\n",
      "21450\n",
      "21452\n",
      "21453\n",
      "21454\n",
      "21458\n",
      "21460\n",
      "21469\n",
      "21483\n",
      "21507\n",
      "21510\n",
      "21513\n",
      "21514\n",
      "21520\n",
      "21522\n",
      "21530\n",
      "21538\n",
      "21548\n",
      "21553\n",
      "21569\n",
      "21574\n",
      "21580\n",
      "21584\n",
      "21592\n",
      "21607\n",
      "21613\n",
      "21619\n",
      "21623\n",
      "21630\n",
      "21633\n",
      "21645\n",
      "21651\n",
      "21666\n",
      "21667\n",
      "21675\n",
      "21688\n",
      "21700\n",
      "21706\n",
      "21727\n",
      "21734\n",
      "21743\n",
      "21745\n",
      "21766\n",
      "21769\n",
      "21771\n",
      "21773\n",
      "21786\n",
      "21795\n",
      "21806\n",
      "21817\n",
      "21819\n",
      "21827\n",
      "21828\n",
      "21835\n",
      "21845\n",
      "21857\n",
      "21860\n",
      "21866\n",
      "21885\n",
      "21892\n",
      "21900\n",
      "21901\n",
      "21903\n",
      "21910\n",
      "21921\n",
      "21939\n",
      "21951\n",
      "21955\n",
      "21962\n",
      "21965\n",
      "21975\n",
      "21990\n",
      "22004\n",
      "22007\n",
      "22010\n",
      "22017\n",
      "22019\n",
      "22020\n",
      "22024\n",
      "22029\n",
      "22043\n",
      "22067\n",
      "22078\n",
      "22083\n",
      "22098\n",
      "22119\n",
      "22122\n",
      "22126\n",
      "22129\n",
      "22130\n",
      "22160\n",
      "22165\n",
      "22180\n",
      "22192\n",
      "22200\n",
      "22207\n",
      "22208\n",
      "22213\n",
      "22225\n",
      "22234\n",
      "22251\n",
      "22253\n",
      "22266\n",
      "22277\n",
      "22285\n",
      "22289\n",
      "22291\n",
      "22297\n",
      "22312\n",
      "22330\n",
      "22332\n",
      "22336\n",
      "22337\n",
      "22350\n",
      "22371\n",
      "22383\n",
      "22386\n",
      "22392\n",
      "22421\n",
      "22423\n",
      "22424\n",
      "22426\n",
      "22432\n",
      "22442\n",
      "22449\n",
      "22450\n",
      "22466\n",
      "22475\n",
      "22490\n",
      "22493\n",
      "22500\n",
      "22503\n",
      "22505\n",
      "22516\n",
      "22520\n",
      "22534\n",
      "22538\n",
      "22545\n",
      "22549\n",
      "22553\n",
      "22557\n",
      "22559\n",
      "22578\n",
      "22579\n",
      "22581\n",
      "22585\n",
      "22587\n",
      "22588\n",
      "22600\n",
      "22624\n",
      "22626\n",
      "22644\n",
      "22656\n",
      "22663\n",
      "22667\n",
      "22677\n",
      "22697\n",
      "22712\n",
      "22718\n",
      "22732\n",
      "22735\n",
      "22751\n",
      "22753\n",
      "22754\n",
      "22766\n",
      "22782\n",
      "22788\n",
      "22792\n",
      "22797\n",
      "22800\n",
      "22801\n",
      "22804\n",
      "22815\n",
      "22817\n",
      "22818\n",
      "22823\n",
      "22835\n",
      "22836\n",
      "22844\n",
      "22849\n",
      "22851\n",
      "22855\n",
      "22856\n",
      "22860\n",
      "22862\n",
      "22872\n",
      "22888\n",
      "22891\n",
      "22896\n",
      "22914\n",
      "22921\n",
      "22930\n",
      "22933\n",
      "22936\n",
      "22938\n",
      "22941\n",
      "22946\n",
      "22951\n",
      "22954\n",
      "22963\n",
      "22970\n",
      "22987\n",
      "23000\n",
      "23014\n",
      "23023\n",
      "23028\n",
      "23036\n",
      "23038\n",
      "23039\n",
      "23049\n",
      "23060\n",
      "23082\n",
      "23092\n",
      "23100\n",
      "23108\n",
      "23109\n",
      "23140\n",
      "23150\n",
      "23153\n",
      "23154\n",
      "23156\n",
      "23161\n",
      "23162\n",
      "23168\n",
      "23178\n",
      "23194\n",
      "23197\n",
      "23201\n",
      "23209\n",
      "23219\n",
      "23224\n",
      "23238\n",
      "23242\n",
      "23244\n",
      "23249\n",
      "23257\n",
      "23262\n",
      "23264\n",
      "23292\n",
      "23304\n",
      "23317\n",
      "23318\n",
      "23325\n",
      "23339\n",
      "23364\n",
      "23365\n",
      "23368\n",
      "23371\n",
      "23380\n",
      "23384\n",
      "23390\n",
      "23394\n",
      "23397\n",
      "23407\n",
      "23440\n",
      "23450\n",
      "23473\n",
      "23475\n",
      "23477\n",
      "23478\n",
      "23483\n",
      "23489\n",
      "23498\n",
      "23503\n",
      "23517\n",
      "23523\n",
      "23533\n",
      "23546\n",
      "23549\n",
      "23568\n",
      "23577\n",
      "23592\n",
      "23596\n",
      "23613\n",
      "23632\n",
      "23637\n",
      "23642\n",
      "23651\n",
      "23657\n",
      "23666\n",
      "23675\n",
      "23678\n",
      "23680\n",
      "23687\n",
      "23694\n",
      "23696\n",
      "23697\n",
      "23705\n",
      "23706\n",
      "23707\n",
      "23711\n",
      "23719\n",
      "23725\n",
      "23734\n",
      "23735\n",
      "23739\n",
      "23749\n",
      "23752\n",
      "23755\n",
      "23761\n",
      "23762\n",
      "23779\n",
      "23780\n",
      "23785\n",
      "23786\n",
      "23788\n",
      "23790\n",
      "23792\n",
      "23802\n",
      "23809\n",
      "23811\n",
      "23815\n",
      "23826\n",
      "23829\n",
      "23837\n",
      "23843\n",
      "23844\n",
      "23845\n",
      "23847\n",
      "23849\n",
      "23872\n",
      "23873\n",
      "23876\n",
      "23885\n",
      "23895\n",
      "23897\n",
      "23927\n",
      "23929\n",
      "23933\n",
      "23943\n",
      "23946\n",
      "23949\n",
      "23955\n",
      "23979\n",
      "23986\n",
      "23990\n",
      "24018\n",
      "24021\n",
      "24029\n",
      "24030\n",
      "24049\n",
      "24067\n",
      "24071\n",
      "24074\n",
      "24083\n",
      "24084\n",
      "24086\n",
      "24094\n",
      "24099\n",
      "24105\n",
      "24111\n",
      "24114\n",
      "24121\n",
      "24123\n",
      "24129\n",
      "24132\n",
      "24134\n",
      "24141\n",
      "24163\n",
      "24170\n",
      "24177\n",
      "24181\n",
      "24190\n",
      "24198\n",
      "24226\n",
      "24235\n",
      "24237\n",
      "24242\n",
      "24243\n",
      "24248\n",
      "24252\n",
      "24253\n",
      "24263\n",
      "24271\n",
      "24276\n",
      "24280\n",
      "24285\n",
      "24289\n",
      "24307\n",
      "24317\n",
      "24330\n",
      "24338\n",
      "24344\n",
      "24357\n",
      "24361\n",
      "24374\n",
      "24387\n",
      "24401\n",
      "24402\n",
      "24444\n",
      "24447\n",
      "24465\n",
      "24468\n",
      "24477\n",
      "24480\n",
      "24486\n",
      "24504\n",
      "24510\n",
      "24548\n",
      "24556\n",
      "24562\n",
      "24569\n",
      "24571\n",
      "24573\n",
      "24577\n",
      "24580\n",
      "24581\n",
      "24586\n",
      "24588\n",
      "24607\n",
      "24612\n",
      "24616\n",
      "24622\n",
      "24631\n",
      "24653\n",
      "24656\n",
      "24663\n",
      "24672\n",
      "24674\n",
      "24675\n",
      "24676\n",
      "24677\n",
      "24682\n",
      "24683\n",
      "24687\n",
      "24692\n",
      "24711\n",
      "24712\n",
      "24714\n",
      "24743\n",
      "24772\n",
      "24776\n",
      "24785\n",
      "24794\n",
      "24806\n",
      "24807\n",
      "24808\n",
      "24825\n",
      "24837\n",
      "24846\n",
      "24854\n",
      "24856\n",
      "24865\n",
      "24868\n",
      "24872\n",
      "24889\n",
      "24891\n",
      "24900\n",
      "24901\n",
      "24902\n",
      "24937\n",
      "24938\n",
      "24955\n",
      "24957\n",
      "24958\n",
      "24975\n",
      "24977\n",
      "24978\n",
      "24981\n",
      "24982\n",
      "24995\n",
      "25001\n",
      "25004\n",
      "25006\n",
      "25008\n",
      "25009\n",
      "25012\n",
      "25016\n",
      "25027\n",
      "25030\n",
      "25039\n",
      "25049\n",
      "25053\n",
      "25060\n",
      "25069\n",
      "25070\n",
      "25072\n",
      "25075\n",
      "25076\n",
      "25104\n",
      "25108\n",
      "25111\n",
      "25113\n",
      "25115\n",
      "25117\n",
      "25121\n",
      "25131\n",
      "25135\n",
      "25143\n",
      "25148\n",
      "25150\n",
      "25155\n",
      "25157\n",
      "25167\n",
      "25175\n",
      "25177\n",
      "25178\n",
      "25184\n",
      "25188\n",
      "25189\n",
      "25212\n",
      "25217\n",
      "25219\n",
      "25225\n",
      "25232\n",
      "25234\n",
      "25235\n",
      "25251\n",
      "25255\n",
      "25256\n",
      "25259\n",
      "25286\n",
      "25299\n",
      "25302\n",
      "25313\n",
      "25317\n",
      "25318\n",
      "25323\n",
      "25326\n",
      "25329\n",
      "25332\n",
      "25337\n",
      "25338\n",
      "25345\n",
      "25354\n",
      "25363\n",
      "25367\n",
      "25368\n",
      "25377\n",
      "25384\n",
      "25385\n",
      "25395\n",
      "25396\n",
      "25397\n",
      "25403\n",
      "25411\n",
      "25413\n",
      "25415\n",
      "25416\n",
      "25449\n",
      "25458\n",
      "25473\n",
      "25477\n",
      "25481\n",
      "25482\n",
      "25487\n",
      "25490\n",
      "25497\n",
      "25504\n",
      "25510\n",
      "25515\n",
      "25523\n",
      "25526\n",
      "25528\n",
      "25538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25540\n",
      "25553\n",
      "25557\n",
      "25574\n",
      "25578\n",
      "25581\n",
      "25590\n",
      "25634\n",
      "25655\n",
      "25658\n",
      "25668\n",
      "25679\n",
      "25696\n",
      "25697\n",
      "25699\n",
      "25708\n",
      "25711\n",
      "25714\n",
      "25722\n",
      "25723\n",
      "25724\n",
      "25725\n",
      "25734\n",
      "25736\n",
      "25739\n",
      "25757\n",
      "25764\n",
      "25772\n",
      "25775\n",
      "25780\n",
      "25782\n",
      "25786\n",
      "25790\n",
      "25801\n",
      "25809\n",
      "25814\n",
      "25815\n",
      "25835\n",
      "25848\n",
      "25851\n",
      "25860\n",
      "25876\n",
      "25879\n",
      "25882\n",
      "25888\n",
      "25889\n",
      "25891\n",
      "25905\n",
      "25921\n",
      "25929\n",
      "25933\n",
      "25935\n",
      "25941\n",
      "25949\n",
      "25950\n",
      "25954\n",
      "25963\n",
      "25965\n",
      "25975\n",
      "25989\n",
      "25995\n",
      "25996\n",
      "25997\n",
      "26012\n",
      "26013\n",
      "26027\n",
      "26031\n",
      "26036\n",
      "26040\n",
      "26043\n",
      "26046\n",
      "26052\n",
      "26055\n",
      "26066\n",
      "26067\n",
      "26094\n",
      "26095\n",
      "26102\n",
      "26105\n",
      "26118\n",
      "26119\n",
      "26124\n",
      "26125\n",
      "26134\n",
      "26136\n",
      "26139\n",
      "26140\n",
      "26160\n",
      "26161\n",
      "26168\n",
      "26178\n",
      "26187\n",
      "26192\n",
      "26200\n",
      "26201\n",
      "26208\n",
      "26212\n",
      "26215\n",
      "26218\n",
      "26219\n",
      "26220\n",
      "26224\n",
      "26228\n",
      "26231\n",
      "26261\n",
      "26263\n",
      "26271\n",
      "26274\n",
      "26277\n",
      "26286\n",
      "26288\n",
      "26291\n",
      "26303\n",
      "26308\n",
      "26313\n",
      "26317\n",
      "26320\n",
      "26324\n",
      "26331\n",
      "26337\n",
      "26350\n",
      "26356\n",
      "26368\n",
      "26377\n",
      "26384\n",
      "26391\n",
      "26395\n",
      "26398\n",
      "26401\n",
      "26421\n",
      "26431\n",
      "26459\n",
      "26469\n",
      "26474\n",
      "26475\n",
      "26479\n",
      "26491\n",
      "26494\n",
      "26515\n",
      "26519\n",
      "26523\n",
      "26532\n",
      "26535\n",
      "26576\n",
      "26579\n",
      "26601\n",
      "26603\n",
      "26618\n",
      "26620\n",
      "26632\n",
      "26633\n",
      "26637\n",
      "26638\n",
      "26648\n",
      "26649\n",
      "26661\n",
      "26672\n",
      "26673\n",
      "26678\n",
      "26685\n",
      "26688\n",
      "26690\n",
      "26698\n",
      "26704\n",
      "26705\n",
      "26709\n",
      "26715\n",
      "26724\n",
      "26727\n",
      "26737\n",
      "26754\n",
      "26771\n",
      "26779\n",
      "26780\n",
      "26781\n",
      "26837\n",
      "26847\n",
      "26855\n",
      "26863\n",
      "26864\n",
      "26868\n",
      "26872\n",
      "26884\n",
      "26887\n",
      "26889\n",
      "26901\n",
      "26921\n",
      "26926\n",
      "26927\n",
      "26928\n",
      "26932\n",
      "26935\n",
      "26936\n",
      "26948\n",
      "26956\n",
      "26964\n",
      "26967\n",
      "26996\n",
      "27002\n",
      "27003\n",
      "27020\n",
      "27022\n",
      "27024\n",
      "27026\n",
      "27036\n",
      "27043\n",
      "27048\n",
      "27049\n",
      "27067\n",
      "27077\n",
      "27083\n",
      "27084\n",
      "27089\n",
      "27092\n",
      "27102\n",
      "27103\n",
      "27106\n",
      "27117\n",
      "27119\n",
      "27121\n",
      "27132\n",
      "27143\n",
      "27148\n",
      "27152\n",
      "27160\n",
      "27162\n",
      "27163\n",
      "27167\n",
      "27172\n",
      "27179\n",
      "27184\n",
      "27195\n",
      "27209\n",
      "27210\n",
      "27232\n",
      "27237\n",
      "27242\n",
      "27243\n",
      "27245\n",
      "27247\n",
      "27251\n",
      "27256\n",
      "27266\n",
      "27278\n",
      "27282\n",
      "27290\n",
      "27315\n",
      "27337\n",
      "27355\n",
      "27362\n",
      "27365\n",
      "27367\n",
      "27374\n",
      "27379\n",
      "27390\n",
      "27398\n",
      "27399\n",
      "27400\n",
      "27414\n",
      "27416\n",
      "27424\n",
      "27425\n",
      "27427\n",
      "27431\n",
      "27447\n",
      "27449\n",
      "27462\n",
      "27463\n",
      "27464\n",
      "27467\n",
      "27468\n",
      "27471\n",
      "27472\n",
      "27476\n",
      "27486\n",
      "27499\n",
      "27500\n",
      "27504\n",
      "27511\n",
      "27512\n",
      "27517\n",
      "27530\n",
      "27536\n",
      "27542\n",
      "27547\n",
      "27551\n",
      "27553\n",
      "27554\n",
      "27563\n",
      "27574\n",
      "27576\n",
      "27577\n",
      "27588\n",
      "27597\n",
      "27598\n",
      "27599\n",
      "27603\n",
      "27616\n",
      "27617\n",
      "27639\n",
      "27643\n",
      "27655\n",
      "27657\n",
      "27659\n",
      "27661\n",
      "27665\n",
      "27667\n",
      "27677\n",
      "27690\n",
      "27694\n",
      "27710\n",
      "27712\n",
      "27714\n",
      "27723\n",
      "27726\n",
      "27729\n",
      "27745\n",
      "27750\n",
      "27758\n",
      "27770\n",
      "27778\n",
      "27790\n",
      "27793\n",
      "27795\n",
      "27797\n",
      "27799\n",
      "27800\n",
      "27808\n",
      "27816\n",
      "27823\n",
      "27829\n",
      "27863\n",
      "27866\n",
      "27869\n",
      "27872\n",
      "27879\n",
      "27894\n",
      "27901\n",
      "27905\n",
      "27910\n",
      "27927\n",
      "27931\n",
      "27934\n",
      "27949\n",
      "27953\n",
      "27960\n",
      "27964\n",
      "27979\n",
      "27981\n",
      "27995\n",
      "28016\n",
      "28017\n",
      "28019\n",
      "28026\n",
      "28032\n",
      "28043\n",
      "28050\n",
      "28055\n",
      "28057\n",
      "28061\n",
      "28065\n",
      "28073\n",
      "28076\n",
      "28082\n",
      "28089\n",
      "28090\n",
      "28092\n",
      "28093\n",
      "28099\n",
      "28101\n",
      "28104\n",
      "28109\n",
      "28115\n",
      "28138\n",
      "28152\n",
      "28166\n",
      "28174\n",
      "28175\n",
      "28180\n",
      "28182\n",
      "28190\n",
      "28192\n",
      "28202\n",
      "28206\n",
      "28218\n",
      "28221\n",
      "28222\n",
      "28223\n",
      "28227\n",
      "28239\n",
      "28240\n",
      "28247\n",
      "28259\n",
      "28264\n",
      "28265\n",
      "28270\n",
      "28278\n",
      "28281\n",
      "28291\n",
      "28292\n",
      "28293\n",
      "28296\n",
      "28298\n",
      "28313\n",
      "28315\n",
      "28336\n",
      "28338\n",
      "28343\n",
      "28356\n",
      "28358\n",
      "28365\n",
      "28388\n",
      "28394\n",
      "28399\n",
      "28410\n",
      "28416\n",
      "28423\n",
      "28424\n",
      "28426\n",
      "28447\n",
      "28457\n",
      "28462\n",
      "28496\n",
      "28502\n",
      "28505\n",
      "28508\n",
      "28515\n",
      "28516\n",
      "28524\n",
      "28525\n",
      "28530\n",
      "28531\n",
      "28541\n",
      "28544\n",
      "28566\n",
      "28576\n",
      "28578\n",
      "28600\n",
      "28601\n",
      "28611\n",
      "28619\n",
      "28628\n",
      "28644\n",
      "28651\n",
      "28660\n",
      "28671\n",
      "28676\n",
      "28677\n",
      "28681\n",
      "28683\n",
      "28698\n",
      "28712\n",
      "28716\n",
      "28721\n",
      "28724\n",
      "28729\n",
      "28735\n",
      "28742\n",
      "28752\n",
      "28753\n",
      "28765\n",
      "28775\n",
      "28776\n",
      "28790\n",
      "28792\n",
      "28805\n",
      "28806\n",
      "28808\n",
      "28814\n",
      "28820\n",
      "28824\n",
      "28827\n",
      "28843\n",
      "28865\n",
      "28866\n",
      "28868\n",
      "28869\n",
      "28880\n",
      "28882\n",
      "28883\n",
      "28887\n",
      "28902\n",
      "28911\n",
      "28915\n",
      "28926\n",
      "28930\n",
      "28933\n",
      "28935\n",
      "28939\n",
      "28941\n",
      "28944\n",
      "28973\n",
      "28999\n",
      "29002\n",
      "29027\n",
      "29030\n",
      "29035\n",
      "29043\n",
      "29050\n",
      "29052\n",
      "29054\n",
      "29055\n",
      "29073\n",
      "29075\n",
      "29092\n",
      "29105\n",
      "29110\n",
      "29115\n",
      "29121\n",
      "29123\n",
      "29130\n",
      "29135\n",
      "29137\n",
      "29138\n",
      "29141\n",
      "29142\n",
      "29164\n",
      "29175\n",
      "29176\n",
      "29191\n",
      "29219\n",
      "29251\n",
      "29252\n",
      "29261\n",
      "29262\n",
      "29274\n",
      "29276\n",
      "29278\n",
      "29299\n",
      "29304\n",
      "29305\n",
      "29316\n",
      "29317\n",
      "29328\n",
      "29334\n",
      "29336\n",
      "29338\n",
      "29340\n",
      "29343\n",
      "29350\n",
      "29352\n",
      "29358\n",
      "29359\n",
      "29360\n",
      "29364\n",
      "29366\n",
      "29372\n",
      "29375\n",
      "29377\n",
      "29378\n",
      "29392\n",
      "29398\n",
      "29411\n",
      "29426\n",
      "29430\n",
      "29433\n",
      "29436\n",
      "29453\n",
      "29463\n",
      "29467\n",
      "29470\n",
      "29478\n",
      "29481\n",
      "29483\n",
      "29493\n",
      "29495\n",
      "29496\n",
      "29497\n",
      "29502\n",
      "29503\n",
      "29504\n",
      "29509\n",
      "29512\n",
      "29520\n",
      "29530\n",
      "29541\n",
      "29544\n",
      "29545\n",
      "29552\n",
      "29553\n",
      "29556\n",
      "29558\n",
      "29570\n",
      "29576\n",
      "29579\n",
      "29581\n",
      "29601\n",
      "29615\n",
      "29619\n",
      "29620\n",
      "29621\n",
      "29629\n",
      "29631\n",
      "29633\n",
      "29646\n",
      "29660\n",
      "29664\n",
      "29665\n",
      "29666\n",
      "29672\n",
      "29682\n",
      "29688\n",
      "29690\n",
      "29708\n",
      "29722\n",
      "29726\n",
      "29730\n",
      "29744\n",
      "29755\n",
      "29765\n",
      "29767\n",
      "29810\n",
      "29829\n",
      "29845\n",
      "29859\n",
      "29861\n",
      "29862\n",
      "29866\n",
      "29872\n",
      "29884\n",
      "29894\n",
      "29905\n",
      "29908\n",
      "29918\n",
      "29937\n",
      "29946\n",
      "29950\n",
      "29961\n",
      "29965\n",
      "29967\n",
      "29968\n",
      "29969\n",
      "29971\n",
      "29972\n",
      "29985\n",
      "29991\n",
      "29999\n",
      "30011\n",
      "30015\n",
      "30026\n",
      "30047\n",
      "30049\n",
      "30054\n",
      "30065\n",
      "30067\n",
      "30070\n",
      "30071\n",
      "30075\n",
      "30084\n",
      "30088\n",
      "30089\n",
      "30093\n",
      "30095\n",
      "30105\n",
      "30114\n",
      "30120\n",
      "30129\n",
      "30138\n",
      "30139\n",
      "30140\n",
      "30143\n",
      "30144\n",
      "30154\n",
      "30183\n",
      "30198\n",
      "30202\n",
      "30208\n",
      "30214\n",
      "30217\n",
      "30225\n",
      "30234\n",
      "30239\n",
      "30243\n",
      "30247\n",
      "30253\n",
      "30259\n",
      "30262\n",
      "30266\n",
      "30274\n",
      "30275\n",
      "30284\n",
      "30296\n",
      "30298\n",
      "30299\n",
      "30303\n",
      "30316\n",
      "30322\n",
      "30336\n",
      "30341\n",
      "30344\n",
      "30348\n",
      "30349\n",
      "30354\n",
      "30376\n",
      "30393\n",
      "30408\n",
      "30410\n",
      "30411\n",
      "30412\n",
      "30414\n",
      "30437\n",
      "30447\n",
      "30457\n",
      "30460\n",
      "30461\n",
      "30464\n",
      "30466\n",
      "30476\n",
      "30477\n",
      "30501\n",
      "30503\n",
      "30505\n",
      "30507\n",
      "30508\n",
      "30530\n",
      "30534\n",
      "30541\n",
      "30544\n",
      "30548\n",
      "30550\n",
      "30551\n",
      "30552\n",
      "30561\n",
      "30564\n",
      "30565\n",
      "30569\n",
      "30575\n",
      "30579\n",
      "30590\n",
      "30595\n",
      "30601\n",
      "30606\n",
      "30616\n",
      "30637\n",
      "30638\n",
      "30641\n",
      "30646\n",
      "30648\n",
      "30650\n",
      "30659\n",
      "30660\n",
      "30669\n",
      "30671\n",
      "30678\n",
      "30682\n",
      "30695\n",
      "30696\n",
      "30699\n",
      "30707\n",
      "30714\n",
      "30737\n",
      "30754\n",
      "30775\n",
      "30777\n",
      "30786\n",
      "30790\n",
      "30798\n",
      "30820\n",
      "30822\n",
      "30825\n",
      "30829\n",
      "30846\n",
      "30855\n",
      "30861\n",
      "30868\n",
      "30887\n",
      "30911\n",
      "30915\n",
      "30927\n",
      "30940\n",
      "30962\n",
      "30964\n",
      "30977\n",
      "30979\n",
      "30985\n",
      "30987\n",
      "31014\n",
      "31029\n",
      "31033\n",
      "31043\n",
      "31044\n",
      "31051\n",
      "31054\n",
      "31056\n",
      "31062\n",
      "31073\n",
      "31086\n",
      "31088\n",
      "31093\n",
      "31112\n",
      "31120\n",
      "31122\n",
      "31133\n",
      "31135\n",
      "31136\n",
      "31139\n",
      "31153\n",
      "31160\n",
      "31170\n",
      "31171\n",
      "31183\n",
      "31185\n",
      "31187\n",
      "31190\n",
      "31193\n",
      "31198\n",
      "31199\n",
      "31203\n",
      "31205\n",
      "31206\n",
      "31207\n",
      "31212\n",
      "31226\n",
      "31260\n",
      "31262\n",
      "31275\n",
      "31279\n",
      "31290\n",
      "31301\n",
      "31305\n",
      "31312\n",
      "31320\n",
      "31322\n",
      "31330\n",
      "31332\n",
      "31336\n",
      "31340\n",
      "31361\n",
      "31376\n",
      "31385\n",
      "31439\n",
      "31441\n",
      "31442\n",
      "31451\n",
      "31454\n",
      "31470\n",
      "31489\n",
      "31496\n",
      "31501\n",
      "31502\n",
      "31520\n",
      "31521\n",
      "31564\n",
      "31569\n",
      "31573\n",
      "31574\n",
      "31585\n",
      "31610\n",
      "31641\n",
      "31651\n",
      "31662\n",
      "31664\n",
      "31680\n",
      "31684\n",
      "31692\n",
      "31700\n",
      "31709\n",
      "31712\n",
      "31717\n",
      "31719\n",
      "31755\n",
      "31770\n",
      "31778\n",
      "31779\n",
      "31792\n",
      "31812\n",
      "31842\n",
      "31854\n",
      "31880\n",
      "31881\n",
      "31891\n",
      "31904\n",
      "31910\n",
      "31911\n",
      "31912\n",
      "31928\n",
      "31931\n",
      "31942\n",
      "31949\n",
      "31965\n",
      "31966\n",
      "31969\n",
      "31972\n",
      "31973\n",
      "31977\n",
      "31982\n",
      "31987\n",
      "31994\n",
      "31996\n",
      "32001\n",
      "32005\n",
      "32013\n",
      "32014\n",
      "32023\n",
      "32050\n",
      "32061\n",
      "32074\n",
      "32082\n",
      "32099\n",
      "32104\n",
      "32106\n",
      "32122\n",
      "32125\n",
      "32130\n",
      "32135\n",
      "32140\n",
      "32150\n",
      "32153\n",
      "32154\n",
      "32156\n",
      "32161\n",
      "32177\n",
      "32181\n",
      "32192\n",
      "32193\n",
      "32195\n",
      "32203\n",
      "32234\n",
      "32247\n",
      "32286\n",
      "32287\n",
      "32299\n",
      "32303\n",
      "32317\n",
      "32348\n",
      "32362\n",
      "32367\n",
      "32373\n",
      "32376\n",
      "32377\n",
      "32380\n",
      "32387\n",
      "32396\n",
      "32399\n",
      "32406\n",
      "32410\n",
      "32412\n",
      "32420\n",
      "32421\n",
      "32425\n",
      "32429\n",
      "32434\n",
      "32436\n",
      "32438\n",
      "32440\n",
      "32441\n",
      "32447\n",
      "32453\n",
      "32460\n",
      "32465\n",
      "32466\n",
      "32475\n",
      "32495\n",
      "32500\n",
      "32502\n",
      "32504\n",
      "32511\n",
      "32512\n",
      "32517\n",
      "32527\n",
      "32536\n",
      "32540\n",
      "32548\n",
      "32559\n",
      "32568\n",
      "32572\n",
      "32574\n",
      "32592\n",
      "32605\n",
      "32613\n",
      "32628\n",
      "32637\n",
      "32639\n",
      "32652\n",
      "32658\n",
      "32664\n",
      "32677\n",
      "32679\n",
      "32684\n",
      "32707\n",
      "32733\n",
      "32740\n",
      "32742\n",
      "32743\n",
      "32746\n",
      "32749\n",
      "32775\n",
      "32786\n",
      "32790\n",
      "32791\n",
      "32797\n",
      "32805\n",
      "32809\n",
      "32810\n",
      "40000\n",
      "40059\n",
      "40078\n",
      "40094\n",
      "40120\n",
      "40124\n",
      "40179\n",
      "40310\n",
      "40337\n",
      "40388\n",
      "40435\n",
      "40461\n",
      "40463\n",
      "40464\n",
      "40474\n",
      "40526\n",
      "40546\n",
      "40576\n",
      "40577\n",
      "40695\n",
      "40697\n",
      "40708\n",
      "40729\n",
      "40736\n",
      "40775\n",
      "40822\n",
      "40837\n",
      "40851\n",
      "40883\n",
      "40900\n",
      "40904\n",
      "40911\n",
      "40923\n",
      "40934\n",
      "41024\n",
      "41034\n",
      "41035\n",
      "41077\n",
      "41115\n",
      "41121\n",
      "41182\n",
      "41191\n",
      "41192\n",
      "41216\n",
      "41217\n",
      "41311\n",
      "41350\n",
      "41359\n",
      "41373\n",
      "41442\n",
      "41446\n",
      "41448\n",
      "41515\n",
      "41603\n",
      "41621\n",
      "41639\n",
      "41682\n",
      "41702\n",
      "41719\n",
      "41724\n",
      "41768\n",
      "41795\n",
      "41890\n",
      "41897\n",
      "41902\n",
      "41943\n",
      "41958\n",
      "41966\n",
      "41976\n",
      "42035\n",
      "42054\n",
      "42055\n",
      "42058\n",
      "42067\n",
      "42073\n",
      "42129\n",
      "42135\n",
      "42196\n",
      "42310\n",
      "42327\n",
      "42346\n",
      "42400\n",
      "42410\n",
      "42468\n",
      "42492\n",
      "42525\n",
      "42574\n",
      "42606\n",
      "42683\n",
      "42694\n",
      "42715\n",
      "42763\n",
      "42794\n",
      "42820\n",
      "42851\n",
      "42858\n",
      "42892\n",
      "42919\n",
      "42929\n",
      "42970\n",
      "43067\n",
      "43112\n",
      "43121\n",
      "43122\n",
      "43126\n",
      "43128\n",
      "43147\n",
      "43186\n",
      "43305\n",
      "43320\n",
      "43323\n",
      "43325\n",
      "43439\n",
      "43456\n",
      "43459\n",
      "43472\n",
      "43478\n",
      "43484\n",
      "43501\n",
      "43529\n",
      "43551\n",
      "43561\n",
      "43599\n",
      "43656\n",
      "43673\n",
      "43803\n",
      "43812\n",
      "43881\n",
      "43911\n",
      "43937\n",
      "43943\n",
      "43946\n",
      "43961\n",
      "43982\n",
      "43991\n",
      "44061\n",
      "44064\n",
      "44083\n",
      "44206\n",
      "44232\n",
      "44245\n",
      "44265\n",
      "44277\n",
      "44300\n",
      "44326\n",
      "44408\n",
      "44413\n",
      "44451\n",
      "44486\n",
      "44508\n",
      "44521\n",
      "44530\n",
      "44570\n",
      "44602\n",
      "44622\n",
      "44715\n",
      "44735\n",
      "44741\n",
      "44751\n",
      "44770\n",
      "44787\n",
      "44793\n",
      "44799\n",
      "44841\n",
      "44851\n",
      "44890\n",
      "44941\n",
      "45092\n",
      "45176\n",
      "45213\n",
      "45232\n",
      "45269\n",
      "45280\n",
      "45309\n",
      "45310\n",
      "45321\n",
      "45339\n",
      "45344\n",
      "45395\n",
      "45407\n",
      "45410\n",
      "45426\n",
      "45431\n",
      "45509\n",
      "45542\n",
      "45566\n",
      "45575\n",
      "45576\n",
      "45589\n",
      "45655\n",
      "45736\n",
      "45783\n",
      "45801\n",
      "45910\n",
      "45914\n",
      "45985\n",
      "46034\n",
      "46077\n",
      "46081\n",
      "46105\n",
      "46127\n",
      "46205\n",
      "46228\n",
      "46251\n",
      "46260\n",
      "46287\n",
      "46339\n",
      "46355\n",
      "46373\n",
      "46411\n",
      "46423\n",
      "46467\n",
      "46487\n",
      "46497\n",
      "46521\n",
      "46527\n",
      "46566\n",
      "46569\n",
      "46586\n",
      "46588\n",
      "46733\n",
      "46781\n",
      "46809\n",
      "46815\n",
      "46836\n",
      "46845\n",
      "46851\n",
      "46854\n",
      "46889\n",
      "46904\n",
      "46926\n",
      "46934\n",
      "46943\n",
      "47035\n",
      "47045\n",
      "47118\n",
      "47137\n",
      "47219\n",
      "47226\n",
      "47240\n",
      "47257\n",
      "47271\n",
      "47430\n",
      "47492\n",
      "47556\n",
      "47569\n",
      "47677\n",
      "47683\n",
      "47731\n",
      "47733\n",
      "47747\n",
      "47748\n",
      "47758\n",
      "47775\n",
      "47785\n",
      "47787\n",
      "47790\n",
      "47816\n",
      "47827\n",
      "47906\n",
      "47907\n",
      "47941\n",
      "47956\n",
      "47983\n",
      "48011\n",
      "48025\n",
      "48032\n",
      "48038\n",
      "48118\n",
      "48123\n",
      "48143\n",
      "48159\n",
      "48177\n",
      "48233\n",
      "48292\n",
      "48297\n",
      "48327\n",
      "48340\n",
      "48344\n",
      "48372\n",
      "48388\n",
      "48420\n",
      "48451\n",
      "48453\n",
      "48527\n",
      "48580\n",
      "48693\n",
      "48697\n",
      "48734\n",
      "48779\n",
      "48794\n",
      "48812\n",
      "48821\n",
      "48826\n",
      "48872\n",
      "48885\n",
      "48974\n",
      "48999\n",
      "49019\n",
      "49037\n",
      "49068\n",
      "49081\n",
      "49106\n",
      "49225\n",
      "49292\n",
      "49311\n",
      "49328\n",
      "49380\n",
      "49431\n",
      "49453\n",
      "49500\n",
      "49544\n",
      "49555\n",
      "49580\n",
      "49611\n",
      "49623\n",
      "49634\n",
      "49649\n",
      "49683\n",
      "49840\n",
      "49872\n",
      "49925\n",
      "49952\n",
      "49999\n",
      "50017\n",
      "50041\n",
      "50049\n",
      "50079\n",
      "50087\n",
      "50093\n",
      "50113\n",
      "50140\n",
      "50141\n",
      "50148\n",
      "50237\n",
      "50259\n",
      "50302\n",
      "50315\n",
      "50321\n",
      "50362\n",
      "50391\n",
      "50405\n",
      "50409\n",
      "50440\n",
      "50445\n",
      "50476\n",
      "50507\n",
      "50532\n",
      "50618\n",
      "50623\n",
      "50648\n",
      "50651\n",
      "50710\n",
      "50721\n",
      "50735\n",
      "50744\n",
      "50757\n",
      "50760\n",
      "50761\n",
      "50762\n",
      "50772\n",
      "50807\n",
      "50817\n",
      "50822\n",
      "50859\n",
      "50895\n",
      "50899\n",
      "50976\n",
      "50991\n",
      "51025\n",
      "51027\n",
      "51039\n",
      "51078\n",
      "51145\n",
      "51165\n",
      "51178\n",
      "51180\n",
      "51188\n",
      "51195\n",
      "51203\n",
      "51226\n",
      "51275\n",
      "51327\n",
      "51335\n",
      "51337\n",
      "51349\n",
      "51418\n",
      "51462\n",
      "51482\n",
      "51493\n",
      "51545\n",
      "51555\n",
      "51558\n",
      "51596\n",
      "51628\n",
      "51663\n",
      "51698\n",
      "51724\n",
      "51754\n",
      "51798\n",
      "51821\n",
      "51839\n",
      "51859\n",
      "51891\n",
      "51909\n",
      "51992\n",
      "52021\n",
      "52068\n",
      "52109\n",
      "52125\n",
      "52172\n",
      "52191\n",
      "52260\n",
      "52263\n",
      "52296\n",
      "52307\n",
      "52314\n",
      "52330\n",
      "52350\n",
      "52370\n",
      "52420\n",
      "52452\n",
      "52453\n",
      "52467\n",
      "52482\n",
      "52529\n",
      "52547\n",
      "52566\n",
      "52574\n",
      "52592\n",
      "52593\n",
      "52606\n",
      "52622\n",
      "52641\n",
      "52647\n",
      "52676\n",
      "52697\n",
      "52710\n",
      "52736\n",
      "52779\n",
      "52808\n",
      "52816\n",
      "52828\n",
      "52837\n",
      "52872\n",
      "52875\n",
      "52878\n",
      "52898\n",
      "52899\n",
      "52932\n",
      "52945\n",
      "52952\n",
      "52974\n",
      "52978\n",
      "52996\n",
      "53014\n",
      "53119\n",
      "53176\n",
      "53216\n",
      "53238\n",
      "53247\n",
      "53269\n",
      "53309\n",
      "53355\n",
      "53371\n",
      "53372\n",
      "53377\n",
      "53404\n",
      "53411\n",
      "53432\n",
      "53437\n",
      "53459\n",
      "53464\n",
      "53466\n",
      "53470\n",
      "53492\n",
      "53534\n",
      "53596\n",
      "53600\n",
      "53632\n",
      "53636\n",
      "53650\n",
      "53663\n",
      "53669\n",
      "53677\n",
      "53714\n",
      "53759\n",
      "53775\n",
      "53787\n",
      "53804\n",
      "53850\n",
      "53876\n",
      "53878\n",
      "53896\n",
      "53939\n",
      "54121\n",
      "54177\n",
      "54182\n",
      "54197\n",
      "54241\n",
      "54247\n",
      "54276\n",
      "54289\n",
      "54305\n",
      "54348\n",
      "54353\n",
      "54385\n",
      "54393\n",
      "54406\n",
      "54444\n",
      "54487\n",
      "54523\n",
      "54610\n",
      "54613\n",
      "54636\n",
      "54660\n",
      "54681\n",
      "54708\n",
      "54806\n",
      "54811\n",
      "54825\n",
      "54826\n",
      "54830\n",
      "54832\n",
      "54894\n",
      "54922\n",
      "54935\n",
      "54994\n",
      "55013\n",
      "55027\n",
      "55049\n",
      "55059\n",
      "55094\n",
      "55104\n",
      "55180\n",
      "55186\n",
      "55260\n",
      "55273\n",
      "55281\n",
      "55337\n",
      "55357\n",
      "55423\n",
      "55523\n",
      "55529\n",
      "55559\n",
      "55571\n",
      "55591\n",
      "55597\n",
      "55601\n",
      "55616\n",
      "55672\n",
      "55677\n",
      "55753\n",
      "55783\n",
      "55909\n",
      "55920\n",
      "55921\n",
      "55966\n",
      "55973\n",
      "55992\n",
      "56112\n",
      "56116\n",
      "56128\n",
      "56179\n",
      "56187\n",
      "56201\n",
      "56229\n",
      "56243\n",
      "56269\n",
      "56283\n",
      "56287\n",
      "56289\n",
      "56307\n",
      "56317\n",
      "56391\n",
      "56440\n",
      "56478\n",
      "56490\n",
      "56502\n",
      "56527\n",
      "56552\n",
      "56635\n",
      "56703\n",
      "56757\n",
      "56758\n",
      "56796\n",
      "56798\n",
      "56840\n",
      "56854\n",
      "56855\n",
      "56861\n",
      "57001\n",
      "57051\n",
      "57073\n",
      "57081\n",
      "57091\n",
      "57105\n",
      "57114\n",
      "57138\n",
      "57139\n",
      "57157\n",
      "57172\n",
      "57199\n",
      "57255\n",
      "57288\n",
      "57290\n",
      "57299\n",
      "57321\n",
      "57330\n",
      "57342\n",
      "57412\n",
      "57476\n",
      "57496\n",
      "57599\n",
      "57637\n",
      "57686\n",
      "57734\n",
      "57735\n",
      "57765\n",
      "57806\n",
      "57907\n",
      "57911\n",
      "57935\n",
      "57981\n",
      "57985\n",
      "58008\n",
      "58108\n",
      "58128\n",
      "58134\n",
      "58153\n",
      "58163\n",
      "58242\n",
      "58247\n",
      "58249\n",
      "58264\n",
      "58286\n",
      "58296\n",
      "58308\n",
      "58319\n",
      "58351\n",
      "58377\n",
      "58389\n",
      "58392\n",
      "58416\n",
      "58433\n",
      "58451\n",
      "58501\n",
      "58505\n",
      "58526\n",
      "58528\n",
      "58580\n",
      "58617\n",
      "58732\n",
      "58736\n",
      "58740\n",
      "58747\n",
      "58773\n",
      "58781\n",
      "58782\n",
      "58792\n",
      "58810\n",
      "58821\n",
      "58825\n",
      "58834\n",
      "58854\n",
      "58868\n",
      "58899\n",
      "58917\n",
      "58955\n",
      "58965\n",
      "58993\n",
      "59036\n",
      "59049\n",
      "59067\n",
      "59085\n",
      "59133\n",
      "59158\n",
      "59194\n",
      "59198\n",
      "59210\n",
      "59215\n",
      "59225\n",
      "59311\n",
      "59314\n",
      "59347\n",
      "59375\n",
      "59411\n",
      "59442\n",
      "59448\n",
      "59496\n",
      "59505\n",
      "59507\n",
      "59513\n",
      "59549\n",
      "59618\n",
      "59628\n",
      "59630\n",
      "59657\n",
      "59677\n",
      "59716\n",
      "59720\n",
      "59761\n",
      "59797\n",
      "59801\n",
      "59828\n",
      "59841\n",
      "59845\n",
      "59848\n",
      "59864\n",
      "59875\n",
      "59889\n",
      "59917\n",
      "59924\n",
      "59936\n",
      "59945\n",
      "59960\n",
      "59979\n",
      "59986\n",
      "59991\n",
      "60020\n",
      "60057\n",
      "60104\n",
      "60106\n",
      "60115\n",
      "60142\n",
      "60180\n",
      "60226\n",
      "60274\n",
      "60295\n",
      "60436\n",
      "60518\n",
      "60668\n",
      "60737\n",
      "60773\n",
      "60798\n",
      "60829\n",
      "60842\n",
      "60864\n",
      "60878\n",
      "60977\n",
      "61005\n",
      "61030\n",
      "61076\n",
      "61078\n",
      "61084\n",
      "61157\n",
      "61163\n",
      "61179\n",
      "61182\n",
      "61195\n",
      "61201\n",
      "61223\n",
      "61259\n",
      "61472\n",
      "61500\n",
      "61519\n",
      "61605\n",
      "61620\n",
      "61630\n",
      "61642\n",
      "61667\n",
      "61671\n",
      "61738\n",
      "61802\n",
      "61809\n",
      "61816\n",
      "61825\n",
      "61932\n",
      "61943\n",
      "62032\n",
      "62072\n",
      "62114\n",
      "62183\n",
      "62186\n",
      "62194\n",
      "62239\n",
      "62259\n",
      "62261\n",
      "62284\n",
      "62298\n",
      "62380\n",
      "62389\n",
      "62466\n",
      "62478\n",
      "62479\n",
      "62514\n",
      "62527\n",
      "62543\n",
      "62594\n",
      "62603\n",
      "62608\n",
      "62613\n",
      "62641\n",
      "62646\n",
      "62674\n",
      "62681\n",
      "62691\n",
      "62717\n",
      "62733\n",
      "62734\n",
      "62813\n",
      "62917\n",
      "62925\n",
      "62930\n",
      "63009\n",
      "63039\n",
      "63048\n",
      "63053\n",
      "63058\n",
      "63109\n",
      "63110\n",
      "63116\n",
      "63130\n",
      "63177\n",
      "63201\n",
      "63290\n",
      "63292\n",
      "63320\n",
      "63327\n",
      "63360\n",
      "63364\n",
      "63368\n",
      "63386\n",
      "63415\n",
      "63519\n",
      "63539\n",
      "63552\n",
      "63601\n",
      "63628\n",
      "63637\n",
      "63660\n",
      "63692\n",
      "63701\n",
      "63721\n",
      "63733\n",
      "63734\n",
      "63755\n",
      "63756\n",
      "63875\n",
      "63878\n",
      "63899\n",
      "63943\n",
      "63961\n",
      "63992\n",
      "64026\n",
      "64089\n",
      "64153\n",
      "64160\n",
      "64171\n",
      "64230\n",
      "64274\n",
      "64277\n",
      "64286\n",
      "64296\n",
      "64298\n",
      "64361\n",
      "64411\n",
      "64416\n",
      "64538\n",
      "64550\n",
      "64700\n",
      "64740\n",
      "64752\n",
      "64785\n",
      "64825\n",
      "64873\n",
      "64874\n",
      "64904\n",
      "64906\n",
      "64919\n",
      "64969\n",
      "64988\n",
      "64999\n",
      "65003\n",
      "65004\n",
      "65056\n",
      "65082\n",
      "65164\n",
      "65190\n",
      "65217\n",
      "65267\n",
      "65284\n",
      "65309\n",
      "65351\n",
      "65401\n",
      "65431\n",
      "65449\n",
      "65454\n",
      "65467\n",
      "65481\n",
      "65558\n",
      "65582\n",
      "65594\n",
      "65623\n",
      "65665\n",
      "65703\n",
      "65710\n",
      "65759\n",
      "65779\n",
      "65824\n",
      "65833\n",
      "65861\n",
      "65906\n",
      "65925\n",
      "65954\n",
      "65956\n",
      "65959\n",
      "65973\n",
      "65979\n",
      "65982\n",
      "65994\n",
      "66015\n",
      "66061\n",
      "66079\n",
      "66093\n",
      "66095\n",
      "66151\n",
      "66157\n",
      "66158\n",
      "66172\n",
      "66244\n",
      "66256\n",
      "66264\n",
      "66298\n",
      "66307\n",
      "66338\n",
      "66384\n",
      "66405\n",
      "66412\n",
      "66499\n",
      "66505\n",
      "66507\n",
      "66532\n",
      "66596\n",
      "66604\n",
      "66637\n",
      "66688\n",
      "66706\n",
      "66723\n",
      "66745\n",
      "66761\n",
      "66818\n",
      "66823\n",
      "66825\n",
      "66831\n",
      "66899\n",
      "66919\n",
      "66957\n",
      "66988\n",
      "67017\n",
      "67050\n",
      "67060\n",
      "67067\n",
      "67112\n",
      "67134\n",
      "67150\n",
      "67164\n",
      "67172\n",
      "67209\n",
      "67241\n",
      "67265\n",
      "67281\n",
      "67301\n",
      "67344\n",
      "67348\n",
      "67384\n",
      "67426\n",
      "67446\n",
      "67460\n",
      "67473\n",
      "67481\n",
      "67512\n",
      "67527\n",
      "67579\n",
      "67617\n",
      "67624\n",
      "67683\n",
      "67711\n",
      "67735\n",
      "67803\n",
      "67835\n",
      "67887\n",
      "67906\n",
      "67910\n",
      "67924\n",
      "67956\n",
      "68059\n",
      "68065\n",
      "68089\n",
      "68099\n",
      "68103\n",
      "68123\n",
      "68127\n",
      "68135\n",
      "68140\n",
      "68174\n",
      "68184\n",
      "68186\n",
      "68218\n",
      "68221\n",
      "68251\n",
      "68295\n",
      "68356\n",
      "68389\n",
      "68391\n",
      "68401\n",
      "68433\n",
      "68453\n",
      "68457\n",
      "68564\n",
      "68579\n",
      "68605\n",
      "68699\n",
      "68702\n",
      "68704\n",
      "68764\n",
      "68780\n",
      "68785\n",
      "68867\n",
      "68870\n",
      "68900\n",
      "68905\n",
      "68915\n",
      "68944\n",
      "68946\n",
      "68956\n",
      "69000\n",
      "69011\n",
      "69098\n",
      "69169\n",
      "69194\n",
      "69225\n",
      "69237\n",
      "69271\n",
      "69293\n",
      "69295\n",
      "69323\n",
      "69328\n",
      "69338\n",
      "69344\n",
      "69354\n",
      "69359\n",
      "69398\n",
      "69433\n",
      "69438\n",
      "69447\n",
      "69483\n",
      "69531\n",
      "69578\n",
      "69654\n",
      "69713\n",
      "69745\n",
      "69761\n",
      "69766\n",
      "69776\n",
      "69806\n",
      "69851\n",
      "69855\n",
      "69857\n",
      "69912\n",
      "69995\n",
      "70004\n",
      "70078\n",
      "70080\n",
      "70180\n",
      "70188\n",
      "70191\n",
      "70226\n",
      "70252\n",
      "70267\n",
      "70313\n",
      "70330\n",
      "70355\n",
      "70386\n",
      "70445\n",
      "70469\n",
      "70476\n",
      "70485\n",
      "70494\n",
      "70500\n",
      "70516\n",
      "70521\n",
      "70558\n",
      "70563\n",
      "70587\n",
      "70645\n",
      "70709\n",
      "70728\n",
      "70745\n",
      "70772\n",
      "70807\n",
      "70842\n",
      "70851\n",
      "70854\n",
      "70872\n",
      "70886\n",
      "70890\n",
      "70907\n",
      "70937\n",
      "70989\n",
      "71012\n",
      "71083\n",
      "71084\n",
      "71107\n",
      "71108\n",
      "71130\n",
      "71146\n",
      "71177\n",
      "71184\n",
      "71190\n",
      "71192\n",
      "71194\n",
      "71220\n",
      "71244\n",
      "71277\n",
      "71336\n",
      "71377\n",
      "71397\n",
      "71527\n",
      "71645\n",
      "71774\n",
      "71825\n",
      "71848\n",
      "71871\n",
      "71878\n",
      "71962\n",
      "71986\n",
      "72000\n",
      "72033\n",
      "72043\n",
      "72073\n",
      "72083\n",
      "72147\n",
      "72163\n",
      "72196\n",
      "72197\n",
      "72230\n",
      "72233\n",
      "72308\n",
      "72329\n",
      "72354\n",
      "72421\n",
      "72447\n",
      "72482\n",
      "72536\n",
      "72541\n",
      "72545\n",
      "72582\n",
      "72678\n",
      "72707\n",
      "72714\n",
      "72725\n",
      "72753\n",
      "72790\n",
      "72844\n",
      "72847\n",
      "72885\n",
      "72930\n",
      "72940\n",
      "72969\n",
      "72978\n",
      "72992\n",
      "73043\n",
      "73058\n",
      "73068\n",
      "73129\n",
      "73143\n",
      "73186\n",
      "73200\n",
      "73206\n",
      "73211\n",
      "73242\n",
      "73249\n",
      "73322\n",
      "73358\n",
      "73370\n",
      "73385\n",
      "73409\n",
      "73454\n",
      "73488\n",
      "73565\n",
      "73572\n",
      "73575\n",
      "73612\n",
      "73645\n",
      "73648\n",
      "73686\n",
      "73713\n",
      "73755\n",
      "73770\n",
      "73811\n",
      "73941\n",
      "73946\n",
      "73973\n",
      "73979\n",
      "74001\n",
      "74002\n",
      "74016\n",
      "74032\n",
      "74046\n",
      "74081\n",
      "74144\n",
      "74220\n",
      "74223\n",
      "74232\n",
      "74252\n",
      "74289\n",
      "74319\n",
      "74346\n",
      "74376\n",
      "74408\n",
      "74426\n",
      "74444\n",
      "74456\n",
      "74463\n",
      "74482\n",
      "74514\n",
      "74533\n",
      "74554\n",
      "74562\n",
      "74626\n",
      "74627\n",
      "74630\n",
      "74632\n",
      "74674\n",
      "74677\n",
      "74687\n",
      "74693\n",
      "74711\n",
      "74725\n",
      "74816\n",
      "74817\n",
      "74821\n",
      "74835\n",
      "74857\n",
      "74894\n",
      "74924\n",
      "74935\n",
      "74955\n",
      "74976\n",
      "75027\n",
      "75071\n",
      "75100\n",
      "75155\n",
      "75188\n",
      "75194\n",
      "75201\n",
      "75223\n",
      "75249\n",
      "75251\n",
      "75300\n",
      "75305\n",
      "75315\n",
      "75320\n",
      "75335\n",
      "75420\n",
      "75451\n",
      "75454\n",
      "75493\n",
      "75500\n",
      "75509\n",
      "75510\n",
      "75525\n",
      "75536\n",
      "75557\n",
      "75586\n",
      "75609\n",
      "75631\n",
      "75658\n",
      "75666\n",
      "75668\n",
      "75741\n",
      "75772\n",
      "75782\n",
      "75795\n",
      "75796\n",
      "75824\n",
      "75867\n",
      "75883\n",
      "75899\n",
      "76001\n",
      "76005\n",
      "76012\n",
      "76028\n",
      "76056\n",
      "76134\n",
      "76151\n",
      "76165\n",
      "76174\n",
      "76186\n",
      "76193\n",
      "76196\n",
      "76222\n",
      "76240\n",
      "76251\n",
      "76261\n",
      "76282\n",
      "76327\n",
      "76418\n",
      "76435\n",
      "76446\n",
      "76459\n",
      "76476\n",
      "76477\n",
      "76479\n",
      "76529\n",
      "76558\n",
      "76580\n",
      "76594\n",
      "76602\n",
      "76652\n",
      "76654\n",
      "76695\n",
      "76698\n",
      "76732\n",
      "76760\n",
      "76780\n",
      "76782\n",
      "76797\n",
      "76800\n",
      "76803\n",
      "76835\n",
      "76853\n",
      "76930\n",
      "76994\n",
      "77019\n",
      "77037\n",
      "77053\n",
      "77129\n",
      "77135\n",
      "77149\n",
      "77163\n",
      "77220\n",
      "77259\n",
      "77265\n",
      "77280\n",
      "77330\n",
      "77341\n",
      "77383\n",
      "77395\n",
      "77413\n",
      "77427\n",
      "77436\n",
      "77443\n",
      "77469\n",
      "77471\n",
      "77480\n",
      "77484\n",
      "77502\n",
      "77511\n",
      "77527\n",
      "77614\n",
      "77660\n",
      "77661\n",
      "77673\n",
      "77676\n",
      "77746\n",
      "77771\n",
      "77815\n",
      "77816\n",
      "77836\n",
      "77842\n",
      "77850\n",
      "77900\n",
      "77924\n",
      "77947\n",
      "77951\n",
      "77980\n",
      "77987\n",
      "78007\n",
      "78076\n",
      "78100\n",
      "78215\n",
      "78238\n",
      "78318\n",
      "78342\n",
      "78388\n",
      "78415\n",
      "78419\n",
      "78454\n",
      "78481\n",
      "78504\n",
      "78506\n",
      "78536\n",
      "78565\n",
      "78641\n",
      "78685\n",
      "78701\n",
      "78704\n",
      "78705\n",
      "78708\n",
      "78716\n",
      "78782\n",
      "78814\n",
      "78895\n",
      "78966\n",
      "79015\n",
      "79038\n",
      "79050\n",
      "79051\n",
      "79090\n",
      "79126\n",
      "79262\n",
      "79288\n",
      "79294\n",
      "79348\n",
      "79556\n",
      "79576\n",
      "79585\n",
      "79602\n",
      "79645\n",
      "79651\n",
      "79709\n",
      "79735\n",
      "79795\n",
      "79804\n",
      "79836\n",
      "79851\n",
      "79873\n",
      "79900\n",
      "79923\n",
      "79931\n",
      "79977\n",
      "79998\n",
      "80015\n",
      "80041\n",
      "80045\n",
      "80046\n",
      "80081\n",
      "80105\n",
      "80106\n",
      "80111\n",
      "80163\n",
      "80237\n",
      "80260\n",
      "80274\n",
      "80286\n",
      "80287\n",
      "80298\n",
      "80305\n",
      "80308\n",
      "80313\n",
      "80342\n",
      "80350\n",
      "80369\n",
      "80423\n",
      "80490\n",
      "80511\n",
      "80536\n",
      "80547\n",
      "80603\n",
      "80628\n",
      "80658\n",
      "80778\n",
      "80789\n",
      "80791\n",
      "80799\n",
      "80805\n",
      "80847\n",
      "80927\n",
      "80956\n",
      "80982\n",
      "81020\n",
      "81025\n",
      "81041\n",
      "81096\n",
      "81114\n",
      "81122\n",
      "81146\n",
      "81154\n",
      "81157\n",
      "81202\n",
      "81209\n",
      "81229\n",
      "81232\n",
      "81233\n",
      "81245\n",
      "81247\n",
      "81317\n",
      "81342\n",
      "81349\n",
      "81362\n",
      "81387\n",
      "81425\n",
      "81443\n",
      "81444\n",
      "81456\n",
      "81464\n",
      "81491\n",
      "81515\n",
      "81529\n",
      "81543\n",
      "81545\n",
      "81560\n",
      "81592\n",
      "81608\n",
      "81660\n",
      "81662\n",
      "81692\n",
      "81715\n",
      "81754\n",
      "81778\n",
      "81807\n",
      "81810\n",
      "81846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81850\n",
      "81865\n",
      "81866\n",
      "81893\n",
      "81904\n",
      "81926\n",
      "81933\n",
      "81958\n",
      "82003\n",
      "82038\n",
      "82065\n",
      "82073\n",
      "82091\n",
      "82100\n",
      "82104\n",
      "82150\n",
      "82195\n",
      "82202\n",
      "82208\n",
      "82217\n",
      "82229\n",
      "82231\n",
      "82377\n",
      "82393\n",
      "82405\n",
      "82433\n",
      "82494\n",
      "82551\n",
      "82575\n",
      "82609\n",
      "82649\n",
      "82681\n",
      "82685\n",
      "82810\n",
      "82843\n",
      "82846\n",
      "82950\n",
      "82973\n",
      "83013\n",
      "83057\n",
      "83079\n",
      "83120\n",
      "83128\n",
      "83156\n",
      "83180\n",
      "83182\n",
      "83204\n",
      "83210\n",
      "83255\n",
      "83301\n",
      "83361\n",
      "83393\n",
      "83401\n",
      "83433\n",
      "83466\n",
      "83509\n",
      "83527\n",
      "83532\n",
      "83550\n",
      "83564\n",
      "83603\n",
      "83607\n",
      "83608\n",
      "83653\n",
      "83691\n",
      "83700\n",
      "83751\n",
      "83892\n",
      "83901\n",
      "83932\n",
      "83937\n",
      "84107\n",
      "84223\n",
      "84306\n",
      "84310\n",
      "84350\n",
      "84421\n",
      "84450\n",
      "84461\n",
      "84463\n",
      "84465\n",
      "84467\n",
      "84469\n",
      "84519\n",
      "84534\n",
      "84595\n",
      "84600\n",
      "84615\n",
      "84624\n",
      "84633\n",
      "84669\n",
      "84687\n",
      "84845\n",
      "84874\n",
      "84884\n",
      "84909\n",
      "84914\n",
      "84966\n",
      "84983\n",
      "85036\n",
      "85050\n",
      "85079\n",
      "85131\n",
      "85138\n",
      "85163\n",
      "85195\n",
      "85255\n",
      "85258\n",
      "85264\n",
      "85327\n",
      "85350\n",
      "85402\n",
      "85417\n",
      "85441\n",
      "85457\n",
      "85479\n",
      "85489\n",
      "85490\n",
      "85539\n",
      "85639\n",
      "85685\n",
      "85700\n",
      "85704\n",
      "85723\n",
      "85740\n",
      "85767\n",
      "85828\n",
      "85842\n",
      "85866\n",
      "85870\n",
      "85899\n",
      "85929\n",
      "85999\n",
      "86018\n",
      "86086\n",
      "86089\n",
      "86146\n",
      "86158\n",
      "86193\n",
      "86209\n",
      "86276\n",
      "86279\n",
      "86317\n",
      "86393\n",
      "86530\n",
      "86561\n",
      "86628\n",
      "86645\n",
      "86648\n",
      "86663\n",
      "86684\n",
      "86709\n",
      "86711\n",
      "86717\n",
      "86719\n",
      "86773\n",
      "86805\n",
      "86810\n",
      "86824\n",
      "86831\n",
      "86845\n",
      "86866\n",
      "86879\n",
      "86934\n",
      "86939\n",
      "86961\n",
      "87053\n",
      "87055\n",
      "87160\n",
      "87257\n",
      "87266\n",
      "87275\n",
      "87284\n",
      "87291\n",
      "87348\n",
      "87352\n",
      "87410\n",
      "87470\n",
      "87621\n",
      "87687\n",
      "87704\n",
      "87869\n",
      "87880\n",
      "87926\n",
      "87936\n",
      "87948\n",
      "87962\n",
      "87963\n",
      "87978\n",
      "87980\n",
      "87982\n",
      "87986\n",
      "88008\n",
      "88009\n",
      "88018\n",
      "88065\n",
      "88079\n",
      "88091\n",
      "88099\n",
      "88163\n",
      "88164\n",
      "88206\n",
      "88214\n",
      "88254\n",
      "88265\n",
      "88306\n",
      "88325\n",
      "88343\n",
      "88356\n",
      "88432\n",
      "88446\n",
      "88452\n",
      "88508\n",
      "88523\n",
      "88635\n",
      "88636\n",
      "88647\n",
      "88685\n",
      "88711\n",
      "88733\n",
      "88765\n",
      "88782\n",
      "88832\n",
      "88857\n",
      "88882\n",
      "89040\n",
      "89092\n",
      "89095\n",
      "89100\n",
      "89103\n",
      "89119\n",
      "89124\n",
      "89277\n",
      "89290\n",
      "89329\n",
      "89347\n",
      "89356\n",
      "89419\n",
      "89447\n",
      "89493\n",
      "89502\n",
      "89536\n",
      "89556\n",
      "89616\n",
      "89633\n",
      "89735\n",
      "89754\n",
      "89797\n",
      "89799\n",
      "89817\n",
      "89840\n",
      "89927\n",
      "89992\n",
      "90020\n",
      "90021\n",
      "90026\n",
      "90033\n",
      "90044\n",
      "90122\n",
      "90143\n",
      "90165\n",
      "90317\n",
      "90354\n",
      "90365\n",
      "90369\n",
      "90392\n",
      "90396\n",
      "90403\n",
      "90418\n",
      "90441\n",
      "90455\n",
      "90460\n",
      "90508\n",
      "90522\n",
      "90533\n",
      "90538\n",
      "90546\n",
      "90560\n",
      "90626\n",
      "90658\n",
      "90676\n",
      "90688\n",
      "90699\n",
      "90720\n",
      "90725\n",
      "90755\n",
      "90802\n",
      "90805\n",
      "90814\n",
      "90834\n",
      "90890\n",
      "90902\n",
      "90954\n",
      "90990\n",
      "91024\n",
      "91090\n",
      "91103\n",
      "91143\n",
      "91150\n",
      "91158\n",
      "91199\n",
      "91200\n",
      "91242\n",
      "91245\n",
      "91327\n",
      "91451\n",
      "91465\n",
      "91511\n",
      "91513\n",
      "91525\n",
      "91529\n",
      "91548\n",
      "91551\n",
      "91561\n",
      "91588\n",
      "91601\n",
      "91616\n",
      "91669\n",
      "91705\n",
      "91730\n",
      "91738\n",
      "91773\n",
      "91802\n",
      "91827\n",
      "91838\n",
      "91842\n",
      "91855\n",
      "91862\n",
      "91872\n",
      "91934\n",
      "91995\n",
      "92063\n",
      "92066\n",
      "92117\n",
      "92135\n",
      "92137\n",
      "92175\n",
      "92195\n",
      "92212\n",
      "92277\n",
      "92281\n",
      "92287\n",
      "92326\n",
      "92331\n",
      "92373\n",
      "92393\n",
      "92410\n",
      "92420\n",
      "92438\n",
      "92525\n",
      "92551\n",
      "92589\n",
      "92644\n",
      "92651\n",
      "92668\n",
      "92757\n",
      "92796\n",
      "92801\n",
      "92820\n",
      "92843\n",
      "92873\n",
      "93018\n",
      "93025\n",
      "93026\n",
      "93039\n",
      "93054\n",
      "93078\n",
      "93113\n",
      "93119\n",
      "93187\n",
      "93191\n",
      "93203\n",
      "93233\n",
      "93272\n",
      "93277\n",
      "93284\n",
      "93325\n",
      "93336\n",
      "93360\n",
      "93381\n",
      "93387\n",
      "93422\n",
      "93432\n",
      "93459\n",
      "93475\n",
      "93506\n",
      "93517\n",
      "93535\n",
      "93560\n",
      "93564\n",
      "93571\n",
      "93593\n",
      "93602\n",
      "93610\n",
      "93632\n",
      "93648\n",
      "93659\n",
      "93663\n",
      "93679\n",
      "93701\n",
      "93705\n",
      "93722\n",
      "93799\n",
      "93806\n",
      "93836\n",
      "93838\n",
      "93870\n",
      "93900\n",
      "93924\n",
      "93932\n",
      "93950\n",
      "93970\n",
      "94016\n",
      "94021\n",
      "94046\n",
      "94049\n",
      "94091\n",
      "94105\n",
      "94117\n",
      "94241\n",
      "94256\n",
      "94264\n",
      "94297\n",
      "94312\n",
      "94329\n",
      "94430\n",
      "94525\n",
      "94530\n",
      "94574\n",
      "94575\n",
      "94581\n",
      "94597\n",
      "94611\n",
      "94696\n",
      "94698\n",
      "94762\n",
      "94785\n",
      "94817\n",
      "94821\n",
      "94828\n",
      "94853\n",
      "94872\n",
      "94924\n",
      "94937\n",
      "94962\n",
      "94977\n",
      "95000\n",
      "95057\n",
      "95076\n",
      "95100\n",
      "95118\n",
      "95122\n",
      "95136\n",
      "95182\n",
      "95238\n",
      "95312\n",
      "95372\n",
      "95377\n",
      "95380\n",
      "95427\n",
      "95447\n",
      "95523\n",
      "95561\n",
      "95637\n",
      "95705\n",
      "95708\n",
      "95725\n",
      "95759\n",
      "95765\n",
      "95782\n",
      "95816\n",
      "95821\n",
      "95830\n",
      "95834\n",
      "95895\n",
      "95909\n",
      "95925\n",
      "96025\n",
      "96040\n",
      "96100\n",
      "96137\n",
      "96145\n",
      "96148\n",
      "96177\n",
      "96199\n",
      "96212\n",
      "96218\n",
      "96234\n",
      "96240\n",
      "96259\n",
      "96260\n",
      "96371\n",
      "96391\n",
      "96404\n",
      "96429\n",
      "96442\n",
      "96456\n",
      "96468\n",
      "96491\n",
      "96577\n",
      "96651\n",
      "96666\n",
      "96677\n",
      "96686\n",
      "96719\n",
      "96731\n",
      "96734\n",
      "96741\n",
      "96767\n",
      "96774\n",
      "96777\n",
      "96791\n",
      "96793\n",
      "96810\n",
      "96848\n",
      "96908\n",
      "96924\n",
      "96928\n",
      "96950\n",
      "96958\n",
      "96962\n",
      "96965\n",
      "96984\n",
      "96997\n",
      "97020\n",
      "97023\n",
      "97037\n",
      "97042\n",
      "97070\n",
      "97178\n",
      "97181\n",
      "97206\n",
      "97212\n",
      "97307\n",
      "97353\n",
      "97421\n",
      "97441\n",
      "97488\n",
      "97529\n",
      "97547\n",
      "97564\n",
      "97567\n",
      "97569\n",
      "97714\n",
      "97830\n",
      "97842\n",
      "97902\n",
      "97916\n",
      "97917\n",
      "97924\n",
      "97942\n",
      "97984\n",
      "98050\n",
      "98103\n",
      "98130\n",
      "98169\n",
      "98220\n",
      "98249\n",
      "98268\n",
      "98276\n",
      "98279\n",
      "98336\n",
      "98342\n",
      "98347\n",
      "98385\n",
      "98481\n",
      "98494\n",
      "98518\n",
      "98554\n",
      "98593\n",
      "98666\n",
      "98744\n",
      "98759\n",
      "98813\n",
      "98829\n",
      "98853\n",
      "98864\n",
      "98887\n",
      "98920\n",
      "98930\n",
      "98931\n",
      "98973\n",
      "98994\n",
      "99054\n",
      "99085\n",
      "99096\n",
      "99100\n",
      "99102\n",
      "99120\n",
      "99166\n",
      "99178\n",
      "99185\n",
      "99231\n",
      "99260\n",
      "99268\n",
      "99299\n",
      "99312\n",
      "99322\n",
      "99339\n",
      "99346\n",
      "99361\n",
      "99383\n",
      "99384\n",
      "99408\n",
      "99439\n",
      "99464\n",
      "99469\n",
      "99538\n",
      "99556\n",
      "99562\n",
      "99613\n",
      "99647\n",
      "99650\n",
      "99660\n",
      "99712\n",
      "99756\n",
      "99781\n",
      "99783\n",
      "99822\n",
      "99883\n",
      "99897\n",
      "99923\n",
      "99982\n"
     ]
    }
   ],
   "source": [
    "for g in p:\n",
    "    if g[1]!=98:\n",
    "        print(g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'SUBJECT_ID', u'TimeStamp', u'Albumin', u'Alk. Phosphate', u'ALT',\n",
       "       u'AST', u'Total Bili', u'BUN', u'Cholesterol', u'Creatinine',\n",
       "       u'Arterial BP [Diastolic]', u'FiO2 Set', u'GCS Total', u'Glucose',\n",
       "       u'HCO3', u'Hematocrit', u'Heart Rate', u'Potassium', u'Lactic Acid',\n",
       "       u'Arterial BP Mean', u'MechVent', u'Magnesium', u'Sodium', u'NIDiasABP',\n",
       "       u'NIMAP', u'NISysABP', u'Arterial PaCO2', u'Arterial PaO2',\n",
       "       u'Arterial pH', u'Platelets', u'Respiratory Rate', u'SaO2',\n",
       "       u'Arterial BP [Systolic]', u'Temperature C', u'TroponinI', u'TroponinT',\n",
       "       u'Urine', u'WBC', u'Previous WeightF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.02173913,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.04007561,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.04007561,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.79032258, 0.        , 0.00204374, ..., 0.        , 0.01587902,\n",
       "        0.        ],\n",
       "       [0.79032258, 0.        , 0.00204374, ..., 0.        , 0.01039698,\n",
       "        0.        ],\n",
       "       [0.79032258, 0.        , 0.00204374, ..., 0.        , 0.01342155,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix3D = np.array(df.drop(['SUBJECT_ID', 'TimeStamp'], 1))\n",
    "matrix3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'SUBJECT_ID', u'TimeStamp', u'Albumin', u'Alk. Phosphate', u'ALT',\n",
       "       u'AST', u'Total Bili', u'BUN', u'Cholesterol', u'Creatinine',\n",
       "       u'Arterial BP [Diastolic]', u'FiO2 Set', u'GCS Total', u'Glucose',\n",
       "       u'HCO3', u'Hematocrit', u'Heart Rate', u'Potassium', u'Lactic Acid',\n",
       "       u'Arterial BP Mean', u'MechVent', u'Magnesium', u'Sodium', u'NIDiasABP',\n",
       "       u'NIMAP', u'NISysABP', u'Arterial PaCO2', u'Arterial PaO2',\n",
       "       u'Arterial pH', u'Platelets', u'Respiratory Rate', u'SaO2',\n",
       "       u'Arterial BP [Systolic]', u'Temperature C', u'TroponinI', u'TroponinT',\n",
       "       u'Urine', u'WBC', u'Previous WeightF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(322763, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6587, 49, 37)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix3D = np.array(matrix3D).reshape((6587, 49, 37))\n",
    "matrix3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Albumin', u'Alk. Phosphate', u'ALT', u'AST', u'Total Bili', u'BUN',\n",
       "       u'Cholesterol', u'Creatinine', u'Arterial BP [Diastolic]', u'FiO2 Set',\n",
       "       u'GCS Total', u'Glucose', u'HCO3', u'Hematocrit', u'Heart Rate',\n",
       "       u'Potassium', u'Lactic Acid', u'Arterial BP Mean', u'MechVent',\n",
       "       u'Magnesium', u'Sodium', u'NIDiasABP', u'NIMAP', u'NISysABP',\n",
       "       u'Arterial PaCO2', u'Arterial PaO2', u'Arterial pH', u'Platelets',\n",
       "       u'Respiratory Rate', u'SaO2', u'Arterial BP [Systolic]',\n",
       "       u'Temperature C', u'TroponinI', u'TroponinT', u'Urine', u'WBC',\n",
       "       u'Previous WeightF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = df.columns[2:]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "21\n",
      "22\n",
      "23\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = []\n",
    "for k in range(37):\n",
    "    h = (sum(sum(matrix3D[:,:,k])))\n",
    "    if int(h)==0:\n",
    "        g.append(False)\n",
    "        print(k)\n",
    "    else:\n",
    "        g.append(True)\n",
    "g = np.array(g)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delCol = [18, 21, 22, 23, 32, 33, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 21, 22, 23, 32, 33, 34]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'MechVent', u'NIDiasABP', u'NIMAP', u'NISysABP', u'TroponinI',\n",
       "       u'TroponinT', u'Urine'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[delCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6587, 49, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix3D = matrix3D[:,:,g]\n",
    "matrix3D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  LABEL\n",
       "0          17      0\n",
       "1          21      0\n",
       "2          23      0\n",
       "3          34      0\n",
       "4          36      1\n",
       "5          61      0\n",
       "6          68      1\n",
       "7          85      0\n",
       "8          94      0\n",
       "9         103      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes = pd.read_csv('../finalset/outcomes.csv')\n",
    "outcomes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6587, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array(outcomes.drop(['SUBJECT_ID'], 1))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6587, 49, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = matrix3D\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_readmit = outcomes['LABEL']==1\n",
    "#df_readmit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_readmit =df_readmit.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_readmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_readmit = X[df_readmit]\n",
    "#X_readmit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_readmit = np.repeat(X_readmit,1,axis=0)\n",
    "#X_readmit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.concatenate((X,X_readmit))\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_readmit = Y[df_readmit]\n",
    "#Y_readmit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_readmit = np.repeat(Y_readmit,1,axis=0)\n",
    "#Y_readmit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = np.concatenate((Y,Y_readmit))\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new = X.reshape(X.shape[0],1,X.shape[1],X.shape[2])\n",
    "#X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_new = Y.reshape(Y.shape[0],1,Y.shape[1])\n",
    "#print(Y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new = np.insert(X_new, 1, Y_new, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.shuffle(X_new)\n",
    "#X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X_new[:,0]\n",
    "\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = X_new[:,1]\n",
    "#Y = Y[:,0]\n",
    "#Y = Y[:,0]\n",
    "#Y = Y.reshape(Y.shape[0],1)\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13174, 49, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X,X))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13174, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.concatenate((Y,Y))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cfde2410bbc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X,X))\n",
    "X = np.concatenate((X,X))\n",
    "X = np.concatenate((X,X))\n",
    "display(X.shape)\n",
    "Y = np.concatenate((Y,Y))\n",
    "Y = np.concatenate((Y,Y))\n",
    "Y = np.concatenate((Y,Y))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105392, 1, 98, 30)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X.reshape(X.shape[0],1,X.shape[1],X.shape[2])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105392, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_new = Y.reshape(Y.shape[0],1,Y.shape[1])\n",
    "print(Y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105392, 2, 98, 30)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.insert(X_new, 1, Y_new, axis=1)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105392, 2, 98, 30)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(X_new)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105392, 98, 30)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_new[:,0]\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105392, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = X_new[:,1]\n",
    "Y = Y[:,0]\n",
    "Y = Y[:,0]\n",
    "Y = Y.reshape(Y.shape[0],1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 98, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10392, 98, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(95000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10392, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = X[:95000]\n",
    "X_test = X[95000:]\n",
    "Y_train = Y[:95000]\n",
    "Y_test = Y[95000:]\n",
    "display(X_train.shape)\n",
    "display(X_test.shape)\n",
    "display(Y_train.shape)\n",
    "display(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35461.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3915.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout,Bidirectional,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 98, 100)           52400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 70,396\n",
      "Trainable params: 70,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 7696 samples\n",
      "Epoch 1/30\n",
      "45000/45000 [==============================] - 65s 1ms/step - loss: 0.2278 - acc: 0.6368 - val_loss: 0.2236 - val_acc: 0.6446\n",
      "Epoch 2/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2258 - acc: 0.6405 - val_loss: 0.2232 - val_acc: 0.6460\n",
      "Epoch 3/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2252 - acc: 0.6402 - val_loss: 0.2225 - val_acc: 0.6453\n",
      "Epoch 4/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2247 - acc: 0.6420 - val_loss: 0.2225 - val_acc: 0.6472\n",
      "Epoch 5/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2238 - acc: 0.6435 - val_loss: 0.2206 - val_acc: 0.6476\n",
      "Epoch 6/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2246 - acc: 0.6417 - val_loss: 0.2207 - val_acc: 0.6522\n",
      "Epoch 7/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2245 - acc: 0.6430 - val_loss: 0.2200 - val_acc: 0.6548\n",
      "Epoch 8/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2224 - acc: 0.6474 - val_loss: 0.2225 - val_acc: 0.6499\n",
      "Epoch 9/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2228 - acc: 0.6463 - val_loss: 0.2205 - val_acc: 0.6503\n",
      "Epoch 10/30\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2220 - acc: 0.6482 - val_loss: 0.2196 - val_acc: 0.6567\n",
      "Epoch 11/30\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2211 - acc: 0.6482 - val_loss: 0.2192 - val_acc: 0.6581\n",
      "Epoch 12/30\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2195 - acc: 0.6542 - val_loss: 0.2181 - val_acc: 0.6566\n",
      "Epoch 13/30\n",
      "45000/45000 [==============================] - 66s 1ms/step - loss: 0.2200 - acc: 0.6529 - val_loss: 0.2172 - val_acc: 0.6600\n",
      "Epoch 14/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2191 - acc: 0.6547 - val_loss: 0.2203 - val_acc: 0.6479\n",
      "Epoch 15/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2183 - acc: 0.6553 - val_loss: 0.2191 - val_acc: 0.6536\n",
      "Epoch 16/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2181 - acc: 0.6540 - val_loss: 0.2172 - val_acc: 0.6576\n",
      "Epoch 17/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2165 - acc: 0.6557 - val_loss: 0.2170 - val_acc: 0.6551\n",
      "Epoch 18/30\n",
      "45000/45000 [==============================] - 66s 1ms/step - loss: 0.2165 - acc: 0.6577 - val_loss: 0.2155 - val_acc: 0.6597\n",
      "Epoch 19/30\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2156 - acc: 0.6600 - val_loss: 0.2254 - val_acc: 0.6496\n",
      "Epoch 20/30\n",
      "45000/45000 [==============================] - 65s 1ms/step - loss: 0.2177 - acc: 0.6529 - val_loss: 0.2144 - val_acc: 0.6606\n",
      "Epoch 21/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2138 - acc: 0.6605 - val_loss: 0.2147 - val_acc: 0.6620\n",
      "Epoch 22/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2144 - acc: 0.6601 - val_loss: 0.2138 - val_acc: 0.6674\n",
      "Epoch 23/30\n",
      "45000/45000 [==============================] - 68s 2ms/step - loss: 0.2111 - acc: 0.6668 - val_loss: 0.2117 - val_acc: 0.6639\n",
      "Epoch 24/30\n",
      "45000/45000 [==============================] - 69s 2ms/step - loss: 0.2109 - acc: 0.6680 - val_loss: 0.2178 - val_acc: 0.6587\n",
      "Epoch 25/30\n",
      "45000/45000 [==============================] - 66s 1ms/step - loss: 0.2140 - acc: 0.6632 - val_loss: 0.2088 - val_acc: 0.6736\n",
      "Epoch 26/30\n",
      "45000/45000 [==============================] - 61s 1ms/step - loss: 0.2089 - acc: 0.6722 - val_loss: 0.2083 - val_acc: 0.6762\n",
      "Epoch 27/30\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.2048 - acc: 0.6792 - val_loss: 0.2109 - val_acc: 0.6709\n",
      "Epoch 28/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2025 - acc: 0.6846 - val_loss: 0.2039 - val_acc: 0.6858\n",
      "Epoch 29/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2007 - acc: 0.6886 - val_loss: 0.2036 - val_acc: 0.6856\n",
      "Epoch 30/30\n",
      "45000/45000 [==============================] - 63s 1ms/step - loss: 0.2008 - acc: 0.6868 - val_loss: 0.1986 - val_acc: 0.6926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa708721c50>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "model.add(LSTM(25))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "model.add(Dense(100,activation='tanh'))\n",
    "model.add(Dense(25,activation='tanh'))\n",
    "model.add(Dense(10,activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30,batch_size=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7696/7696 [==============================] - 5s 653us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.25675675675676"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 29s 655us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.9688888888889"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, Y_train)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 98, 100)           52400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 70,396\n",
      "Trainable params: 70,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 55s 576us/step - loss: 0.2281 - acc: 0.6370 - val_loss: 0.2257 - val_acc: 0.6378\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 59s 622us/step - loss: 0.2246 - acc: 0.6432 - val_loss: 0.2252 - val_acc: 0.6417\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 58s 616us/step - loss: 0.2231 - acc: 0.6457 - val_loss: 0.2247 - val_acc: 0.6387\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 59s 624us/step - loss: 0.2212 - acc: 0.6508 - val_loss: 0.2218 - val_acc: 0.6488\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 57s 601us/step - loss: 0.2196 - acc: 0.6526 - val_loss: 0.2222 - val_acc: 0.6463\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 58s 611us/step - loss: 0.2182 - acc: 0.6562 - val_loss: 0.2186 - val_acc: 0.6525\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 60s 629us/step - loss: 0.2166 - acc: 0.6591 - val_loss: 0.2183 - val_acc: 0.6554\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 59s 618us/step - loss: 0.2186 - acc: 0.6574 - val_loss: 0.2193 - val_acc: 0.6507\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 57s 597us/step - loss: 0.2149 - acc: 0.6640 - val_loss: 0.2166 - val_acc: 0.6583\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 59s 620us/step - loss: 0.2129 - acc: 0.6660 - val_loss: 0.2139 - val_acc: 0.6605\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 59s 623us/step - loss: 0.2118 - acc: 0.6674 - val_loss: 0.2165 - val_acc: 0.6576\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 60s 634us/step - loss: 0.2113 - acc: 0.6679 - val_loss: 0.2145 - val_acc: 0.6643\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 59s 617us/step - loss: 0.2106 - acc: 0.6688 - val_loss: 0.2137 - val_acc: 0.6603\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 58s 609us/step - loss: 0.2089 - acc: 0.6719 - val_loss: 0.2087 - val_acc: 0.6713\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 56s 593us/step - loss: 0.2068 - acc: 0.6759 - val_loss: 0.2109 - val_acc: 0.6671\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 58s 613us/step - loss: 0.2041 - acc: 0.6814 - val_loss: 0.2087 - val_acc: 0.6681\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 57s 595us/step - loss: 0.1990 - acc: 0.6897 - val_loss: 0.2057 - val_acc: 0.6724\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 52s 549us/step - loss: 0.1974 - acc: 0.6936 - val_loss: 0.1964 - val_acc: 0.6920\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 54s 574us/step - loss: 0.2004 - acc: 0.6898 - val_loss: 0.2042 - val_acc: 0.6820\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 55s 576us/step - loss: 0.1933 - acc: 0.7000 - val_loss: 0.1987 - val_acc: 0.6863\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 55s 577us/step - loss: 0.1906 - acc: 0.7070 - val_loss: 0.1909 - val_acc: 0.7048\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 56s 592us/step - loss: 0.1855 - acc: 0.7144 - val_loss: 0.1858 - val_acc: 0.7157\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 59s 623us/step - loss: 0.1844 - acc: 0.7175 - val_loss: 0.1879 - val_acc: 0.7141\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 57s 601us/step - loss: 0.1777 - acc: 0.7279 - val_loss: 0.1803 - val_acc: 0.7230\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 58s 607us/step - loss: 0.1702 - acc: 0.7401 - val_loss: 0.1687 - val_acc: 0.7426\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 58s 606us/step - loss: 0.1677 - acc: 0.7441 - val_loss: 0.1691 - val_acc: 0.7396\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 59s 623us/step - loss: 0.1665 - acc: 0.7471 - val_loss: 0.1652 - val_acc: 0.7523\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 58s 607us/step - loss: 0.1618 - acc: 0.7560 - val_loss: 0.1727 - val_acc: 0.7365\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 57s 600us/step - loss: 0.1601 - acc: 0.7570 - val_loss: 0.1578 - val_acc: 0.7583\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 56s 595us/step - loss: 0.1513 - acc: 0.7700 - val_loss: 0.1599 - val_acc: 0.7565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80b00e27f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelw = Sequential()\n",
    "modelw.add(LSTM(100, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "modelw.add(LSTM(25))\n",
    "modelw.add(Dropout(0.2))\n",
    "#modelw.add(BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "modelw.add(Dense(100,activation='tanh'))\n",
    "modelw.add(Dense(25,activation='tanh'))\n",
    "modelw.add(Dense(10,activation='tanh'))\n",
    "modelw.add(Dense(1, activation='sigmoid'))\n",
    "modelw.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(modelw.summary())\n",
    "modelw.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30,batch_size=1000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10392/10392 [==============================] - 7s 711us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.65434949502658"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = modelw.evaluate(X_test, Y_test)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "#save keras model\n",
    "model_json = modelw.to_json()\n",
    "with open(\"models/model16times_30epoch1000batch.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "modelw.save_weights(\"models/model16times_30epoch1000batch.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.79032258, 0.00998296, 0.01042305, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412],\n",
       "         [0.79032258, 0.00998296, 0.01042305, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412],\n",
       "         [0.79032258, 0.00998296, 0.01042305, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412],\n",
       "         ...,\n",
       "         [0.77419355, 0.00900901, 0.01440834, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412],\n",
       "         [0.77419355, 0.00900901, 0.01440834, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412],\n",
       "         [0.79032258, 0.00998296, 0.01042305, ..., 0.        ,\n",
       "          0.0100189 , 0.22680412]],\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02438563, 0.        ]],\n",
       " \n",
       "        [[0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01323251, 0.        ],\n",
       "         [0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01323251, 0.        ],\n",
       "         [0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01323251, 0.        ],\n",
       "         ...,\n",
       "         [0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01606805, 0.        ],\n",
       "         [0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01606805, 0.        ],\n",
       "         [0.64516129, 0.        , 0.00326998, ..., 0.        ,\n",
       "          0.01606805, 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.4250859 ]],\n",
       " \n",
       "        [[0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         ...,\n",
       "         [0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.74193548, 0.        , 0.00183936, ..., 0.        ,\n",
       "          0.01587902, 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01776938, 0.34123712],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01776938, 0.34123712],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01776938, 0.34123712],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01531191, 0.34123712],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01531191, 0.34123712],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01531191, 0.34123712]]]), array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01720227, 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ],\n",
       "         [0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ],\n",
       "         [0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ],\n",
       "         [0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ],\n",
       "         [0.        , 0.        , 0.01675863, ..., 0.02409639,\n",
       "          0.01512287, 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.01928166, 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.21649485]],\n",
       " \n",
       "        [[0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ],\n",
       "         [0.        , 0.        , 0.00255467, ..., 0.        ,\n",
       "          0.01587902, 0.        ]],\n",
       " \n",
       "        [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.02759924, 0.        ]]]), array([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [X_train,Y_train,X_test,Y_test]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finalset/datasets_16times.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 98, 100)           52400     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25)                25200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 82,996\n",
      "Trainable params: 82,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 396s 4ms/step - loss: 0.2259 - acc: 0.6408 - val_loss: 0.2270 - val_acc: 0.6385\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 384s 4ms/step - loss: 0.2230 - acc: 0.6471 - val_loss: 0.2236 - val_acc: 0.6406\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 397s 4ms/step - loss: 0.2208 - acc: 0.6519 - val_loss: 0.2231 - val_acc: 0.6481\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 393s 4ms/step - loss: 0.2162 - acc: 0.6603 - val_loss: 0.2207 - val_acc: 0.6441\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 384s 4ms/step - loss: 0.2124 - acc: 0.6648 - val_loss: 0.2117 - val_acc: 0.6630\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 377s 4ms/step - loss: 0.2082 - acc: 0.6738 - val_loss: 0.2062 - val_acc: 0.6720\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 370s 4ms/step - loss: 0.2024 - acc: 0.6843 - val_loss: 0.2031 - val_acc: 0.6794\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 393s 4ms/step - loss: 0.1964 - acc: 0.6952 - val_loss: 0.1997 - val_acc: 0.6930\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 391s 4ms/step - loss: 0.1881 - acc: 0.7105 - val_loss: 0.1896 - val_acc: 0.7085\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 395s 4ms/step - loss: 0.1873 - acc: 0.7133 - val_loss: 0.1817 - val_acc: 0.7206\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 403s 4ms/step - loss: 0.1748 - acc: 0.7348 - val_loss: 0.1853 - val_acc: 0.7156\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 394s 4ms/step - loss: 0.1737 - acc: 0.7376 - val_loss: 0.1723 - val_acc: 0.7287\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 404s 4ms/step - loss: 0.1660 - acc: 0.7496 - val_loss: 0.1648 - val_acc: 0.7531\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 393s 4ms/step - loss: 0.1612 - acc: 0.7565 - val_loss: 0.1638 - val_acc: 0.7491\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 403s 4ms/step - loss: 0.1601 - acc: 0.7547 - val_loss: 0.1609 - val_acc: 0.7505\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 400s 4ms/step - loss: 0.1530 - acc: 0.7666 - val_loss: 0.1569 - val_acc: 0.7578\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 395s 4ms/step - loss: 0.1473 - acc: 0.7761 - val_loss: 0.1467 - val_acc: 0.7742\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 412s 4ms/step - loss: 0.1437 - acc: 0.7816 - val_loss: 0.1427 - val_acc: 0.7799\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 377s 4ms/step - loss: 0.1429 - acc: 0.7814 - val_loss: 0.1484 - val_acc: 0.7688\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 388s 4ms/step - loss: 0.1362 - acc: 0.7920 - val_loss: 0.1376 - val_acc: 0.7878\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 408s 4ms/step - loss: 0.1312 - acc: 0.8019 - val_loss: 0.1450 - val_acc: 0.7833\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 392s 4ms/step - loss: 0.1300 - acc: 0.8042 - val_loss: 0.1296 - val_acc: 0.8030\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 411s 4ms/step - loss: 0.1311 - acc: 0.7985 - val_loss: 0.1435 - val_acc: 0.7753\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 405s 4ms/step - loss: 0.1294 - acc: 0.8010 - val_loss: 0.1299 - val_acc: 0.8001\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 393s 4ms/step - loss: 0.1320 - acc: 0.7963 - val_loss: 0.1429 - val_acc: 0.7779\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 391s 4ms/step - loss: 0.1301 - acc: 0.7990 - val_loss: 0.1539 - val_acc: 0.7619\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 385s 4ms/step - loss: 0.1265 - acc: 0.8046 - val_loss: 0.1294 - val_acc: 0.7945\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 390s 4ms/step - loss: 0.1264 - acc: 0.8038 - val_loss: 0.1374 - val_acc: 0.7819\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 370s 4ms/step - loss: 0.1294 - acc: 0.8013 - val_loss: 0.1610 - val_acc: 0.7564\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 395s 4ms/step - loss: 0.1273 - acc: 0.8055 - val_loss: 0.1241 - val_acc: 0.8084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8060478da0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb = Sequential()\n",
    "modelb.add(LSTM(100, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "modelb.add(Bidirectional(LSTM(25),merge_mode='sum'))\n",
    "modelb.add(Dropout(0.2))\n",
    "#modelb.add(BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "modelb.add(Dense(100,activation='tanh'))\n",
    "modelb.add(Dense(25,activation='tanh'))\n",
    "modelb.add(Dense(10,activation='tanh'))\n",
    "modelb.add(Dense(1, activation='sigmoid'))\n",
    "modelb.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(modelb.summary())\n",
    "modelb.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30,batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10392/10392 [==============================] - 8s 806us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.84103155815207"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresb = modelb.evaluate(X_test, Y_test)\n",
    "scoresb[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 98, 30)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7696, 2940)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(45000, 2940)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-627015f192c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_trainpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_testpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2635\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdas_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# hide NaN warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2637\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_yeo_johnson_transform\u001b[0;34m(self, x, lmbda)\u001b[0m\n\u001b[1;32m   2743\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# lmbda != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2745\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m         \u001b[0;31m# when x < 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer()\n",
    "X_trainpt = X_train.reshape((X_train.shape[0],2940))\n",
    "X_testpt = X_test.reshape((X_test.shape[0],2940))\n",
    "display(X_testpt.shape)\n",
    "display(X_trainpt.shape)\n",
    "pt.fit(X_trainpt)\n",
    "X_trainpt = pt.transform(X_trainpt)\n",
    "X_testpt = pt.transform(X_testpt)\n",
    "display(X_trainpt)\n",
    "X_testpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainpt = X_trainpt.reshape((X_trainpt.shape[0],98,30))\n",
    "X_testpt = X_testpt.reshape((X_testpt.shape[0],98,30))\n",
    "display(X_testpt.shape)\n",
    "display(X_trainpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpt = Sequential()\n",
    "modelpt.add(LSTM(100, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "modelpt.add(LSTM(25))\n",
    "modelpt.add(Dropout(0.2))\n",
    "#modelpt.add(BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "modelpt.add(Dense(100,activation='tanh'))\n",
    "modelpt.add(Dense(25,activation='tanh'))\n",
    "modelpt.add(Dense(10,activation='tanh'))\n",
    "modelpt.add(Dense(1, activation='sigmoid'))\n",
    "modelpt.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(modelpt.summary())\n",
    "modelpt.fit(X_trainpt, Y_train, validation_data=(X_testpt, Y_test), epochs=30,batch_size=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorespt = modelpt.evaluate(X_testpt, Y_test)\n",
    "scorespt[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 2940)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7696, 2940)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Normalizer(copy=True, norm='l2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.00128286,\n",
       "        0.        ],\n",
       "       [0.03789661, 0.00333721, 0.00051354, ..., 0.        , 0.00064156,\n",
       "        0.01785294],\n",
       "       [0.03606941, 0.        , 0.        , ..., 0.        , 0.00129916,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.00150139,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.0069234 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.0124651 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2.76191917e-02, 1.84845268e-03, 1.22488547e-04, ...,\n",
       "        0.00000000e+00, 1.86668841e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 4.81152853e-03, 0.00000000e+00],\n",
       "       [4.30319146e-02, 2.14992429e-03, 7.78949141e-05, ...,\n",
       "        0.00000000e+00, 1.22483494e-03, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 9.22829555e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.93200624e-03, 1.44628939e-02],\n",
       "       [0.00000000e+00, 2.73540577e-03, 1.65895710e-04, ...,\n",
       "        0.00000000e+00, 9.08398494e-04, 1.51522509e-02]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "X_trainn = X_train.reshape((X_train.shape[0],2940))\n",
    "X_testn = X_test.reshape((X_test.shape[0],2940))\n",
    "display(X_trainn.shape)\n",
    "display(X_testn.shape)\n",
    "transformer = Normalizer().fit(X_trainn)\n",
    "display(transformer)\n",
    "X_trainn = transformer.transform(X_trainn)\n",
    "X_testn = transformer.transform(X_testn)\n",
    "display(X_trainn)\n",
    "display(X_testn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7696, 98, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(45000, 98, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_trainn = X_trainn.reshape((X_trainn.shape[0],98,30))\n",
    "X_testn = X_testn.reshape((X_testn.shape[0],98,30))\n",
    "display(X_testn.shape)\n",
    "display(X_trainn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 98, 100)           52400     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 70,396\n",
      "Trainable params: 70,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 45000 samples, validate on 7696 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 65s 1ms/step - loss: 0.2304 - acc: 0.6319 - val_loss: 0.2246 - val_acc: 0.6424\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2270 - acc: 0.6390 - val_loss: 0.2240 - val_acc: 0.6451\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 60s 1ms/step - loss: 0.2266 - acc: 0.6395 - val_loss: 0.2243 - val_acc: 0.6451\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2272 - acc: 0.6387 - val_loss: 0.2240 - val_acc: 0.6451\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2265 - acc: 0.6393 - val_loss: 0.2270 - val_acc: 0.6379\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2265 - acc: 0.6400 - val_loss: 0.2238 - val_acc: 0.6451\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2261 - acc: 0.6394 - val_loss: 0.2237 - val_acc: 0.6453\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2260 - acc: 0.6398 - val_loss: 0.2233 - val_acc: 0.6459\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2262 - acc: 0.6397 - val_loss: 0.2287 - val_acc: 0.6290\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 62s 1ms/step - loss: 0.2268 - acc: 0.6386 - val_loss: 0.2306 - val_acc: 0.6294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa72788eac8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeln = Sequential()\n",
    "modeln.add(LSTM(100, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "modeln.add(LSTM(25))\n",
    "modeln.add(Dropout(0.2))\n",
    "#modeln.add(BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "modeln.add(Dense(100,activation='tanh'))\n",
    "modeln.add(Dense(25,activation='tanh'))\n",
    "modeln.add(Dense(10,activation='tanh'))\n",
    "modeln.add(Dense(1, activation='sigmoid'))\n",
    "modeln.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(modeln.summary())\n",
    "modeln.fit(X_trainn, Y_train, validation_data=(X_testn, Y_test), epochs=10,batch_size=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7696/7696 [==============================] - 5s 656us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.94178794178794"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoren = modeln.evaluate(X_testn, Y_test)\n",
    "scoren[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "anss = []\n",
    "class LSTM(object):\n",
    "    def __init__(self):\n",
    "        # define lower bound of benchmark function\n",
    "        self.Lower = 0\n",
    "        # define upper bound of benchmark function\n",
    "        self.Upper = 1\n",
    "\n",
    "    # function which returns evaluate function\n",
    "    def function(self):\n",
    "        def evalute(D,sol):\n",
    "            from keras.models import Sequential\n",
    "            from keras.layers import Dense\n",
    "            from keras.layers import LSTM\n",
    "            from keras.layers import Dropout\n",
    "            sol = np.array(sol)\n",
    "            sol = sigmoid(sol)\n",
    "            #display(sol)\n",
    "            op = sol>=0.5\n",
    "            X_tr = X_train[:,:,op]\n",
    "            X_te = X_test[:,:,op]\n",
    "            modelf = Sequential()\n",
    "            modelf.add(LSTM(100, input_shape=(X_tr.shape[1],X_tr.shape[2]),return_sequences=True))\n",
    "            modelf.add(LSTM(50))\n",
    "            modelf.add(Dropout(0.2))\n",
    "            #modelf.add(BatchNormalization())\n",
    "            modelf.add(Dense(100,activation='tanh'))\n",
    "            modelf.add(Dense(25,activation='tanh'))\n",
    "            modelf.add(Dense(10,activation='tanh'))\n",
    "            modelf.add(Dense(1, activation='sigmoid'))\n",
    "            modelf.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "            modelf.fit(X_tr, Y_train, validation_data=(X_te, Y_test), epochs=30,batch_size=1000,verbose=1)\n",
    "            scores = modelf.evaluate(X_te, Y_test, verbose=0)\n",
    "            anss.append(scores[1])\n",
    "            k = -1*scores[1]\n",
    "            return k\n",
    "        return evalute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2278 - acc: 0.6375 - val_loss: 0.2258 - val_acc: 0.6423\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2244 - acc: 0.6438 - val_loss: 0.2251 - val_acc: 0.6416\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2229 - acc: 0.6463 - val_loss: 0.2233 - val_acc: 0.6441\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2218 - acc: 0.6493 - val_loss: 0.2234 - val_acc: 0.6445\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2202 - acc: 0.6529 - val_loss: 0.2208 - val_acc: 0.6496\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2180 - acc: 0.6573 - val_loss: 0.2202 - val_acc: 0.6482\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2169 - acc: 0.6597 - val_loss: 0.2177 - val_acc: 0.6566\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 93s 980us/step - loss: 0.2149 - acc: 0.6627 - val_loss: 0.2167 - val_acc: 0.6569\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 65s 681us/step - loss: 0.2129 - acc: 0.6652 - val_loss: 0.2121 - val_acc: 0.6625\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 63s 667us/step - loss: 0.2116 - acc: 0.6680 - val_loss: 0.2156 - val_acc: 0.6570\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 63s 666us/step - loss: 0.2091 - acc: 0.6710 - val_loss: 0.2069 - val_acc: 0.6690\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 93s 975us/step - loss: 0.2038 - acc: 0.6810 - val_loss: 0.2048 - val_acc: 0.6798\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 83s 878us/step - loss: 0.1994 - acc: 0.6899 - val_loss: 0.2010 - val_acc: 0.6823\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 63s 665us/step - loss: 0.1939 - acc: 0.6980 - val_loss: 0.2018 - val_acc: 0.6778\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 63s 665us/step - loss: 0.1917 - acc: 0.7006 - val_loss: 0.1958 - val_acc: 0.6947\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 64s 679us/step - loss: 0.1823 - acc: 0.7184 - val_loss: 0.1843 - val_acc: 0.7115\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 65s 687us/step - loss: 0.1729 - acc: 0.7365 - val_loss: 0.1748 - val_acc: 0.7291\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 66s 696us/step - loss: 0.1709 - acc: 0.7407 - val_loss: 0.1746 - val_acc: 0.7290\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 69s 729us/step - loss: 0.1606 - acc: 0.7595 - val_loss: 0.1568 - val_acc: 0.7629\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 69s 731us/step - loss: 0.1528 - acc: 0.7713 - val_loss: 0.1521 - val_acc: 0.7677\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 70s 734us/step - loss: 0.1577 - acc: 0.7645 - val_loss: 0.1635 - val_acc: 0.7571\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 68s 716us/step - loss: 0.1522 - acc: 0.7730 - val_loss: 0.1538 - val_acc: 0.7687\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 68s 718us/step - loss: 0.1442 - acc: 0.7865 - val_loss: 0.1421 - val_acc: 0.7897\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 66s 691us/step - loss: 0.1372 - acc: 0.7966 - val_loss: 0.1377 - val_acc: 0.7897\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 69s 725us/step - loss: 0.1412 - acc: 0.7914 - val_loss: 0.1411 - val_acc: 0.7837\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 69s 721us/step - loss: 0.1345 - acc: 0.8014 - val_loss: 0.1330 - val_acc: 0.7986\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 69s 730us/step - loss: 0.1316 - acc: 0.8053 - val_loss: 0.1340 - val_acc: 0.8033\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 65s 684us/step - loss: 0.1265 - acc: 0.8139 - val_loss: 0.1286 - val_acc: 0.8115\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.1276 - acc: 0.8127 - val_loss: 0.1423 - val_acc: 0.7906\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.1251 - acc: 0.8176 - val_loss: 0.1202 - val_acc: 0.8187\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 74s 782us/step - loss: 0.2279 - acc: 0.6366 - val_loss: 0.2256 - val_acc: 0.6387\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 63s 664us/step - loss: 0.2245 - acc: 0.6435 - val_loss: 0.2254 - val_acc: 0.6406\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 63s 665us/step - loss: 0.2235 - acc: 0.6454 - val_loss: 0.2239 - val_acc: 0.6414\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 63s 663us/step - loss: 0.2217 - acc: 0.6481 - val_loss: 0.2223 - val_acc: 0.6460\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 63s 668us/step - loss: 0.2211 - acc: 0.6503 - val_loss: 0.2237 - val_acc: 0.6426\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.2208 - acc: 0.6495 - val_loss: 0.2205 - val_acc: 0.6465\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 63s 662us/step - loss: 0.2178 - acc: 0.6550 - val_loss: 0.2197 - val_acc: 0.6524\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 63s 664us/step - loss: 0.2161 - acc: 0.6596 - val_loss: 0.2189 - val_acc: 0.6526\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 64s 671us/step - loss: 0.2135 - acc: 0.6638 - val_loss: 0.2141 - val_acc: 0.6604\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 63s 667us/step - loss: 0.2101 - acc: 0.6688 - val_loss: 0.2121 - val_acc: 0.6638\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 63s 666us/step - loss: 0.2077 - acc: 0.6755 - val_loss: 0.2074 - val_acc: 0.6690\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 63s 666us/step - loss: 0.2030 - acc: 0.6842 - val_loss: 0.2048 - val_acc: 0.6813\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 63s 666us/step - loss: 0.2001 - acc: 0.6902 - val_loss: 0.2022 - val_acc: 0.6827\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 64s 675us/step - loss: 0.1961 - acc: 0.6967 - val_loss: 0.2021 - val_acc: 0.6813\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 64s 672us/step - loss: 0.1901 - acc: 0.7081 - val_loss: 0.1906 - val_acc: 0.7053\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 64s 673us/step - loss: 0.1824 - acc: 0.7216 - val_loss: 0.1873 - val_acc: 0.7079\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.1781 - acc: 0.7284 - val_loss: 0.1833 - val_acc: 0.7175\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 64s 673us/step - loss: 0.1777 - acc: 0.7303 - val_loss: 0.1779 - val_acc: 0.7208\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 65s 682us/step - loss: 0.1686 - acc: 0.7444 - val_loss: 0.1863 - val_acc: 0.7191\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 70s 737us/step - loss: 0.1656 - acc: 0.7519 - val_loss: 0.1695 - val_acc: 0.7464\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 69s 725us/step - loss: 0.1604 - acc: 0.7587 - val_loss: 0.1667 - val_acc: 0.7504\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 69s 727us/step - loss: 0.1554 - acc: 0.7684 - val_loss: 0.1639 - val_acc: 0.7548\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 68s 721us/step - loss: 0.1517 - acc: 0.7742 - val_loss: 0.1529 - val_acc: 0.7716\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 68s 715us/step - loss: 0.1774 - acc: 0.7360 - val_loss: 0.1675 - val_acc: 0.7475\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.1557 - acc: 0.7679 - val_loss: 0.1552 - val_acc: 0.7650\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.1469 - acc: 0.7813 - val_loss: 0.1513 - val_acc: 0.7747\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1431 - acc: 0.7877 - val_loss: 0.1460 - val_acc: 0.7821\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.1385 - acc: 0.7948 - val_loss: 0.1426 - val_acc: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1392 - acc: 0.7964 - val_loss: 0.1433 - val_acc: 0.7875\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1312 - acc: 0.8073 - val_loss: 0.1337 - val_acc: 0.8011\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 65s 684us/step - loss: 0.2277 - acc: 0.6393 - val_loss: 0.2256 - val_acc: 0.6383\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2246 - acc: 0.6428 - val_loss: 0.2245 - val_acc: 0.6424\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2232 - acc: 0.6469 - val_loss: 0.2228 - val_acc: 0.6485\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2210 - acc: 0.6527 - val_loss: 0.2210 - val_acc: 0.6510\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2192 - acc: 0.6560 - val_loss: 0.2215 - val_acc: 0.6483\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2179 - acc: 0.6573 - val_loss: 0.2187 - val_acc: 0.6531\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2168 - acc: 0.6596 - val_loss: 0.2196 - val_acc: 0.6557\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2150 - acc: 0.6618 - val_loss: 0.2166 - val_acc: 0.6556\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2124 - acc: 0.6655 - val_loss: 0.2136 - val_acc: 0.6611\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2122 - acc: 0.6655 - val_loss: 0.2122 - val_acc: 0.6636\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2113 - acc: 0.6665 - val_loss: 0.2150 - val_acc: 0.6589\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2088 - acc: 0.6721 - val_loss: 0.2103 - val_acc: 0.6676\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2058 - acc: 0.6775 - val_loss: 0.2081 - val_acc: 0.6729\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2017 - acc: 0.6858 - val_loss: 0.2000 - val_acc: 0.6881\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1980 - acc: 0.6934 - val_loss: 0.2020 - val_acc: 0.6805\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2001 - acc: 0.6917 - val_loss: 0.2053 - val_acc: 0.6807\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1957 - acc: 0.6973 - val_loss: 0.1919 - val_acc: 0.6984\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1902 - acc: 0.7079 - val_loss: 0.1874 - val_acc: 0.7138\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 654us/step - loss: 0.1847 - acc: 0.7173 - val_loss: 0.1806 - val_acc: 0.7247\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1794 - acc: 0.7263 - val_loss: 0.1866 - val_acc: 0.7107\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1761 - acc: 0.7306 - val_loss: 0.1763 - val_acc: 0.7289\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1749 - acc: 0.7347 - val_loss: 0.1714 - val_acc: 0.7356\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1666 - acc: 0.7478 - val_loss: 0.1668 - val_acc: 0.7380\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1577 - acc: 0.7607 - val_loss: 0.1621 - val_acc: 0.7450\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1624 - acc: 0.7548 - val_loss: 0.1620 - val_acc: 0.7526\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1556 - acc: 0.7658 - val_loss: 0.1533 - val_acc: 0.7710\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1963 - acc: 0.7035 - val_loss: 0.1869 - val_acc: 0.7198\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1679 - acc: 0.7467 - val_loss: 0.1711 - val_acc: 0.7381\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1696 - acc: 0.7459 - val_loss: 0.1654 - val_acc: 0.7505\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1548 - acc: 0.7697 - val_loss: 0.1574 - val_acc: 0.7650\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.2270 - acc: 0.6393 - val_loss: 0.2258 - val_acc: 0.6386\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2248 - acc: 0.6431 - val_loss: 0.2251 - val_acc: 0.6431\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2235 - acc: 0.6446 - val_loss: 0.2235 - val_acc: 0.6418\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2216 - acc: 0.6487 - val_loss: 0.2213 - val_acc: 0.6484\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2196 - acc: 0.6534 - val_loss: 0.2197 - val_acc: 0.6533\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2174 - acc: 0.6575 - val_loss: 0.2184 - val_acc: 0.6542\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2163 - acc: 0.6601 - val_loss: 0.2173 - val_acc: 0.6559\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2150 - acc: 0.6617 - val_loss: 0.2154 - val_acc: 0.6592\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2118 - acc: 0.6675 - val_loss: 0.2153 - val_acc: 0.6564\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2110 - acc: 0.6685 - val_loss: 0.2140 - val_acc: 0.6619\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2089 - acc: 0.6726 - val_loss: 0.2072 - val_acc: 0.6719\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2029 - acc: 0.6835 - val_loss: 0.2015 - val_acc: 0.6816\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1976 - acc: 0.6925 - val_loss: 0.2012 - val_acc: 0.6706\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1919 - acc: 0.7030 - val_loss: 0.1967 - val_acc: 0.6935\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1892 - acc: 0.7077 - val_loss: 0.1924 - val_acc: 0.7009\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1854 - acc: 0.7137 - val_loss: 0.1885 - val_acc: 0.7056\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.1772 - acc: 0.7293 - val_loss: 0.1826 - val_acc: 0.7173\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1719 - acc: 0.7382 - val_loss: 0.1762 - val_acc: 0.7257\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1701 - acc: 0.7418 - val_loss: 0.1730 - val_acc: 0.7314\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1633 - acc: 0.7525 - val_loss: 0.1768 - val_acc: 0.7242\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.1674 - acc: 0.7473 - val_loss: 0.1668 - val_acc: 0.7380\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1557 - acc: 0.7649 - val_loss: 0.1639 - val_acc: 0.7495\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1493 - acc: 0.7745 - val_loss: 0.1460 - val_acc: 0.7780\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.1480 - acc: 0.7772 - val_loss: 0.1522 - val_acc: 0.7669\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1482 - acc: 0.7765 - val_loss: 0.1427 - val_acc: 0.7771\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1409 - acc: 0.7874 - val_loss: 0.1401 - val_acc: 0.7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1407 - acc: 0.7888 - val_loss: 0.1439 - val_acc: 0.7802\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1336 - acc: 0.7999 - val_loss: 0.1317 - val_acc: 0.7989\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1433 - acc: 0.7863 - val_loss: 0.1624 - val_acc: 0.7595\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1387 - acc: 0.7931 - val_loss: 0.1331 - val_acc: 0.7969\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 65s 687us/step - loss: 0.2267 - acc: 0.6402 - val_loss: 0.2255 - val_acc: 0.6373\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2245 - acc: 0.6428 - val_loss: 0.2249 - val_acc: 0.6409\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2228 - acc: 0.6472 - val_loss: 0.2239 - val_acc: 0.6429\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2215 - acc: 0.6502 - val_loss: 0.2228 - val_acc: 0.6468\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2197 - acc: 0.6537 - val_loss: 0.2212 - val_acc: 0.6529\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2181 - acc: 0.6565 - val_loss: 0.2214 - val_acc: 0.6486\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2169 - acc: 0.6589 - val_loss: 0.2184 - val_acc: 0.6580\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2144 - acc: 0.6637 - val_loss: 0.2168 - val_acc: 0.6609\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2142 - acc: 0.6641 - val_loss: 0.2158 - val_acc: 0.6579\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2126 - acc: 0.6662 - val_loss: 0.2133 - val_acc: 0.6679\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2099 - acc: 0.6728 - val_loss: 0.2142 - val_acc: 0.6656\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2082 - acc: 0.6753 - val_loss: 0.2083 - val_acc: 0.6701\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2051 - acc: 0.6819 - val_loss: 0.2066 - val_acc: 0.6739\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2052 - acc: 0.6808 - val_loss: 0.2060 - val_acc: 0.6768\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2001 - acc: 0.6893 - val_loss: 0.2026 - val_acc: 0.6805\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1954 - acc: 0.6977 - val_loss: 0.1966 - val_acc: 0.6934\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1910 - acc: 0.7042 - val_loss: 0.1931 - val_acc: 0.6931\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1849 - acc: 0.7152 - val_loss: 0.1844 - val_acc: 0.7155\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1809 - acc: 0.7227 - val_loss: 0.1851 - val_acc: 0.7157\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1746 - acc: 0.7322 - val_loss: 0.1750 - val_acc: 0.7283\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1694 - acc: 0.7400 - val_loss: 0.1753 - val_acc: 0.7257\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1720 - acc: 0.7374 - val_loss: 0.1796 - val_acc: 0.7250\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1648 - acc: 0.7490 - val_loss: 0.1698 - val_acc: 0.7355\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1592 - acc: 0.7582 - val_loss: 0.1560 - val_acc: 0.7610\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1634 - acc: 0.7540 - val_loss: 0.1639 - val_acc: 0.7475\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1523 - acc: 0.7703 - val_loss: 0.1570 - val_acc: 0.7593\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1471 - acc: 0.7774 - val_loss: 0.1491 - val_acc: 0.7693\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1409 - acc: 0.7860 - val_loss: 0.1457 - val_acc: 0.7795\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1361 - acc: 0.7920 - val_loss: 0.1336 - val_acc: 0.7983\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1304 - acc: 0.8014 - val_loss: 0.1325 - val_acc: 0.7972\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 66s 694us/step - loss: 0.2277 - acc: 0.6374 - val_loss: 0.2254 - val_acc: 0.6399\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2243 - acc: 0.6441 - val_loss: 0.2243 - val_acc: 0.6444\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2231 - acc: 0.6464 - val_loss: 0.2227 - val_acc: 0.6442\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 647us/step - loss: 0.2212 - acc: 0.6513 - val_loss: 0.2237 - val_acc: 0.6442\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2191 - acc: 0.6560 - val_loss: 0.2204 - val_acc: 0.6509\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2177 - acc: 0.6580 - val_loss: 0.2185 - val_acc: 0.6560\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2169 - acc: 0.6593 - val_loss: 0.2175 - val_acc: 0.6578\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2153 - acc: 0.6619 - val_loss: 0.2148 - val_acc: 0.6621\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2128 - acc: 0.6662 - val_loss: 0.2131 - val_acc: 0.6645\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2102 - acc: 0.6707 - val_loss: 0.2128 - val_acc: 0.6672\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2126 - acc: 0.6666 - val_loss: 0.2129 - val_acc: 0.6677\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2071 - acc: 0.6778 - val_loss: 0.2063 - val_acc: 0.6757\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2038 - acc: 0.6836 - val_loss: 0.2055 - val_acc: 0.6779\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2032 - acc: 0.6856 - val_loss: 0.2015 - val_acc: 0.6885\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2009 - acc: 0.6911 - val_loss: 0.2012 - val_acc: 0.6925\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1979 - acc: 0.6957 - val_loss: 0.1989 - val_acc: 0.6949\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1950 - acc: 0.7011 - val_loss: 0.1961 - val_acc: 0.6986\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1896 - acc: 0.7089 - val_loss: 0.1937 - val_acc: 0.7021\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1871 - acc: 0.7128 - val_loss: 0.1858 - val_acc: 0.7162\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1887 - acc: 0.7118 - val_loss: 0.1971 - val_acc: 0.6962\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1859 - acc: 0.7160 - val_loss: 0.1876 - val_acc: 0.7120\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1800 - acc: 0.7252 - val_loss: 0.1762 - val_acc: 0.7285\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1740 - acc: 0.7371 - val_loss: 0.1774 - val_acc: 0.7373\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1710 - acc: 0.7436 - val_loss: 0.1710 - val_acc: 0.7433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1639 - acc: 0.7549 - val_loss: 0.1637 - val_acc: 0.7533\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1635 - acc: 0.7556 - val_loss: 0.1664 - val_acc: 0.7508\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1582 - acc: 0.7651 - val_loss: 0.1588 - val_acc: 0.7615\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1552 - acc: 0.7694 - val_loss: 0.1597 - val_acc: 0.7643\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1500 - acc: 0.7805 - val_loss: 0.1578 - val_acc: 0.7652\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1508 - acc: 0.7779 - val_loss: 0.1548 - val_acc: 0.7675\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 65s 686us/step - loss: 0.2281 - acc: 0.6360 - val_loss: 0.2260 - val_acc: 0.6375\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2246 - acc: 0.6440 - val_loss: 0.2252 - val_acc: 0.6422\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.2231 - acc: 0.6472 - val_loss: 0.2236 - val_acc: 0.6423\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2229 - acc: 0.6461 - val_loss: 0.2238 - val_acc: 0.6424\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 638us/step - loss: 0.2213 - acc: 0.6502 - val_loss: 0.2216 - val_acc: 0.6493\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2195 - acc: 0.6535 - val_loss: 0.2241 - val_acc: 0.6413\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2201 - acc: 0.6524 - val_loss: 0.2205 - val_acc: 0.6478\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 637us/step - loss: 0.2180 - acc: 0.6563 - val_loss: 0.2193 - val_acc: 0.6484\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2167 - acc: 0.6580 - val_loss: 0.2204 - val_acc: 0.6475\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2150 - acc: 0.6619 - val_loss: 0.2166 - val_acc: 0.6592\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 61s 637us/step - loss: 0.2127 - acc: 0.6677 - val_loss: 0.2162 - val_acc: 0.6622\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2121 - acc: 0.6697 - val_loss: 0.2138 - val_acc: 0.6669\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.2084 - acc: 0.6742 - val_loss: 0.2086 - val_acc: 0.6768\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2064 - acc: 0.6784 - val_loss: 0.2099 - val_acc: 0.6721\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.2023 - acc: 0.6846 - val_loss: 0.2016 - val_acc: 0.6918\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1981 - acc: 0.6940 - val_loss: 0.2016 - val_acc: 0.6823\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1984 - acc: 0.6946 - val_loss: 0.1986 - val_acc: 0.6935\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 61s 638us/step - loss: 0.1921 - acc: 0.7045 - val_loss: 0.1922 - val_acc: 0.7003\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1870 - acc: 0.7139 - val_loss: 0.1956 - val_acc: 0.7000\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 61s 637us/step - loss: 0.1821 - acc: 0.7212 - val_loss: 0.1827 - val_acc: 0.7224\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1839 - acc: 0.7188 - val_loss: 0.1890 - val_acc: 0.7094\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1776 - acc: 0.7280 - val_loss: 0.1800 - val_acc: 0.7174\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1685 - acc: 0.7436 - val_loss: 0.1745 - val_acc: 0.7368\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 637us/step - loss: 0.1627 - acc: 0.7528 - val_loss: 0.1662 - val_acc: 0.7464\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1591 - acc: 0.7597 - val_loss: 0.1640 - val_acc: 0.7510\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 61s 639us/step - loss: 0.1642 - acc: 0.7520 - val_loss: 0.1839 - val_acc: 0.7226\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1554 - acc: 0.7658 - val_loss: 0.1562 - val_acc: 0.7574\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1521 - acc: 0.7722 - val_loss: 0.1557 - val_acc: 0.7634\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1445 - acc: 0.7832 - val_loss: 0.1770 - val_acc: 0.7338\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.1584 - acc: 0.7617 - val_loss: 0.1542 - val_acc: 0.7652\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 66s 696us/step - loss: 0.2275 - acc: 0.6382 - val_loss: 0.2255 - val_acc: 0.6373\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2243 - acc: 0.6435 - val_loss: 0.2241 - val_acc: 0.6412\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2229 - acc: 0.6464 - val_loss: 0.2221 - val_acc: 0.6475\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2212 - acc: 0.6501 - val_loss: 0.2240 - val_acc: 0.6446\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2204 - acc: 0.6521 - val_loss: 0.2209 - val_acc: 0.6497\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2189 - acc: 0.6543 - val_loss: 0.2220 - val_acc: 0.6455\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2183 - acc: 0.6561 - val_loss: 0.2178 - val_acc: 0.6550\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2166 - acc: 0.6595 - val_loss: 0.2184 - val_acc: 0.6521\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2168 - acc: 0.6582 - val_loss: 0.2183 - val_acc: 0.6481\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2136 - acc: 0.6649 - val_loss: 0.2164 - val_acc: 0.6616\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2115 - acc: 0.6715 - val_loss: 0.2124 - val_acc: 0.6585\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2070 - acc: 0.6774 - val_loss: 0.2069 - val_acc: 0.6729\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2029 - acc: 0.6840 - val_loss: 0.2055 - val_acc: 0.6779\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1981 - acc: 0.6918 - val_loss: 0.2007 - val_acc: 0.6846\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1967 - acc: 0.6947 - val_loss: 0.2011 - val_acc: 0.6825\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1907 - acc: 0.7040 - val_loss: 0.1937 - val_acc: 0.6943\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1823 - acc: 0.7183 - val_loss: 0.1791 - val_acc: 0.7192\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 653us/step - loss: 0.1802 - acc: 0.7208 - val_loss: 0.1726 - val_acc: 0.7322\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1722 - acc: 0.7364 - val_loss: 0.1683 - val_acc: 0.7427\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1656 - acc: 0.7479 - val_loss: 0.1629 - val_acc: 0.7518\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1569 - acc: 0.7619 - val_loss: 0.1521 - val_acc: 0.7680\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1518 - acc: 0.7713 - val_loss: 0.1523 - val_acc: 0.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1486 - acc: 0.7766 - val_loss: 0.1562 - val_acc: 0.7669\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1442 - acc: 0.7825 - val_loss: 0.1487 - val_acc: 0.7727\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1365 - acc: 0.7945 - val_loss: 0.1408 - val_acc: 0.7925\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1361 - acc: 0.7946 - val_loss: 0.1459 - val_acc: 0.7811\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1315 - acc: 0.8027 - val_loss: 0.1306 - val_acc: 0.8007\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1334 - acc: 0.8014 - val_loss: 0.1527 - val_acc: 0.7717\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1294 - acc: 0.8079 - val_loss: 0.1283 - val_acc: 0.8068\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1199 - acc: 0.8224 - val_loss: 0.1298 - val_acc: 0.8048\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2271 - acc: 0.6402 - val_loss: 0.2252 - val_acc: 0.6391\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2245 - acc: 0.6433 - val_loss: 0.2237 - val_acc: 0.6417\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2232 - acc: 0.6456 - val_loss: 0.2244 - val_acc: 0.6419\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2215 - acc: 0.6500 - val_loss: 0.2215 - val_acc: 0.6459\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2208 - acc: 0.6524 - val_loss: 0.2221 - val_acc: 0.6483\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2186 - acc: 0.6561 - val_loss: 0.2196 - val_acc: 0.6534\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2180 - acc: 0.6577 - val_loss: 0.2199 - val_acc: 0.6499\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2159 - acc: 0.6609 - val_loss: 0.2168 - val_acc: 0.6581\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2151 - acc: 0.6626 - val_loss: 0.2163 - val_acc: 0.6557\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2149 - acc: 0.6630 - val_loss: 0.2188 - val_acc: 0.6560\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2138 - acc: 0.6658 - val_loss: 0.2142 - val_acc: 0.6573\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2120 - acc: 0.6701 - val_loss: 0.2170 - val_acc: 0.6578\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2103 - acc: 0.6712 - val_loss: 0.2135 - val_acc: 0.6581\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2069 - acc: 0.6790 - val_loss: 0.2088 - val_acc: 0.6739\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2064 - acc: 0.6803 - val_loss: 0.2118 - val_acc: 0.6733\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2028 - acc: 0.6873 - val_loss: 0.2034 - val_acc: 0.6795\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1971 - acc: 0.6966 - val_loss: 0.1970 - val_acc: 0.6926\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1930 - acc: 0.7033 - val_loss: 0.1959 - val_acc: 0.6932\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1898 - acc: 0.7095 - val_loss: 0.1942 - val_acc: 0.6987\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1834 - acc: 0.7203 - val_loss: 0.1877 - val_acc: 0.7110\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1813 - acc: 0.7239 - val_loss: 0.1889 - val_acc: 0.7086\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1756 - acc: 0.7339 - val_loss: 0.1772 - val_acc: 0.7296\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.1677 - acc: 0.7455 - val_loss: 0.1736 - val_acc: 0.7322\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1689 - acc: 0.7465 - val_loss: 0.1661 - val_acc: 0.7486\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 647us/step - loss: 0.1671 - acc: 0.7493 - val_loss: 0.1644 - val_acc: 0.7496\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1541 - acc: 0.7709 - val_loss: 0.1574 - val_acc: 0.7636\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1544 - acc: 0.7696 - val_loss: 0.1589 - val_acc: 0.7562\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.1544 - acc: 0.7710 - val_loss: 0.1620 - val_acc: 0.7565\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1475 - acc: 0.7817 - val_loss: 0.1443 - val_acc: 0.7844\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1377 - acc: 0.7956 - val_loss: 0.1635 - val_acc: 0.7632\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 67s 703us/step - loss: 0.2276 - acc: 0.6390 - val_loss: 0.2261 - val_acc: 0.6401\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2249 - acc: 0.6418 - val_loss: 0.2249 - val_acc: 0.6410\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.2230 - acc: 0.6459 - val_loss: 0.2235 - val_acc: 0.6439\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2215 - acc: 0.6486 - val_loss: 0.2215 - val_acc: 0.6481\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.2204 - acc: 0.6514 - val_loss: 0.2229 - val_acc: 0.6418\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2182 - acc: 0.6558 - val_loss: 0.2224 - val_acc: 0.6449\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2186 - acc: 0.6550 - val_loss: 0.2198 - val_acc: 0.6465\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2169 - acc: 0.6559 - val_loss: 0.2188 - val_acc: 0.6531\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2144 - acc: 0.6617 - val_loss: 0.2148 - val_acc: 0.6613\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.2119 - acc: 0.6675 - val_loss: 0.2115 - val_acc: 0.6654\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.2104 - acc: 0.6710 - val_loss: 0.2137 - val_acc: 0.6635\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2098 - acc: 0.6726 - val_loss: 0.2154 - val_acc: 0.6595\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2084 - acc: 0.6742 - val_loss: 0.2090 - val_acc: 0.6713\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2079 - acc: 0.6756 - val_loss: 0.2047 - val_acc: 0.6786\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2038 - acc: 0.6834 - val_loss: 0.2072 - val_acc: 0.6707\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.2018 - acc: 0.6871 - val_loss: 0.2033 - val_acc: 0.6811\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.2000 - acc: 0.6901 - val_loss: 0.2054 - val_acc: 0.6730\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.2017 - acc: 0.6882 - val_loss: 0.2058 - val_acc: 0.6762\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1953 - acc: 0.6961 - val_loss: 0.1988 - val_acc: 0.6921\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 61s 644us/step - loss: 0.1964 - acc: 0.6973 - val_loss: 0.1993 - val_acc: 0.6877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1920 - acc: 0.7048 - val_loss: 0.1929 - val_acc: 0.6961\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 61s 643us/step - loss: 0.1857 - acc: 0.7163 - val_loss: 0.2003 - val_acc: 0.6872\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1844 - acc: 0.7175 - val_loss: 0.1793 - val_acc: 0.7264\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1792 - acc: 0.7285 - val_loss: 0.1755 - val_acc: 0.7289\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.1878 - acc: 0.7154 - val_loss: 0.2089 - val_acc: 0.6738\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1914 - acc: 0.7077 - val_loss: 0.1848 - val_acc: 0.7195\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 61s 646us/step - loss: 0.1742 - acc: 0.7368 - val_loss: 0.1739 - val_acc: 0.7348\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1710 - acc: 0.7430 - val_loss: 0.1652 - val_acc: 0.7499\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1598 - acc: 0.7595 - val_loss: 0.1605 - val_acc: 0.7516\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1603 - acc: 0.7611 - val_loss: 0.1580 - val_acc: 0.7657\n",
      "0.8187066975054693\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 67s 708us/step - loss: 0.2273 - acc: 0.6385 - val_loss: 0.2257 - val_acc: 0.6387\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2244 - acc: 0.6438 - val_loss: 0.2247 - val_acc: 0.6414\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2227 - acc: 0.6477 - val_loss: 0.2245 - val_acc: 0.6411\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 647us/step - loss: 0.2212 - acc: 0.6502 - val_loss: 0.2221 - val_acc: 0.6473\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 61s 647us/step - loss: 0.2193 - acc: 0.6549 - val_loss: 0.2205 - val_acc: 0.6495\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2193 - acc: 0.6551 - val_loss: 0.2196 - val_acc: 0.6547\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 649us/step - loss: 0.2176 - acc: 0.6592 - val_loss: 0.2189 - val_acc: 0.6553\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 62s 647us/step - loss: 0.2179 - acc: 0.6574 - val_loss: 0.2181 - val_acc: 0.6554\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2147 - acc: 0.6637 - val_loss: 0.2179 - val_acc: 0.6554\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2141 - acc: 0.6642 - val_loss: 0.2169 - val_acc: 0.6584\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2155 - acc: 0.6609 - val_loss: 0.2163 - val_acc: 0.6570\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2118 - acc: 0.6685 - val_loss: 0.2134 - val_acc: 0.6667\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.2157 - acc: 0.6615 - val_loss: 0.2152 - val_acc: 0.6603\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2103 - acc: 0.6709 - val_loss: 0.2140 - val_acc: 0.6620\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 71s 743us/step - loss: 0.2083 - acc: 0.6748 - val_loss: 0.2153 - val_acc: 0.6610\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2060 - acc: 0.6798 - val_loss: 0.2063 - val_acc: 0.6798\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2007 - acc: 0.6879 - val_loss: 0.2054 - val_acc: 0.6816\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1970 - acc: 0.6934 - val_loss: 0.1982 - val_acc: 0.6903\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1897 - acc: 0.7051 - val_loss: 0.1935 - val_acc: 0.6936\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1873 - acc: 0.7096 - val_loss: 0.1808 - val_acc: 0.7168\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 68s 713us/step - loss: 0.1867 - acc: 0.7133 - val_loss: 0.1871 - val_acc: 0.7105\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 65s 689us/step - loss: 0.1797 - acc: 0.7247 - val_loss: 0.1820 - val_acc: 0.7194\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 64s 678us/step - loss: 0.1771 - acc: 0.7297 - val_loss: 0.1772 - val_acc: 0.7259\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 84s 879us/step - loss: 0.1724 - acc: 0.7360 - val_loss: 0.1759 - val_acc: 0.7322\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 64s 669us/step - loss: 0.1664 - acc: 0.7455 - val_loss: 0.1892 - val_acc: 0.7067\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 64s 678us/step - loss: 0.1763 - acc: 0.7298 - val_loss: 0.1747 - val_acc: 0.7318\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 64s 672us/step - loss: 0.1615 - acc: 0.7553 - val_loss: 0.1572 - val_acc: 0.7604\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 89s 936us/step - loss: 0.1562 - acc: 0.7642 - val_loss: 0.1600 - val_acc: 0.7595\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 70s 741us/step - loss: 0.1552 - acc: 0.7655 - val_loss: 0.1540 - val_acc: 0.7677\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1486 - acc: 0.7763 - val_loss: 0.1497 - val_acc: 0.7761\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 118s 1ms/step - loss: 0.2275 - acc: 0.6399 - val_loss: 0.2259 - val_acc: 0.6377\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2245 - acc: 0.6442 - val_loss: 0.2246 - val_acc: 0.6431\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 76s 797us/step - loss: 0.2238 - acc: 0.6456 - val_loss: 0.2240 - val_acc: 0.6426\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.2214 - acc: 0.6507 - val_loss: 0.2233 - val_acc: 0.6482\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.2199 - acc: 0.6545 - val_loss: 0.2205 - val_acc: 0.6539\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2196 - acc: 0.6538 - val_loss: 0.2235 - val_acc: 0.6402\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 110s 1ms/step - loss: 0.2189 - acc: 0.6556 - val_loss: 0.2225 - val_acc: 0.6473\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2169 - acc: 0.6615 - val_loss: 0.2171 - val_acc: 0.6532\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2162 - acc: 0.6609 - val_loss: 0.2179 - val_acc: 0.6543\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2146 - acc: 0.6634 - val_loss: 0.2156 - val_acc: 0.6636\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 146s 2ms/step - loss: 0.2111 - acc: 0.6709 - val_loss: 0.2130 - val_acc: 0.6672\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 186s 2ms/step - loss: 0.2094 - acc: 0.6741 - val_loss: 0.2079 - val_acc: 0.6788\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 187s 2ms/step - loss: 0.2053 - acc: 0.6813 - val_loss: 0.2099 - val_acc: 0.6693\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 186s 2ms/step - loss: 0.2032 - acc: 0.6834 - val_loss: 0.2027 - val_acc: 0.6840\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 190s 2ms/step - loss: 0.1983 - acc: 0.6924 - val_loss: 0.1964 - val_acc: 0.6960\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 185s 2ms/step - loss: 0.1908 - acc: 0.7073 - val_loss: 0.1915 - val_acc: 0.7040\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 166s 2ms/step - loss: 0.1880 - acc: 0.7128 - val_loss: 0.1882 - val_acc: 0.7105\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 173s 2ms/step - loss: 0.1817 - acc: 0.7238 - val_loss: 0.1830 - val_acc: 0.7226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 123s 1ms/step - loss: 0.1771 - acc: 0.7312 - val_loss: 0.1742 - val_acc: 0.7342\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1703 - acc: 0.7423 - val_loss: 0.1685 - val_acc: 0.7462\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1676 - acc: 0.7471 - val_loss: 0.1741 - val_acc: 0.7348\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1645 - acc: 0.7522 - val_loss: 0.1717 - val_acc: 0.7394\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1602 - acc: 0.7583 - val_loss: 0.1584 - val_acc: 0.7598\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1550 - acc: 0.7684 - val_loss: 0.1545 - val_acc: 0.7665\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1497 - acc: 0.7760 - val_loss: 0.1534 - val_acc: 0.7674\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1601 - acc: 0.7634 - val_loss: 0.1646 - val_acc: 0.7542\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1503 - acc: 0.7783 - val_loss: 0.1468 - val_acc: 0.7822\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 72s 755us/step - loss: 0.1425 - acc: 0.7902 - val_loss: 0.1431 - val_acc: 0.7839\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 87s 921us/step - loss: 0.1411 - acc: 0.7917 - val_loss: 0.1433 - val_acc: 0.7864\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1403 - acc: 0.7959 - val_loss: 0.1389 - val_acc: 0.7914\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 123s 1ms/step - loss: 0.2280 - acc: 0.6387 - val_loss: 0.2258 - val_acc: 0.6371\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2247 - acc: 0.6434 - val_loss: 0.2253 - val_acc: 0.6395\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2238 - acc: 0.6445 - val_loss: 0.2250 - val_acc: 0.6383\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2226 - acc: 0.6463 - val_loss: 0.2229 - val_acc: 0.6443\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2224 - acc: 0.6465 - val_loss: 0.2225 - val_acc: 0.6426\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2205 - acc: 0.6505 - val_loss: 0.2212 - val_acc: 0.6483\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2179 - acc: 0.6548 - val_loss: 0.2185 - val_acc: 0.6508\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 92s 967us/step - loss: 0.2166 - acc: 0.6568 - val_loss: 0.2228 - val_acc: 0.6462\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 65s 686us/step - loss: 0.2164 - acc: 0.6592 - val_loss: 0.2162 - val_acc: 0.6568\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 74s 783us/step - loss: 0.2122 - acc: 0.6671 - val_loss: 0.2132 - val_acc: 0.6575\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2090 - acc: 0.6719 - val_loss: 0.2123 - val_acc: 0.6594\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2063 - acc: 0.6784 - val_loss: 0.2072 - val_acc: 0.6772\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2045 - acc: 0.6830 - val_loss: 0.2040 - val_acc: 0.6812\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1998 - acc: 0.6902 - val_loss: 0.2013 - val_acc: 0.6798\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1946 - acc: 0.6983 - val_loss: 0.1977 - val_acc: 0.6952\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1898 - acc: 0.7078 - val_loss: 0.1865 - val_acc: 0.7054\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1855 - acc: 0.7137 - val_loss: 0.1827 - val_acc: 0.7111\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1766 - acc: 0.7299 - val_loss: 0.1771 - val_acc: 0.7268\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1708 - acc: 0.7400 - val_loss: 0.1850 - val_acc: 0.7143\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 80s 843us/step - loss: 0.1674 - acc: 0.7468 - val_loss: 0.1672 - val_acc: 0.7444\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1587 - acc: 0.7600 - val_loss: 0.1555 - val_acc: 0.7628\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1511 - acc: 0.7709 - val_loss: 0.1480 - val_acc: 0.7719\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1404 - acc: 0.7882 - val_loss: 0.1443 - val_acc: 0.7777\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1394 - acc: 0.7885 - val_loss: 0.1364 - val_acc: 0.7930\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1409 - acc: 0.7880 - val_loss: 0.1471 - val_acc: 0.7812\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1313 - acc: 0.8020 - val_loss: 0.1316 - val_acc: 0.8026\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1244 - acc: 0.8128 - val_loss: 0.1326 - val_acc: 0.8017\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1206 - acc: 0.8198 - val_loss: 0.1228 - val_acc: 0.8114\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1155 - acc: 0.8272 - val_loss: 0.1190 - val_acc: 0.8247\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 78s 822us/step - loss: 0.1167 - acc: 0.8260 - val_loss: 0.1172 - val_acc: 0.8225\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 111s 1ms/step - loss: 0.2282 - acc: 0.6383 - val_loss: 0.2263 - val_acc: 0.6390\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2255 - acc: 0.6415 - val_loss: 0.2257 - val_acc: 0.6395\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2238 - acc: 0.6451 - val_loss: 0.2244 - val_acc: 0.6397\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2223 - acc: 0.6492 - val_loss: 0.2233 - val_acc: 0.6430\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2212 - acc: 0.6519 - val_loss: 0.2213 - val_acc: 0.6491\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2200 - acc: 0.6534 - val_loss: 0.2198 - val_acc: 0.6484\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2175 - acc: 0.6578 - val_loss: 0.2180 - val_acc: 0.6547\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2169 - acc: 0.6590 - val_loss: 0.2196 - val_acc: 0.6543\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 93s 982us/step - loss: 0.2166 - acc: 0.6588 - val_loss: 0.2178 - val_acc: 0.6562\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 85s 893us/step - loss: 0.2146 - acc: 0.6626 - val_loss: 0.2177 - val_acc: 0.6543\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2140 - acc: 0.6643 - val_loss: 0.2162 - val_acc: 0.6574\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2133 - acc: 0.6653 - val_loss: 0.2118 - val_acc: 0.6653\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2131 - acc: 0.6648 - val_loss: 0.2158 - val_acc: 0.6592\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2105 - acc: 0.6706 - val_loss: 0.2109 - val_acc: 0.6667\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2089 - acc: 0.6720 - val_loss: 0.2075 - val_acc: 0.6728\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2076 - acc: 0.6756 - val_loss: 0.2099 - val_acc: 0.6686\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2102 - acc: 0.6686 - val_loss: 0.2083 - val_acc: 0.6709\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2058 - acc: 0.6784 - val_loss: 0.2083 - val_acc: 0.6728\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 89s 940us/step - loss: 0.2053 - acc: 0.6805 - val_loss: 0.2101 - val_acc: 0.6668\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 89s 934us/step - loss: 0.2008 - acc: 0.6861 - val_loss: 0.2027 - val_acc: 0.6817\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1992 - acc: 0.6903 - val_loss: 0.1984 - val_acc: 0.6925\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2061 - acc: 0.6793 - val_loss: 0.2063 - val_acc: 0.6789\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1985 - acc: 0.6907 - val_loss: 0.1941 - val_acc: 0.6940\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1919 - acc: 0.7034 - val_loss: 0.1936 - val_acc: 0.7004\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1892 - acc: 0.7104 - val_loss: 0.1876 - val_acc: 0.7126\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1889 - acc: 0.7118 - val_loss: 0.1891 - val_acc: 0.7139\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1864 - acc: 0.7174 - val_loss: 0.1832 - val_acc: 0.7264\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1848 - acc: 0.7199 - val_loss: 0.1918 - val_acc: 0.7014\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 87s 913us/step - loss: 0.1817 - acc: 0.7248 - val_loss: 0.1809 - val_acc: 0.7267\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 92s 969us/step - loss: 0.1767 - acc: 0.7356 - val_loss: 0.1823 - val_acc: 0.7244\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 115s 1ms/step - loss: 0.2281 - acc: 0.6376 - val_loss: 0.2259 - val_acc: 0.6407\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2249 - acc: 0.6427 - val_loss: 0.2254 - val_acc: 0.6432\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2235 - acc: 0.6464 - val_loss: 0.2235 - val_acc: 0.6423\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2222 - acc: 0.6489 - val_loss: 0.2231 - val_acc: 0.6461\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2198 - acc: 0.6540 - val_loss: 0.2211 - val_acc: 0.6502\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2191 - acc: 0.6567 - val_loss: 0.2211 - val_acc: 0.6470\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2198 - acc: 0.6525 - val_loss: 0.2200 - val_acc: 0.6483\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2182 - acc: 0.6559 - val_loss: 0.2201 - val_acc: 0.6532\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 75s 788us/step - loss: 0.2158 - acc: 0.6613 - val_loss: 0.2195 - val_acc: 0.6501\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2162 - acc: 0.6621 - val_loss: 0.2173 - val_acc: 0.6591\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2129 - acc: 0.6672 - val_loss: 0.2150 - val_acc: 0.6620\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2108 - acc: 0.6706 - val_loss: 0.2120 - val_acc: 0.6640\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2103 - acc: 0.6713 - val_loss: 0.2108 - val_acc: 0.6680\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2075 - acc: 0.6757 - val_loss: 0.2109 - val_acc: 0.6671\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2050 - acc: 0.6808 - val_loss: 0.2095 - val_acc: 0.6741\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2121 - acc: 0.6691 - val_loss: 0.2108 - val_acc: 0.6666\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2066 - acc: 0.6767 - val_loss: 0.2025 - val_acc: 0.6871\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2020 - acc: 0.6848 - val_loss: 0.2031 - val_acc: 0.6863\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 75s 792us/step - loss: 0.2013 - acc: 0.6887 - val_loss: 0.1997 - val_acc: 0.6877\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1952 - acc: 0.6992 - val_loss: 0.1985 - val_acc: 0.6904\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1936 - acc: 0.7008 - val_loss: 0.1917 - val_acc: 0.7025\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1932 - acc: 0.7023 - val_loss: 0.1955 - val_acc: 0.6987\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1910 - acc: 0.7062 - val_loss: 0.1992 - val_acc: 0.6845\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1892 - acc: 0.7090 - val_loss: 0.1883 - val_acc: 0.7062\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1839 - acc: 0.7187 - val_loss: 0.1880 - val_acc: 0.7089\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1824 - acc: 0.7217 - val_loss: 0.1869 - val_acc: 0.7131\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1768 - acc: 0.7309 - val_loss: 0.1772 - val_acc: 0.7287\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1746 - acc: 0.7345 - val_loss: 0.1764 - val_acc: 0.7285\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 75s 785us/step - loss: 0.1841 - acc: 0.7194 - val_loss: 0.1798 - val_acc: 0.7238\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1691 - acc: 0.7427 - val_loss: 0.1674 - val_acc: 0.7448\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 118s 1ms/step - loss: 0.2271 - acc: 0.6394 - val_loss: 0.2257 - val_acc: 0.6378\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2245 - acc: 0.6439 - val_loss: 0.2252 - val_acc: 0.6401\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2231 - acc: 0.6472 - val_loss: 0.2223 - val_acc: 0.6480\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2226 - acc: 0.6487 - val_loss: 0.2245 - val_acc: 0.6417\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2200 - acc: 0.6544 - val_loss: 0.2202 - val_acc: 0.6536\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2179 - acc: 0.6590 - val_loss: 0.2195 - val_acc: 0.6499\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2177 - acc: 0.6587 - val_loss: 0.2193 - val_acc: 0.6534\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 74s 783us/step - loss: 0.2144 - acc: 0.6643 - val_loss: 0.2162 - val_acc: 0.6585\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2134 - acc: 0.6667 - val_loss: 0.2148 - val_acc: 0.6617\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2096 - acc: 0.6709 - val_loss: 0.2100 - val_acc: 0.6716\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2062 - acc: 0.6787 - val_loss: 0.2073 - val_acc: 0.6754\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2021 - acc: 0.6871 - val_loss: 0.2020 - val_acc: 0.6819\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1986 - acc: 0.6933 - val_loss: 0.1965 - val_acc: 0.6998\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1946 - acc: 0.7007 - val_loss: 0.1953 - val_acc: 0.6941\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1878 - acc: 0.7114 - val_loss: 0.1896 - val_acc: 0.7077\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1831 - acc: 0.7189 - val_loss: 0.1820 - val_acc: 0.7170\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1771 - acc: 0.7292 - val_loss: 0.1813 - val_acc: 0.7157\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 72s 759us/step - loss: 0.1715 - acc: 0.7370 - val_loss: 0.1692 - val_acc: 0.7375\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1629 - acc: 0.7526 - val_loss: 0.1665 - val_acc: 0.7433\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1587 - acc: 0.7587 - val_loss: 0.1596 - val_acc: 0.7569\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1593 - acc: 0.7595 - val_loss: 0.1563 - val_acc: 0.7638\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1494 - acc: 0.7743 - val_loss: 0.1508 - val_acc: 0.7667\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1447 - acc: 0.7806 - val_loss: 0.1406 - val_acc: 0.7891\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1434 - acc: 0.7840 - val_loss: 0.1440 - val_acc: 0.7782\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1367 - acc: 0.7960 - val_loss: 0.1358 - val_acc: 0.7943\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1309 - acc: 0.8053 - val_loss: 0.1292 - val_acc: 0.8060\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 93s 978us/step - loss: 0.1254 - acc: 0.8132 - val_loss: 0.1284 - val_acc: 0.8103\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 83s 869us/step - loss: 0.1267 - acc: 0.8127 - val_loss: 0.1233 - val_acc: 0.8196\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1209 - acc: 0.8213 - val_loss: 0.1409 - val_acc: 0.7900\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1343 - acc: 0.8035 - val_loss: 0.1878 - val_acc: 0.7266\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 118s 1ms/step - loss: 0.2283 - acc: 0.6369 - val_loss: 0.2258 - val_acc: 0.6381\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2247 - acc: 0.6430 - val_loss: 0.2250 - val_acc: 0.6382\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2235 - acc: 0.6455 - val_loss: 0.2229 - val_acc: 0.6456\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2226 - acc: 0.6474 - val_loss: 0.2229 - val_acc: 0.6472\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2210 - acc: 0.6508 - val_loss: 0.2215 - val_acc: 0.6494\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2192 - acc: 0.6544 - val_loss: 0.2206 - val_acc: 0.6500\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 71s 749us/step - loss: 0.2176 - acc: 0.6573 - val_loss: 0.2187 - val_acc: 0.6533\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2169 - acc: 0.6583 - val_loss: 0.2203 - val_acc: 0.6509\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2186 - acc: 0.6551 - val_loss: 0.2173 - val_acc: 0.6561\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2137 - acc: 0.6653 - val_loss: 0.2138 - val_acc: 0.6611\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2114 - acc: 0.6686 - val_loss: 0.2115 - val_acc: 0.6665\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2098 - acc: 0.6708 - val_loss: 0.2085 - val_acc: 0.6705\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2069 - acc: 0.6777 - val_loss: 0.2143 - val_acc: 0.6638\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2082 - acc: 0.6752 - val_loss: 0.2090 - val_acc: 0.6721\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2019 - acc: 0.6868 - val_loss: 0.1998 - val_acc: 0.6828\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 92s 965us/step - loss: 0.1962 - acc: 0.6957 - val_loss: 0.1954 - val_acc: 0.6968\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 81s 857us/step - loss: 0.1914 - acc: 0.7055 - val_loss: 0.1894 - val_acc: 0.7090\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1863 - acc: 0.7129 - val_loss: 0.1888 - val_acc: 0.7053\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1826 - acc: 0.7184 - val_loss: 0.1844 - val_acc: 0.7130\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1777 - acc: 0.7273 - val_loss: 0.1830 - val_acc: 0.7114\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1774 - acc: 0.7278 - val_loss: 0.1959 - val_acc: 0.6986\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1768 - acc: 0.7275 - val_loss: 0.1781 - val_acc: 0.7239\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1703 - acc: 0.7391 - val_loss: 0.1716 - val_acc: 0.7338\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1680 - acc: 0.7437 - val_loss: 0.1777 - val_acc: 0.7276\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1618 - acc: 0.7517 - val_loss: 0.1552 - val_acc: 0.7586\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 82s 866us/step - loss: 0.1645 - acc: 0.7493 - val_loss: 0.1708 - val_acc: 0.7384\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 85s 899us/step - loss: 0.1565 - acc: 0.7608 - val_loss: 0.1521 - val_acc: 0.7637\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1543 - acc: 0.7650 - val_loss: 0.1502 - val_acc: 0.7623\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1459 - acc: 0.7772 - val_loss: 0.1463 - val_acc: 0.7736\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1503 - acc: 0.7717 - val_loss: 0.1503 - val_acc: 0.7675\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 118s 1ms/step - loss: 0.2274 - acc: 0.6382 - val_loss: 0.2258 - val_acc: 0.6379\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2245 - acc: 0.6433 - val_loss: 0.2250 - val_acc: 0.6385\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2238 - acc: 0.6444 - val_loss: 0.2245 - val_acc: 0.6410\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2220 - acc: 0.6479 - val_loss: 0.2216 - val_acc: 0.6449\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2209 - acc: 0.6517 - val_loss: 0.2216 - val_acc: 0.6466\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 70s 732us/step - loss: 0.2192 - acc: 0.6534 - val_loss: 0.2197 - val_acc: 0.6516\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2181 - acc: 0.6575 - val_loss: 0.2197 - val_acc: 0.6490\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2195 - acc: 0.6532 - val_loss: 0.2185 - val_acc: 0.6519\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2153 - acc: 0.6617 - val_loss: 0.2172 - val_acc: 0.6575\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2133 - acc: 0.6642 - val_loss: 0.2134 - val_acc: 0.6603\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2114 - acc: 0.6690 - val_loss: 0.2147 - val_acc: 0.6602\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2060 - acc: 0.6771 - val_loss: 0.2077 - val_acc: 0.6663\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2033 - acc: 0.6827 - val_loss: 0.2061 - val_acc: 0.6761\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1981 - acc: 0.6913 - val_loss: 0.1983 - val_acc: 0.6871\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1909 - acc: 0.7038 - val_loss: 0.1923 - val_acc: 0.6932\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 74s 780us/step - loss: 0.1862 - acc: 0.7127 - val_loss: 0.2067 - val_acc: 0.6794\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1880 - acc: 0.7111 - val_loss: 0.1906 - val_acc: 0.7045\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1786 - acc: 0.7255 - val_loss: 0.1759 - val_acc: 0.7313\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1791 - acc: 0.7257 - val_loss: 0.1747 - val_acc: 0.7309\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1648 - acc: 0.7480 - val_loss: 0.1695 - val_acc: 0.7415\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1629 - acc: 0.7531 - val_loss: 0.1636 - val_acc: 0.7491\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1564 - acc: 0.7640 - val_loss: 0.1591 - val_acc: 0.7569\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1479 - acc: 0.7769 - val_loss: 0.1540 - val_acc: 0.7660\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1488 - acc: 0.7762 - val_loss: 0.1438 - val_acc: 0.7824\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 91s 962us/step - loss: 0.1439 - acc: 0.7833 - val_loss: 0.1412 - val_acc: 0.7830\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 75s 786us/step - loss: 0.1342 - acc: 0.7988 - val_loss: 0.1366 - val_acc: 0.7929\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1324 - acc: 0.8032 - val_loss: 0.1351 - val_acc: 0.8005\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1314 - acc: 0.8050 - val_loss: 0.1446 - val_acc: 0.7895\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1294 - acc: 0.8082 - val_loss: 0.1294 - val_acc: 0.8055\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1237 - acc: 0.8169 - val_loss: 0.1318 - val_acc: 0.8068\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 117s 1ms/step - loss: 0.2276 - acc: 0.6382 - val_loss: 0.2255 - val_acc: 0.6376\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2240 - acc: 0.6446 - val_loss: 0.2234 - val_acc: 0.6436\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2228 - acc: 0.6466 - val_loss: 0.2241 - val_acc: 0.6410\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2218 - acc: 0.6497 - val_loss: 0.2227 - val_acc: 0.6465\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 72s 763us/step - loss: 0.2195 - acc: 0.6538 - val_loss: 0.2206 - val_acc: 0.6487\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 92s 973us/step - loss: 0.2183 - acc: 0.6569 - val_loss: 0.2191 - val_acc: 0.6518\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2173 - acc: 0.6591 - val_loss: 0.2183 - val_acc: 0.6529\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2162 - acc: 0.6605 - val_loss: 0.2173 - val_acc: 0.6557\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2138 - acc: 0.6646 - val_loss: 0.2160 - val_acc: 0.6577\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2117 - acc: 0.6678 - val_loss: 0.2119 - val_acc: 0.6623\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2104 - acc: 0.6699 - val_loss: 0.2110 - val_acc: 0.6670\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2118 - acc: 0.6656 - val_loss: 0.2093 - val_acc: 0.6690\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2065 - acc: 0.6765 - val_loss: 0.2060 - val_acc: 0.6716\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2062 - acc: 0.6776 - val_loss: 0.2057 - val_acc: 0.6800\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 69s 729us/step - loss: 0.2013 - acc: 0.6854 - val_loss: 0.1999 - val_acc: 0.6843\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1996 - acc: 0.6886 - val_loss: 0.2011 - val_acc: 0.6865\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2003 - acc: 0.6890 - val_loss: 0.1955 - val_acc: 0.6990\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1943 - acc: 0.6984 - val_loss: 0.1898 - val_acc: 0.7019\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1876 - acc: 0.7099 - val_loss: 0.1933 - val_acc: 0.7046\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1883 - acc: 0.7099 - val_loss: 0.1879 - val_acc: 0.7073\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1841 - acc: 0.7157 - val_loss: 0.1820 - val_acc: 0.7138\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1759 - acc: 0.7285 - val_loss: 0.1742 - val_acc: 0.7311\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1740 - acc: 0.7334 - val_loss: 0.1783 - val_acc: 0.7243\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1736 - acc: 0.7344 - val_loss: 0.1704 - val_acc: 0.7389\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 70s 734us/step - loss: 0.1654 - acc: 0.7458 - val_loss: 0.1695 - val_acc: 0.7369\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1706 - acc: 0.7389 - val_loss: 0.1639 - val_acc: 0.7445\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1579 - acc: 0.7587 - val_loss: 0.1599 - val_acc: 0.7563\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1586 - acc: 0.7597 - val_loss: 0.1652 - val_acc: 0.7481\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1600 - acc: 0.7580 - val_loss: 0.1575 - val_acc: 0.7596\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1478 - acc: 0.7773 - val_loss: 0.1526 - val_acc: 0.7644\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 118s 1ms/step - loss: 0.2274 - acc: 0.6393 - val_loss: 0.2261 - val_acc: 0.6376\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2247 - acc: 0.6435 - val_loss: 0.2249 - val_acc: 0.6410\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2230 - acc: 0.6471 - val_loss: 0.2232 - val_acc: 0.6472\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2211 - acc: 0.6519 - val_loss: 0.2212 - val_acc: 0.6494\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 60s 632us/step - loss: 0.2199 - acc: 0.6540 - val_loss: 0.2205 - val_acc: 0.6535\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2182 - acc: 0.6580 - val_loss: 0.2215 - val_acc: 0.6503\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2174 - acc: 0.6585 - val_loss: 0.2184 - val_acc: 0.6547\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2156 - acc: 0.6620 - val_loss: 0.2170 - val_acc: 0.6587\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2141 - acc: 0.6629 - val_loss: 0.2172 - val_acc: 0.6576\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2121 - acc: 0.6672 - val_loss: 0.2138 - val_acc: 0.6625\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2086 - acc: 0.6733 - val_loss: 0.2141 - val_acc: 0.6645\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2081 - acc: 0.6751 - val_loss: 0.2067 - val_acc: 0.6720\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2021 - acc: 0.6852 - val_loss: 0.2036 - val_acc: 0.6759\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1970 - acc: 0.6945 - val_loss: 0.1997 - val_acc: 0.6886\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 63s 667us/step - loss: 0.1952 - acc: 0.6987 - val_loss: 0.1950 - val_acc: 0.6937\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1881 - acc: 0.7113 - val_loss: 0.1934 - val_acc: 0.6997\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1844 - acc: 0.7182 - val_loss: 0.1854 - val_acc: 0.7081\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1803 - acc: 0.7255 - val_loss: 0.1807 - val_acc: 0.7248\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1758 - acc: 0.7333 - val_loss: 0.1823 - val_acc: 0.7213\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1713 - acc: 0.7401 - val_loss: 0.1703 - val_acc: 0.7405\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1640 - acc: 0.7514 - val_loss: 0.1613 - val_acc: 0.7537\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1570 - acc: 0.7636 - val_loss: 0.1634 - val_acc: 0.7511\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1520 - acc: 0.7712 - val_loss: 0.1497 - val_acc: 0.7732\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1468 - acc: 0.7799 - val_loss: 0.1522 - val_acc: 0.7687\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 67s 709us/step - loss: 0.1473 - acc: 0.7787 - val_loss: 0.1471 - val_acc: 0.7779\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 90s 952us/step - loss: 0.1408 - acc: 0.7885 - val_loss: 0.1469 - val_acc: 0.7822\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1471 - acc: 0.7805 - val_loss: 0.1391 - val_acc: 0.7910\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1378 - acc: 0.7940 - val_loss: 0.1358 - val_acc: 0.7956\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1366 - acc: 0.7959 - val_loss: 0.1348 - val_acc: 0.7971\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1285 - acc: 0.8067 - val_loss: 0.1330 - val_acc: 0.8025\n",
      "0.822459584341497\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 122s 1ms/step - loss: 0.2274 - acc: 0.6386 - val_loss: 0.2257 - val_acc: 0.6385\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2244 - acc: 0.6437 - val_loss: 0.2249 - val_acc: 0.6403\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2232 - acc: 0.6455 - val_loss: 0.2252 - val_acc: 0.6419\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 92s 970us/step - loss: 0.2219 - acc: 0.6490 - val_loss: 0.2219 - val_acc: 0.6442\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 70s 734us/step - loss: 0.2219 - acc: 0.6482 - val_loss: 0.2231 - val_acc: 0.6400\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2206 - acc: 0.6501 - val_loss: 0.2208 - val_acc: 0.6427\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2188 - acc: 0.6538 - val_loss: 0.2204 - val_acc: 0.6491\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2169 - acc: 0.6575 - val_loss: 0.2182 - val_acc: 0.6525\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2146 - acc: 0.6625 - val_loss: 0.2172 - val_acc: 0.6517\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2131 - acc: 0.6653 - val_loss: 0.2141 - val_acc: 0.6581\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2111 - acc: 0.6685 - val_loss: 0.2148 - val_acc: 0.6588\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2146 - acc: 0.6609 - val_loss: 0.2203 - val_acc: 0.6513\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2124 - acc: 0.6649 - val_loss: 0.2144 - val_acc: 0.6600\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 93s 976us/step - loss: 0.2092 - acc: 0.6712 - val_loss: 0.2092 - val_acc: 0.6717\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2048 - acc: 0.6789 - val_loss: 0.2053 - val_acc: 0.6778\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2020 - acc: 0.6846 - val_loss: 0.2009 - val_acc: 0.6853\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2019 - acc: 0.6867 - val_loss: 0.2143 - val_acc: 0.6606\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2052 - acc: 0.6811 - val_loss: 0.2008 - val_acc: 0.6899\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1954 - acc: 0.6977 - val_loss: 0.1965 - val_acc: 0.6964\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1865 - acc: 0.7137 - val_loss: 0.1893 - val_acc: 0.7085\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1810 - acc: 0.7234 - val_loss: 0.1841 - val_acc: 0.7178\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1767 - acc: 0.7309 - val_loss: 0.1769 - val_acc: 0.7309\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1672 - acc: 0.7464 - val_loss: 0.1733 - val_acc: 0.7369\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 88s 924us/step - loss: 0.1648 - acc: 0.7508 - val_loss: 0.1669 - val_acc: 0.7511\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 70s 738us/step - loss: 0.1624 - acc: 0.7571 - val_loss: 0.1636 - val_acc: 0.7521\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1552 - acc: 0.7671 - val_loss: 0.1601 - val_acc: 0.7607\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1502 - acc: 0.7749 - val_loss: 0.1620 - val_acc: 0.7552\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1483 - acc: 0.7775 - val_loss: 0.1478 - val_acc: 0.7740\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1441 - acc: 0.7851 - val_loss: 0.1462 - val_acc: 0.7794\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1400 - acc: 0.7906 - val_loss: 0.1499 - val_acc: 0.7750\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 124s 1ms/step - loss: 0.2275 - acc: 0.6381 - val_loss: 0.2261 - val_acc: 0.6366\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2250 - acc: 0.6423 - val_loss: 0.2249 - val_acc: 0.6395\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2236 - acc: 0.6454 - val_loss: 0.2237 - val_acc: 0.6444\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 61s 637us/step - loss: 0.2216 - acc: 0.6509 - val_loss: 0.2217 - val_acc: 0.6478\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2196 - acc: 0.6563 - val_loss: 0.2198 - val_acc: 0.6501\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2213 - acc: 0.6534 - val_loss: 0.2226 - val_acc: 0.6508\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2182 - acc: 0.6574 - val_loss: 0.2187 - val_acc: 0.6525\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2163 - acc: 0.6607 - val_loss: 0.2166 - val_acc: 0.6565\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2156 - acc: 0.6619 - val_loss: 0.2155 - val_acc: 0.6561\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2131 - acc: 0.6665 - val_loss: 0.2156 - val_acc: 0.6633\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2107 - acc: 0.6721 - val_loss: 0.2107 - val_acc: 0.6720\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2093 - acc: 0.6734 - val_loss: 0.2109 - val_acc: 0.6671\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2075 - acc: 0.6766 - val_loss: 0.2111 - val_acc: 0.6715\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2050 - acc: 0.6827 - val_loss: 0.2073 - val_acc: 0.6758\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 92s 973us/step - loss: 0.2020 - acc: 0.6883 - val_loss: 0.2006 - val_acc: 0.6863\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1987 - acc: 0.6941 - val_loss: 0.2046 - val_acc: 0.6794\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2012 - acc: 0.6899 - val_loss: 0.2106 - val_acc: 0.6719\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1984 - acc: 0.6944 - val_loss: 0.1941 - val_acc: 0.6960\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1898 - acc: 0.7101 - val_loss: 0.1917 - val_acc: 0.7054\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1879 - acc: 0.7142 - val_loss: 0.1969 - val_acc: 0.6912\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1898 - acc: 0.7114 - val_loss: 0.1910 - val_acc: 0.7046\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1802 - acc: 0.7258 - val_loss: 0.1821 - val_acc: 0.7215\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1754 - acc: 0.7339 - val_loss: 0.1753 - val_acc: 0.7310\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 642us/step - loss: 0.1711 - acc: 0.7396 - val_loss: 0.1701 - val_acc: 0.7350\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 82s 864us/step - loss: 0.1719 - acc: 0.7390 - val_loss: 0.1794 - val_acc: 0.7231\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1689 - acc: 0.7435 - val_loss: 0.1736 - val_acc: 0.7275\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1645 - acc: 0.7501 - val_loss: 0.1649 - val_acc: 0.7475\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1587 - acc: 0.7604 - val_loss: 0.1650 - val_acc: 0.7493\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1598 - acc: 0.7598 - val_loss: 0.1655 - val_acc: 0.7489\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1515 - acc: 0.7722 - val_loss: 0.1489 - val_acc: 0.7701\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 124s 1ms/step - loss: 0.2275 - acc: 0.6382 - val_loss: 0.2259 - val_acc: 0.6410\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2245 - acc: 0.6424 - val_loss: 0.2246 - val_acc: 0.6408\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2222 - acc: 0.6481 - val_loss: 0.2220 - val_acc: 0.6467\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 60s 630us/step - loss: 0.2210 - acc: 0.6516 - val_loss: 0.2236 - val_acc: 0.6454\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2194 - acc: 0.6552 - val_loss: 0.2200 - val_acc: 0.6512\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2181 - acc: 0.6570 - val_loss: 0.2211 - val_acc: 0.6544\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2159 - acc: 0.6620 - val_loss: 0.2167 - val_acc: 0.6589\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2135 - acc: 0.6650 - val_loss: 0.2166 - val_acc: 0.6562\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2139 - acc: 0.6648 - val_loss: 0.2145 - val_acc: 0.6544\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2107 - acc: 0.6686 - val_loss: 0.2129 - val_acc: 0.6664\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2068 - acc: 0.6772 - val_loss: 0.2065 - val_acc: 0.6752\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2079 - acc: 0.6755 - val_loss: 0.2087 - val_acc: 0.6754\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.2035 - acc: 0.6832 - val_loss: 0.2028 - val_acc: 0.6778\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 61s 641us/step - loss: 0.2005 - acc: 0.6874 - val_loss: 0.2026 - val_acc: 0.6805\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 91s 955us/step - loss: 0.1958 - acc: 0.6973 - val_loss: 0.1961 - val_acc: 0.6953\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1911 - acc: 0.7055 - val_loss: 0.1952 - val_acc: 0.6877\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1884 - acc: 0.7090 - val_loss: 0.1870 - val_acc: 0.7039\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1841 - acc: 0.7175 - val_loss: 0.1894 - val_acc: 0.7070\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1836 - acc: 0.7182 - val_loss: 0.1858 - val_acc: 0.7143\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1794 - acc: 0.7258 - val_loss: 0.1854 - val_acc: 0.7143\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1746 - acc: 0.7345 - val_loss: 0.1719 - val_acc: 0.7390\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1649 - acc: 0.7499 - val_loss: 0.1603 - val_acc: 0.7573\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 104s 1ms/step - loss: 0.1602 - acc: 0.7578 - val_loss: 0.1773 - val_acc: 0.7290\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 640us/step - loss: 0.1615 - acc: 0.7548 - val_loss: 0.1642 - val_acc: 0.7520\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 91s 958us/step - loss: 0.1553 - acc: 0.7662 - val_loss: 0.1529 - val_acc: 0.7691\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1494 - acc: 0.7742 - val_loss: 0.1679 - val_acc: 0.7481\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1554 - acc: 0.7667 - val_loss: 0.1471 - val_acc: 0.7745\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1432 - acc: 0.7847 - val_loss: 0.1452 - val_acc: 0.7785\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1355 - acc: 0.7961 - val_loss: 0.1338 - val_acc: 0.7976\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1303 - acc: 0.8048 - val_loss: 0.1331 - val_acc: 0.8013\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 125s 1ms/step - loss: 0.2278 - acc: 0.6368 - val_loss: 0.2260 - val_acc: 0.6373\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2245 - acc: 0.6428 - val_loss: 0.2251 - val_acc: 0.6407\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 84s 882us/step - loss: 0.2237 - acc: 0.6450 - val_loss: 0.2244 - val_acc: 0.6473\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 64s 669us/step - loss: 0.2228 - acc: 0.6481 - val_loss: 0.2244 - val_acc: 0.6433\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2214 - acc: 0.6515 - val_loss: 0.2226 - val_acc: 0.6463\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2201 - acc: 0.6543 - val_loss: 0.2231 - val_acc: 0.6460\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2194 - acc: 0.6557 - val_loss: 0.2222 - val_acc: 0.6473\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2196 - acc: 0.6543 - val_loss: 0.2193 - val_acc: 0.6525\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2195 - acc: 0.6565 - val_loss: 0.2186 - val_acc: 0.6569\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2198 - acc: 0.6566 - val_loss: 0.2221 - val_acc: 0.6492\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2177 - acc: 0.6596 - val_loss: 0.2172 - val_acc: 0.6587\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2155 - acc: 0.6630 - val_loss: 0.2155 - val_acc: 0.6568\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 84s 883us/step - loss: 0.2137 - acc: 0.6666 - val_loss: 0.2164 - val_acc: 0.6573\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 654us/step - loss: 0.2116 - acc: 0.6707 - val_loss: 0.2156 - val_acc: 0.6604\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.2099 - acc: 0.6741 - val_loss: 0.2117 - val_acc: 0.6679\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2074 - acc: 0.6787 - val_loss: 0.2095 - val_acc: 0.6712\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2059 - acc: 0.6814 - val_loss: 0.2085 - val_acc: 0.6721\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2020 - acc: 0.6869 - val_loss: 0.2043 - val_acc: 0.6796\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1996 - acc: 0.6922 - val_loss: 0.1994 - val_acc: 0.6881\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1957 - acc: 0.6979 - val_loss: 0.2018 - val_acc: 0.6900\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1970 - acc: 0.6955 - val_loss: 0.1977 - val_acc: 0.6947\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1918 - acc: 0.7048 - val_loss: 0.1908 - val_acc: 0.7050\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 80s 845us/step - loss: 0.1870 - acc: 0.7123 - val_loss: 0.1896 - val_acc: 0.7061\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 61s 645us/step - loss: 0.1830 - acc: 0.7197 - val_loss: 0.1845 - val_acc: 0.7109\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1860 - acc: 0.7166 - val_loss: 0.1830 - val_acc: 0.7215\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1784 - acc: 0.7294 - val_loss: 0.1796 - val_acc: 0.7301\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1696 - acc: 0.7433 - val_loss: 0.1688 - val_acc: 0.7462\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1673 - acc: 0.7469 - val_loss: 0.1718 - val_acc: 0.7330\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1675 - acc: 0.7468 - val_loss: 0.1700 - val_acc: 0.7404\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 112s 1ms/step - loss: 0.1626 - acc: 0.7552 - val_loss: 0.1682 - val_acc: 0.7425\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 129s 1ms/step - loss: 0.2284 - acc: 0.6378 - val_loss: 0.2263 - val_acc: 0.6387\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2248 - acc: 0.6424 - val_loss: 0.2250 - val_acc: 0.6391\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 67s 705us/step - loss: 0.2232 - acc: 0.6459 - val_loss: 0.2237 - val_acc: 0.6441\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 76s 804us/step - loss: 0.2217 - acc: 0.6502 - val_loss: 0.2229 - val_acc: 0.6463\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2195 - acc: 0.6542 - val_loss: 0.2204 - val_acc: 0.6505\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2179 - acc: 0.6562 - val_loss: 0.2208 - val_acc: 0.6521\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2169 - acc: 0.6578 - val_loss: 0.2179 - val_acc: 0.6491\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2156 - acc: 0.6607 - val_loss: 0.2163 - val_acc: 0.6580\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2135 - acc: 0.6629 - val_loss: 0.2157 - val_acc: 0.6557\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2112 - acc: 0.6673 - val_loss: 0.2134 - val_acc: 0.6637\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2081 - acc: 0.6734 - val_loss: 0.2081 - val_acc: 0.6717\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2030 - acc: 0.6819 - val_loss: 0.2056 - val_acc: 0.6774\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 71s 752us/step - loss: 0.2013 - acc: 0.6850 - val_loss: 0.2064 - val_acc: 0.6742\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 70s 741us/step - loss: 0.2002 - acc: 0.6887 - val_loss: 0.2009 - val_acc: 0.6824\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1944 - acc: 0.6989 - val_loss: 0.1965 - val_acc: 0.6931\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1861 - acc: 0.7133 - val_loss: 0.1926 - val_acc: 0.7034\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1831 - acc: 0.7196 - val_loss: 0.1813 - val_acc: 0.7232\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1748 - acc: 0.7339 - val_loss: 0.1773 - val_acc: 0.7281\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1763 - acc: 0.7314 - val_loss: 0.1867 - val_acc: 0.7118\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1775 - acc: 0.7323 - val_loss: 0.1799 - val_acc: 0.7268\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1677 - acc: 0.7475 - val_loss: 0.1627 - val_acc: 0.7567\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1585 - acc: 0.7624 - val_loss: 0.1564 - val_acc: 0.7606\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 75s 788us/step - loss: 0.1513 - acc: 0.7723 - val_loss: 0.1621 - val_acc: 0.7566\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 65s 686us/step - loss: 0.1574 - acc: 0.7658 - val_loss: 0.1517 - val_acc: 0.7732\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1490 - acc: 0.7782 - val_loss: 0.1509 - val_acc: 0.7766\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1461 - acc: 0.7841 - val_loss: 0.1454 - val_acc: 0.7854\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1407 - acc: 0.7923 - val_loss: 0.1469 - val_acc: 0.7832\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1352 - acc: 0.8011 - val_loss: 0.1396 - val_acc: 0.7933\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1389 - acc: 0.7960 - val_loss: 0.1401 - val_acc: 0.7938\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1263 - acc: 0.8163 - val_loss: 0.1295 - val_acc: 0.8122\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 126s 1ms/step - loss: 0.2279 - acc: 0.6352 - val_loss: 0.2255 - val_acc: 0.6392\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2248 - acc: 0.6419 - val_loss: 0.2251 - val_acc: 0.6365\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2238 - acc: 0.6439 - val_loss: 0.2239 - val_acc: 0.6409\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 63s 667us/step - loss: 0.2220 - acc: 0.6475 - val_loss: 0.2221 - val_acc: 0.6452\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2212 - acc: 0.6503 - val_loss: 0.2225 - val_acc: 0.6457\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2198 - acc: 0.6525 - val_loss: 0.2199 - val_acc: 0.6490\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2184 - acc: 0.6553 - val_loss: 0.2190 - val_acc: 0.6522\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2154 - acc: 0.6617 - val_loss: 0.2159 - val_acc: 0.6550\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2142 - acc: 0.6633 - val_loss: 0.2157 - val_acc: 0.6568\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2117 - acc: 0.6676 - val_loss: 0.2150 - val_acc: 0.6615\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2085 - acc: 0.6733 - val_loss: 0.2156 - val_acc: 0.6605\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2076 - acc: 0.6749 - val_loss: 0.2058 - val_acc: 0.6725\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 86s 910us/step - loss: 0.2025 - acc: 0.6846 - val_loss: 0.2013 - val_acc: 0.6799\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 60s 633us/step - loss: 0.1983 - acc: 0.6910 - val_loss: 0.1976 - val_acc: 0.6858\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 94s 989us/step - loss: 0.1943 - acc: 0.6982 - val_loss: 0.1911 - val_acc: 0.6997\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1887 - acc: 0.7073 - val_loss: 0.2033 - val_acc: 0.6818\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1874 - acc: 0.7089 - val_loss: 0.1870 - val_acc: 0.7105\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1819 - acc: 0.7182 - val_loss: 0.1889 - val_acc: 0.7011\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1880 - acc: 0.7096 - val_loss: 0.1968 - val_acc: 0.6883\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1807 - acc: 0.7204 - val_loss: 0.1783 - val_acc: 0.7259\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1756 - acc: 0.7280 - val_loss: 0.1722 - val_acc: 0.7277\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1666 - acc: 0.7409 - val_loss: 0.1679 - val_acc: 0.7397\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1611 - acc: 0.7515 - val_loss: 0.1675 - val_acc: 0.7409\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 60s 630us/step - loss: 0.1623 - acc: 0.7489 - val_loss: 0.1681 - val_acc: 0.7363\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 78s 819us/step - loss: 0.1588 - acc: 0.7545 - val_loss: 0.1686 - val_acc: 0.7383\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1661 - acc: 0.7467 - val_loss: 0.1575 - val_acc: 0.7553\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1519 - acc: 0.7682 - val_loss: 0.1651 - val_acc: 0.7467\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1490 - acc: 0.7738 - val_loss: 0.1495 - val_acc: 0.7726\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1429 - acc: 0.7837 - val_loss: 0.1451 - val_acc: 0.7798\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1425 - acc: 0.7848 - val_loss: 0.1514 - val_acc: 0.7741\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 128s 1ms/step - loss: 0.2274 - acc: 0.6387 - val_loss: 0.2254 - val_acc: 0.6394\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2240 - acc: 0.6446 - val_loss: 0.2246 - val_acc: 0.6411\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2241 - acc: 0.6441 - val_loss: 0.2241 - val_acc: 0.6414\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 60s 634us/step - loss: 0.2225 - acc: 0.6477 - val_loss: 0.2221 - val_acc: 0.6459\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 79s 834us/step - loss: 0.2210 - acc: 0.6499 - val_loss: 0.2213 - val_acc: 0.6487\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2193 - acc: 0.6530 - val_loss: 0.2212 - val_acc: 0.6496\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2188 - acc: 0.6537 - val_loss: 0.2202 - val_acc: 0.6449\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2170 - acc: 0.6568 - val_loss: 0.2190 - val_acc: 0.6488\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2165 - acc: 0.6568 - val_loss: 0.2178 - val_acc: 0.6512\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2139 - acc: 0.6624 - val_loss: 0.2166 - val_acc: 0.6586\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2125 - acc: 0.6663 - val_loss: 0.2160 - val_acc: 0.6603\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2105 - acc: 0.6702 - val_loss: 0.2109 - val_acc: 0.6658\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2115 - acc: 0.6691 - val_loss: 0.2168 - val_acc: 0.6581\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 72s 753us/step - loss: 0.2104 - acc: 0.6719 - val_loss: 0.2083 - val_acc: 0.6754\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 60s 632us/step - loss: 0.2079 - acc: 0.6770 - val_loss: 0.2105 - val_acc: 0.6755\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2056 - acc: 0.6803 - val_loss: 0.2092 - val_acc: 0.6748\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2010 - acc: 0.6906 - val_loss: 0.1980 - val_acc: 0.6952\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1955 - acc: 0.6985 - val_loss: 0.1983 - val_acc: 0.6919\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1905 - acc: 0.7072 - val_loss: 0.2005 - val_acc: 0.6830\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1899 - acc: 0.7080 - val_loss: 0.1963 - val_acc: 0.6954\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1935 - acc: 0.7014 - val_loss: 0.1883 - val_acc: 0.7094\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1807 - acc: 0.7240 - val_loss: 0.1835 - val_acc: 0.7156\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1771 - acc: 0.7287 - val_loss: 0.1818 - val_acc: 0.7156\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1721 - acc: 0.7365 - val_loss: 0.1760 - val_acc: 0.7314\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 60s 633us/step - loss: 0.1665 - acc: 0.7464 - val_loss: 0.1653 - val_acc: 0.7426\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 76s 799us/step - loss: 0.1609 - acc: 0.7554 - val_loss: 0.1660 - val_acc: 0.7473\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1596 - acc: 0.7577 - val_loss: 0.1646 - val_acc: 0.7457\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.1581 - acc: 0.7603 - val_loss: 0.1760 - val_acc: 0.7283\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1606 - acc: 0.7563 - val_loss: 0.1571 - val_acc: 0.7545\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1481 - acc: 0.7735 - val_loss: 0.1487 - val_acc: 0.7718\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 128s 1ms/step - loss: 0.2281 - acc: 0.6363 - val_loss: 0.2258 - val_acc: 0.6376\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2250 - acc: 0.6415 - val_loss: 0.2244 - val_acc: 0.6397\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2236 - acc: 0.6449 - val_loss: 0.2234 - val_acc: 0.6431\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2230 - acc: 0.6460 - val_loss: 0.2225 - val_acc: 0.6455\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 60s 635us/step - loss: 0.2210 - acc: 0.6500 - val_loss: 0.2231 - val_acc: 0.6432\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 75s 791us/step - loss: 0.2204 - acc: 0.6511 - val_loss: 0.2217 - val_acc: 0.6460\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2187 - acc: 0.6557 - val_loss: 0.2205 - val_acc: 0.6495\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2170 - acc: 0.6589 - val_loss: 0.2184 - val_acc: 0.6507\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2154 - acc: 0.6609 - val_loss: 0.2176 - val_acc: 0.6556\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2136 - acc: 0.6645 - val_loss: 0.2148 - val_acc: 0.6608\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2136 - acc: 0.6644 - val_loss: 0.2145 - val_acc: 0.6592\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2104 - acc: 0.6703 - val_loss: 0.2126 - val_acc: 0.6658\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 110s 1ms/step - loss: 0.2068 - acc: 0.6773 - val_loss: 0.2125 - val_acc: 0.6568\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2076 - acc: 0.6750 - val_loss: 0.2063 - val_acc: 0.6755\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 72s 761us/step - loss: 0.2024 - acc: 0.6843 - val_loss: 0.2025 - val_acc: 0.6836\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 60s 635us/step - loss: 0.1991 - acc: 0.6910 - val_loss: 0.2037 - val_acc: 0.6748\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2038 - acc: 0.6859 - val_loss: 0.2036 - val_acc: 0.6848\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1975 - acc: 0.6943 - val_loss: 0.1957 - val_acc: 0.6941\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1891 - acc: 0.7080 - val_loss: 0.1893 - val_acc: 0.7085\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1872 - acc: 0.7120 - val_loss: 0.1884 - val_acc: 0.7102\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1773 - acc: 0.7278 - val_loss: 0.1798 - val_acc: 0.7187\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1759 - acc: 0.7308 - val_loss: 0.1823 - val_acc: 0.7155\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1713 - acc: 0.7377 - val_loss: 0.1680 - val_acc: 0.7393\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1663 - acc: 0.7458 - val_loss: 0.1699 - val_acc: 0.7371\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1655 - acc: 0.7480 - val_loss: 0.1668 - val_acc: 0.7455\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 60s 633us/step - loss: 0.1631 - acc: 0.7519 - val_loss: 0.1833 - val_acc: 0.7229\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.1662 - acc: 0.7483 - val_loss: 0.1623 - val_acc: 0.7539\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1642 - acc: 0.7517 - val_loss: 0.1698 - val_acc: 0.7402\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1583 - acc: 0.7613 - val_loss: 0.1559 - val_acc: 0.7688\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1486 - acc: 0.7749 - val_loss: 0.1511 - val_acc: 0.7742\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 130s 1ms/step - loss: 0.2275 - acc: 0.6397 - val_loss: 0.2257 - val_acc: 0.6389\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2244 - acc: 0.6431 - val_loss: 0.2280 - val_acc: 0.6362\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2230 - acc: 0.6461 - val_loss: 0.2228 - val_acc: 0.6451\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2206 - acc: 0.6516 - val_loss: 0.2224 - val_acc: 0.6458\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2193 - acc: 0.6543 - val_loss: 0.2208 - val_acc: 0.6473\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 59s 624us/step - loss: 0.2190 - acc: 0.6556 - val_loss: 0.2213 - val_acc: 0.6482\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 60s 627us/step - loss: 0.2192 - acc: 0.6539 - val_loss: 0.2309 - val_acc: 0.6258\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 95s 1ms/step - loss: 0.2240 - acc: 0.6465 - val_loss: 0.2218 - val_acc: 0.6492\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2176 - acc: 0.6574 - val_loss: 0.2171 - val_acc: 0.6566\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.2142 - acc: 0.6636 - val_loss: 0.2159 - val_acc: 0.6582\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2123 - acc: 0.6669 - val_loss: 0.2117 - val_acc: 0.6593\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.2103 - acc: 0.6687 - val_loss: 0.2096 - val_acc: 0.6659\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 109s 1ms/step - loss: 0.2082 - acc: 0.6737 - val_loss: 0.2069 - val_acc: 0.6700\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.2060 - acc: 0.6786 - val_loss: 0.2042 - val_acc: 0.6800\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.2012 - acc: 0.6866 - val_loss: 0.2003 - val_acc: 0.6864\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1972 - acc: 0.6939 - val_loss: 0.1984 - val_acc: 0.6857\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 60s 633us/step - loss: 0.1948 - acc: 0.6985 - val_loss: 0.1924 - val_acc: 0.7023\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 59s 625us/step - loss: 0.1873 - acc: 0.7116 - val_loss: 0.1876 - val_acc: 0.7048\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1802 - acc: 0.7234 - val_loss: 0.1792 - val_acc: 0.7235\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 105s 1ms/step - loss: 0.1773 - acc: 0.7277 - val_loss: 0.1787 - val_acc: 0.7215\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1717 - acc: 0.7382 - val_loss: 0.1779 - val_acc: 0.7279\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1725 - acc: 0.7343 - val_loss: 0.1684 - val_acc: 0.7424\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1622 - acc: 0.7514 - val_loss: 0.1614 - val_acc: 0.7539\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 108s 1ms/step - loss: 0.1606 - acc: 0.7569 - val_loss: 0.1684 - val_acc: 0.7433\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 107s 1ms/step - loss: 0.1529 - acc: 0.7695 - val_loss: 0.1574 - val_acc: 0.7645\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1569 - acc: 0.7655 - val_loss: 0.1612 - val_acc: 0.7588\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 94s 990us/step - loss: 0.1491 - acc: 0.7768 - val_loss: 0.1530 - val_acc: 0.7739\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1401 - acc: 0.7928 - val_loss: 0.1402 - val_acc: 0.7937\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.1421 - acc: 0.7906 - val_loss: 0.1656 - val_acc: 0.7566\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.1718 - acc: 0.7408 - val_loss: 0.1537 - val_acc: 0.7677\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 75s 793us/step - loss: 0.2274 - acc: 0.6381 - val_loss: 0.2257 - val_acc: 0.6383\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2248 - acc: 0.6422 - val_loss: 0.2247 - val_acc: 0.6415\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2230 - acc: 0.6457 - val_loss: 0.2223 - val_acc: 0.6460\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.2218 - acc: 0.6485 - val_loss: 0.2240 - val_acc: 0.6418\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2200 - acc: 0.6520 - val_loss: 0.2204 - val_acc: 0.6495\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2180 - acc: 0.6559 - val_loss: 0.2175 - val_acc: 0.6548\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2178 - acc: 0.6571 - val_loss: 0.2185 - val_acc: 0.6542\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.2154 - acc: 0.6604 - val_loss: 0.2165 - val_acc: 0.6576\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2144 - acc: 0.6628 - val_loss: 0.2153 - val_acc: 0.6631\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.2103 - acc: 0.6706 - val_loss: 0.2144 - val_acc: 0.6572\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 648us/step - loss: 0.2102 - acc: 0.6701 - val_loss: 0.2106 - val_acc: 0.6696\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 62s 653us/step - loss: 0.2057 - acc: 0.6790 - val_loss: 0.2055 - val_acc: 0.6759\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1999 - acc: 0.6894 - val_loss: 0.2007 - val_acc: 0.6826\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1996 - acc: 0.6897 - val_loss: 0.2048 - val_acc: 0.6776\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 653us/step - loss: 0.1940 - acc: 0.6993 - val_loss: 0.1964 - val_acc: 0.6915\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1996 - acc: 0.6910 - val_loss: 0.1991 - val_acc: 0.6856\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1902 - acc: 0.7057 - val_loss: 0.1862 - val_acc: 0.7072\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 650us/step - loss: 0.1837 - acc: 0.7178 - val_loss: 0.1824 - val_acc: 0.7186\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 652us/step - loss: 0.1817 - acc: 0.7215 - val_loss: 0.1849 - val_acc: 0.7130\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 651us/step - loss: 0.1808 - acc: 0.7237 - val_loss: 0.1796 - val_acc: 0.7194\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 654us/step - loss: 0.1742 - acc: 0.7339 - val_loss: 0.1712 - val_acc: 0.7301\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1865 - acc: 0.7173 - val_loss: 0.1824 - val_acc: 0.7215\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.1706 - acc: 0.7396 - val_loss: 0.1716 - val_acc: 0.7300\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.1639 - acc: 0.7505 - val_loss: 0.1590 - val_acc: 0.7572\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 653us/step - loss: 0.1593 - acc: 0.7576 - val_loss: 0.1582 - val_acc: 0.7563\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 654us/step - loss: 0.1531 - acc: 0.7687 - val_loss: 0.1546 - val_acc: 0.7635\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.1528 - acc: 0.7715 - val_loss: 0.1554 - val_acc: 0.7609\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.1473 - acc: 0.7779 - val_loss: 0.1565 - val_acc: 0.7600\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1645 - acc: 0.7528 - val_loss: 0.1625 - val_acc: 0.7539\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1482 - acc: 0.7791 - val_loss: 0.1538 - val_acc: 0.7690\n",
      "0.8121632024175484\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 76s 803us/step - loss: 0.2277 - acc: 0.6375 - val_loss: 0.2256 - val_acc: 0.6385\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 63s 661us/step - loss: 0.2243 - acc: 0.6444 - val_loss: 0.2244 - val_acc: 0.6406\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2230 - acc: 0.6464 - val_loss: 0.2228 - val_acc: 0.6446\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2213 - acc: 0.6497 - val_loss: 0.2215 - val_acc: 0.6476\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2195 - acc: 0.6548 - val_loss: 0.2225 - val_acc: 0.6426\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2189 - acc: 0.6563 - val_loss: 0.2363 - val_acc: 0.6279\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2222 - acc: 0.6496 - val_loss: 0.2218 - val_acc: 0.6460\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2184 - acc: 0.6568 - val_loss: 0.2224 - val_acc: 0.6456\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2192 - acc: 0.6551 - val_loss: 0.2188 - val_acc: 0.6554\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2156 - acc: 0.6625 - val_loss: 0.2189 - val_acc: 0.6553\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2153 - acc: 0.6619 - val_loss: 0.2176 - val_acc: 0.6561\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.2126 - acc: 0.6664 - val_loss: 0.2150 - val_acc: 0.6598\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 63s 661us/step - loss: 0.2099 - acc: 0.6724 - val_loss: 0.2101 - val_acc: 0.6697\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2065 - acc: 0.6782 - val_loss: 0.2070 - val_acc: 0.6735\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2049 - acc: 0.6814 - val_loss: 0.2108 - val_acc: 0.6638\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2016 - acc: 0.6864 - val_loss: 0.1986 - val_acc: 0.6884\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1959 - acc: 0.6957 - val_loss: 0.1915 - val_acc: 0.6995\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2155 - acc: 0.6518 - val_loss: 0.2227 - val_acc: 0.6361\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2197 - acc: 0.6421 - val_loss: 0.2163 - val_acc: 0.6425\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2161 - acc: 0.6512 - val_loss: 0.2344 - val_acc: 0.6226\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2332 - acc: 0.6264 - val_loss: 0.2333 - val_acc: 0.6233\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 63s 661us/step - loss: 0.2325 - acc: 0.6267 - val_loss: 0.2326 - val_acc: 0.6233\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 63s 661us/step - loss: 0.2316 - acc: 0.6267 - val_loss: 0.2315 - val_acc: 0.6233\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2299 - acc: 0.6262 - val_loss: 0.2302 - val_acc: 0.6233\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.2320 - acc: 0.6270 - val_loss: 0.2330 - val_acc: 0.6247\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.2329 - acc: 0.6276 - val_loss: 0.2337 - val_acc: 0.6242\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2331 - acc: 0.6272 - val_loss: 0.2316 - val_acc: 0.6241\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 63s 660us/step - loss: 0.2311 - acc: 0.6284 - val_loss: 0.2304 - val_acc: 0.6257\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2305 - acc: 0.6273 - val_loss: 0.2313 - val_acc: 0.6233\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2305 - acc: 0.6267 - val_loss: 0.2310 - val_acc: 0.6233\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 76s 805us/step - loss: 0.2277 - acc: 0.6376 - val_loss: 0.2258 - val_acc: 0.6390\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.2245 - acc: 0.6426 - val_loss: 0.2250 - val_acc: 0.6411\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2229 - acc: 0.6464 - val_loss: 0.2230 - val_acc: 0.6453\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2215 - acc: 0.6493 - val_loss: 0.2223 - val_acc: 0.6467\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2197 - acc: 0.6532 - val_loss: 0.2206 - val_acc: 0.6513\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2179 - acc: 0.6559 - val_loss: 0.2201 - val_acc: 0.6528\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2157 - acc: 0.6596 - val_loss: 0.2204 - val_acc: 0.6433\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2161 - acc: 0.6577 - val_loss: 0.2141 - val_acc: 0.6600\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2120 - acc: 0.6665 - val_loss: 0.2111 - val_acc: 0.6648\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2090 - acc: 0.6716 - val_loss: 0.2109 - val_acc: 0.6674\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2070 - acc: 0.6744 - val_loss: 0.2057 - val_acc: 0.6729\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2057 - acc: 0.6771 - val_loss: 0.2154 - val_acc: 0.6583\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2078 - acc: 0.6747 - val_loss: 0.2031 - val_acc: 0.6792\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.1977 - acc: 0.6920 - val_loss: 0.1998 - val_acc: 0.6851\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.1914 - acc: 0.7031 - val_loss: 0.1951 - val_acc: 0.6938\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.1871 - acc: 0.7109 - val_loss: 0.1922 - val_acc: 0.6971\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.1968 - acc: 0.7001 - val_loss: 0.1919 - val_acc: 0.7025\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1834 - acc: 0.7215 - val_loss: 0.1846 - val_acc: 0.7203\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1781 - acc: 0.7297 - val_loss: 0.1814 - val_acc: 0.7207\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1719 - acc: 0.7405 - val_loss: 0.1732 - val_acc: 0.7365\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1673 - acc: 0.7488 - val_loss: 0.1659 - val_acc: 0.7437\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1652 - acc: 0.7521 - val_loss: 0.1584 - val_acc: 0.7628\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1601 - acc: 0.7619 - val_loss: 0.1772 - val_acc: 0.7351\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.1651 - acc: 0.7546 - val_loss: 0.1599 - val_acc: 0.7601\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1524 - acc: 0.7731 - val_loss: 0.1502 - val_acc: 0.7751\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1471 - acc: 0.7828 - val_loss: 0.1490 - val_acc: 0.7792\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1419 - acc: 0.7914 - val_loss: 0.1383 - val_acc: 0.7931\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1601 - acc: 0.7635 - val_loss: 0.1544 - val_acc: 0.7725\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1418 - acc: 0.7912 - val_loss: 0.1412 - val_acc: 0.7893\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1371 - acc: 0.7985 - val_loss: 0.1406 - val_acc: 0.7909\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 77s 811us/step - loss: 0.2270 - acc: 0.6391 - val_loss: 0.2256 - val_acc: 0.6390\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2246 - acc: 0.6437 - val_loss: 0.2242 - val_acc: 0.6416\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.2234 - acc: 0.6451 - val_loss: 0.2247 - val_acc: 0.6388\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.2223 - acc: 0.6469 - val_loss: 0.2225 - val_acc: 0.6470\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.2201 - acc: 0.6528 - val_loss: 0.2223 - val_acc: 0.6454\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.2182 - acc: 0.6558 - val_loss: 0.2178 - val_acc: 0.6550\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 62s 655us/step - loss: 0.2167 - acc: 0.6582 - val_loss: 0.2188 - val_acc: 0.6539\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.2144 - acc: 0.6626 - val_loss: 0.2156 - val_acc: 0.6610\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2124 - acc: 0.6672 - val_loss: 0.2111 - val_acc: 0.6677\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2091 - acc: 0.6717 - val_loss: 0.2107 - val_acc: 0.6615\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.2081 - acc: 0.6731 - val_loss: 0.2092 - val_acc: 0.6723\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.2024 - acc: 0.6837 - val_loss: 0.2050 - val_acc: 0.6777\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1977 - acc: 0.6931 - val_loss: 0.1982 - val_acc: 0.6901\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 63s 659us/step - loss: 0.1941 - acc: 0.6995 - val_loss: 0.1972 - val_acc: 0.6904\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 63s 658us/step - loss: 0.1896 - acc: 0.7068 - val_loss: 0.1898 - val_acc: 0.7061\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1837 - acc: 0.7162 - val_loss: 0.1885 - val_acc: 0.7055\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1835 - acc: 0.7173 - val_loss: 0.1795 - val_acc: 0.7200\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 62s 657us/step - loss: 0.1755 - acc: 0.7325 - val_loss: 0.1830 - val_acc: 0.7165\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1712 - acc: 0.7404 - val_loss: 0.1704 - val_acc: 0.7416\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 62s 658us/step - loss: 0.1634 - acc: 0.7520 - val_loss: 0.1678 - val_acc: 0.7458\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 62s 656us/step - loss: 0.1585 - acc: 0.7609 - val_loss: 0.1661 - val_acc: 0.7513\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 63s 664us/step - loss: 0.1589 - acc: 0.7593 - val_loss: 0.1598 - val_acc: 0.7561\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 67s 704us/step - loss: 0.1565 - acc: 0.7643 - val_loss: 0.1549 - val_acc: 0.7655\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.1537 - acc: 0.7702 - val_loss: 0.1823 - val_acc: 0.7246\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 66s 694us/step - loss: 0.1570 - acc: 0.7633 - val_loss: 0.1468 - val_acc: 0.7777\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.1447 - acc: 0.7839 - val_loss: 0.1509 - val_acc: 0.7765\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 67s 705us/step - loss: 0.1420 - acc: 0.7887 - val_loss: 0.1496 - val_acc: 0.7805\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 66s 691us/step - loss: 0.1327 - acc: 0.8031 - val_loss: 0.1347 - val_acc: 0.7950\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 67s 704us/step - loss: 0.1392 - acc: 0.7939 - val_loss: 0.1455 - val_acc: 0.7816\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 66s 693us/step - loss: 0.1318 - acc: 0.8061 - val_loss: 0.1285 - val_acc: 0.8125\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 87s 915us/step - loss: 0.2274 - acc: 0.6379 - val_loss: 0.2259 - val_acc: 0.6371\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 65s 688us/step - loss: 0.2247 - acc: 0.6431 - val_loss: 0.2255 - val_acc: 0.6416\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 66s 697us/step - loss: 0.2229 - acc: 0.6465 - val_loss: 0.2236 - val_acc: 0.6423\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2213 - acc: 0.6509 - val_loss: 0.2218 - val_acc: 0.6474\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 65s 689us/step - loss: 0.2213 - acc: 0.6503 - val_loss: 0.2218 - val_acc: 0.6496\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 67s 700us/step - loss: 0.2195 - acc: 0.6538 - val_loss: 0.2204 - val_acc: 0.6507\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 66s 693us/step - loss: 0.2191 - acc: 0.6536 - val_loss: 0.2193 - val_acc: 0.6540\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 66s 696us/step - loss: 0.2173 - acc: 0.6587 - val_loss: 0.2179 - val_acc: 0.6535\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.2151 - acc: 0.6623 - val_loss: 0.2173 - val_acc: 0.6562\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 65s 686us/step - loss: 0.2134 - acc: 0.6657 - val_loss: 0.2141 - val_acc: 0.6663\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 66s 698us/step - loss: 0.2121 - acc: 0.6666 - val_loss: 0.2111 - val_acc: 0.6669\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 66s 697us/step - loss: 0.2086 - acc: 0.6729 - val_loss: 0.2094 - val_acc: 0.6638\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 65s 683us/step - loss: 0.2055 - acc: 0.6791 - val_loss: 0.2067 - val_acc: 0.6742\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 66s 696us/step - loss: 0.2047 - acc: 0.6800 - val_loss: 0.2077 - val_acc: 0.6651\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 66s 699us/step - loss: 0.1987 - acc: 0.6906 - val_loss: 0.2132 - val_acc: 0.6670\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 65s 682us/step - loss: 0.2047 - acc: 0.6846 - val_loss: 0.2080 - val_acc: 0.6764\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 66s 698us/step - loss: 0.1979 - acc: 0.6949 - val_loss: 0.1972 - val_acc: 0.6890\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 85s 891us/step - loss: 0.1945 - acc: 0.7010 - val_loss: 0.1945 - val_acc: 0.6993\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 67s 706us/step - loss: 0.1942 - acc: 0.7022 - val_loss: 0.1902 - val_acc: 0.7083\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.1855 - acc: 0.7166 - val_loss: 0.1837 - val_acc: 0.7159\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 66s 692us/step - loss: 0.1790 - acc: 0.7279 - val_loss: 0.1832 - val_acc: 0.7205\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.1748 - acc: 0.7326 - val_loss: 0.1683 - val_acc: 0.7448\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 67s 708us/step - loss: 0.1698 - acc: 0.7442 - val_loss: 0.1920 - val_acc: 0.7087\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 66s 692us/step - loss: 0.1816 - acc: 0.7260 - val_loss: 0.1693 - val_acc: 0.7475\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 67s 708us/step - loss: 0.1628 - acc: 0.7561 - val_loss: 0.1700 - val_acc: 0.7457\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 69s 726us/step - loss: 0.1595 - acc: 0.7613 - val_loss: 0.1655 - val_acc: 0.7512\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 69s 727us/step - loss: 0.1517 - acc: 0.7738 - val_loss: 0.1443 - val_acc: 0.7864\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 69s 727us/step - loss: 0.1481 - acc: 0.7814 - val_loss: 0.1551 - val_acc: 0.7699\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 68s 712us/step - loss: 0.1426 - acc: 0.7896 - val_loss: 0.1424 - val_acc: 0.7920\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 65s 688us/step - loss: 0.1401 - acc: 0.7948 - val_loss: 0.1465 - val_acc: 0.7908\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 88s 931us/step - loss: 0.2274 - acc: 0.6385 - val_loss: 0.2255 - val_acc: 0.6374\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 66s 699us/step - loss: 0.2242 - acc: 0.6447 - val_loss: 0.2238 - val_acc: 0.6452\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 67s 705us/step - loss: 0.2225 - acc: 0.6478 - val_loss: 0.2223 - val_acc: 0.6468\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 65s 688us/step - loss: 0.2206 - acc: 0.6521 - val_loss: 0.2211 - val_acc: 0.6477\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2195 - acc: 0.6545 - val_loss: 0.2187 - val_acc: 0.6527\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 67s 703us/step - loss: 0.2185 - acc: 0.6560 - val_loss: 0.2186 - val_acc: 0.6566\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 65s 682us/step - loss: 0.2169 - acc: 0.6598 - val_loss: 0.2185 - val_acc: 0.6548\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.2177 - acc: 0.6588 - val_loss: 0.2175 - val_acc: 0.6467\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 67s 704us/step - loss: 0.2194 - acc: 0.6528 - val_loss: 0.2239 - val_acc: 0.6409\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 65s 685us/step - loss: 0.2208 - acc: 0.6503 - val_loss: 0.2175 - val_acc: 0.6580\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.2135 - acc: 0.6647 - val_loss: 0.2129 - val_acc: 0.6629\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 67s 706us/step - loss: 0.2224 - acc: 0.6496 - val_loss: 0.2234 - val_acc: 0.6370\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 65s 681us/step - loss: 0.2255 - acc: 0.6386 - val_loss: 0.2251 - val_acc: 0.6394\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.2230 - acc: 0.6467 - val_loss: 0.2235 - val_acc: 0.6458\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 71s 745us/step - loss: 0.2245 - acc: 0.6486 - val_loss: 0.2254 - val_acc: 0.6452\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 68s 720us/step - loss: 0.2259 - acc: 0.6437 - val_loss: 0.2262 - val_acc: 0.6444\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 92s 973us/step - loss: 0.2232 - acc: 0.6504 - val_loss: 0.2231 - val_acc: 0.6471\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2219 - acc: 0.6525 - val_loss: 0.2227 - val_acc: 0.6495\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 95s 996us/step - loss: 0.2216 - acc: 0.6533 - val_loss: 0.2249 - val_acc: 0.6488\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2309 - acc: 0.6330 - val_loss: 0.2340 - val_acc: 0.6233\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2309 - acc: 0.6249 - val_loss: 0.2318 - val_acc: 0.6233\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 94s 989us/step - loss: 0.2283 - acc: 0.6380 - val_loss: 0.2274 - val_acc: 0.6400\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2258 - acc: 0.6465 - val_loss: 0.2250 - val_acc: 0.6433\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2243 - acc: 0.6479 - val_loss: 0.2247 - val_acc: 0.6452\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 94s 992us/step - loss: 0.2229 - acc: 0.6501 - val_loss: 0.2243 - val_acc: 0.6468\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2231 - acc: 0.6501 - val_loss: 0.2241 - val_acc: 0.6458\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2230 - acc: 0.6492 - val_loss: 0.2263 - val_acc: 0.6412\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 94s 985us/step - loss: 0.2225 - acc: 0.6503 - val_loss: 0.2242 - val_acc: 0.6459\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2218 - acc: 0.6518 - val_loss: 0.2242 - val_acc: 0.6429\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.2214 - acc: 0.6531 - val_loss: 0.2227 - val_acc: 0.6477\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 123s 1ms/step - loss: 0.2281 - acc: 0.6364 - val_loss: 0.2263 - val_acc: 0.6398\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 72s 761us/step - loss: 0.2246 - acc: 0.6436 - val_loss: 0.2256 - val_acc: 0.6380\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 64s 676us/step - loss: 0.2232 - acc: 0.6465 - val_loss: 0.2231 - val_acc: 0.6440\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 66s 695us/step - loss: 0.2217 - acc: 0.6504 - val_loss: 0.2227 - val_acc: 0.6467\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 67s 705us/step - loss: 0.2196 - acc: 0.6550 - val_loss: 0.2212 - val_acc: 0.6506\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.2181 - acc: 0.6575 - val_loss: 0.2198 - val_acc: 0.6523\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 66s 694us/step - loss: 0.2186 - acc: 0.6575 - val_loss: 0.2210 - val_acc: 0.6502\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2169 - acc: 0.6609 - val_loss: 0.2174 - val_acc: 0.6578\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 65s 684us/step - loss: 0.2139 - acc: 0.6641 - val_loss: 0.2144 - val_acc: 0.6611\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 65s 684us/step - loss: 0.2145 - acc: 0.6636 - val_loss: 0.2166 - val_acc: 0.6587\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 67s 704us/step - loss: 0.2126 - acc: 0.6675 - val_loss: 0.2136 - val_acc: 0.6626\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 65s 687us/step - loss: 0.2114 - acc: 0.6694 - val_loss: 0.2133 - val_acc: 0.6609\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 65s 680us/step - loss: 0.2076 - acc: 0.6769 - val_loss: 0.2109 - val_acc: 0.6652\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.2027 - acc: 0.6845 - val_loss: 0.2070 - val_acc: 0.6775\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 66s 693us/step - loss: 0.2006 - acc: 0.6897 - val_loss: 0.1986 - val_acc: 0.6910\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 64s 668us/step - loss: 0.1955 - acc: 0.6967 - val_loss: 0.1970 - val_acc: 0.6917\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.1915 - acc: 0.7054 - val_loss: 0.1919 - val_acc: 0.7050\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.1853 - acc: 0.7145 - val_loss: 0.1887 - val_acc: 0.7030\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 63s 664us/step - loss: 0.1835 - acc: 0.7202 - val_loss: 0.1839 - val_acc: 0.7156\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 66s 697us/step - loss: 0.1780 - acc: 0.7267 - val_loss: 0.1748 - val_acc: 0.7330\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.1704 - acc: 0.7389 - val_loss: 0.1745 - val_acc: 0.7314\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 63s 663us/step - loss: 0.1727 - acc: 0.7357 - val_loss: 0.1675 - val_acc: 0.7400\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 66s 698us/step - loss: 0.1640 - acc: 0.7524 - val_loss: 0.1643 - val_acc: 0.7470\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.1667 - acc: 0.7484 - val_loss: 0.1623 - val_acc: 0.7494\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 63s 665us/step - loss: 0.1535 - acc: 0.7674 - val_loss: 0.1532 - val_acc: 0.7697\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 66s 699us/step - loss: 0.1476 - acc: 0.7782 - val_loss: 0.1596 - val_acc: 0.7596\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 67s 707us/step - loss: 0.1496 - acc: 0.7749 - val_loss: 0.1492 - val_acc: 0.7768\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 64s 677us/step - loss: 0.1564 - acc: 0.7669 - val_loss: 0.1555 - val_acc: 0.7650\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 66s 691us/step - loss: 0.1447 - acc: 0.7847 - val_loss: 0.1431 - val_acc: 0.7850\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 67s 703us/step - loss: 0.1374 - acc: 0.7959 - val_loss: 0.1371 - val_acc: 0.7938\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 86s 906us/step - loss: 0.2279 - acc: 0.6375 - val_loss: 0.2260 - val_acc: 0.6387\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.2247 - acc: 0.6434 - val_loss: 0.2257 - val_acc: 0.6384\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.2233 - acc: 0.6461 - val_loss: 0.2240 - val_acc: 0.6417\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 63s 663us/step - loss: 0.2214 - acc: 0.6503 - val_loss: 0.2228 - val_acc: 0.6450\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 67s 703us/step - loss: 0.2197 - acc: 0.6545 - val_loss: 0.2212 - val_acc: 0.6484\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 67s 709us/step - loss: 0.2178 - acc: 0.6580 - val_loss: 0.2191 - val_acc: 0.6500\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 66s 695us/step - loss: 0.2174 - acc: 0.6585 - val_loss: 0.2192 - val_acc: 0.6509\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 71s 743us/step - loss: 0.2166 - acc: 0.6576 - val_loss: 0.2188 - val_acc: 0.6485\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 90s 945us/step - loss: 0.2148 - acc: 0.6617 - val_loss: 0.2133 - val_acc: 0.6592\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 95s 1ms/step - loss: 0.2112 - acc: 0.6698 - val_loss: 0.2114 - val_acc: 0.6612\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2082 - acc: 0.6744 - val_loss: 0.2071 - val_acc: 0.6691\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2033 - acc: 0.6837 - val_loss: 0.2049 - val_acc: 0.6817\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2041 - acc: 0.6828 - val_loss: 0.2133 - val_acc: 0.6652\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 94s 989us/step - loss: 0.1994 - acc: 0.6900 - val_loss: 0.1980 - val_acc: 0.6964\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1937 - acc: 0.7013 - val_loss: 0.1884 - val_acc: 0.7086\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1847 - acc: 0.7149 - val_loss: 0.1814 - val_acc: 0.7204\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 93s 984us/step - loss: 0.1789 - acc: 0.7252 - val_loss: 0.1800 - val_acc: 0.7210\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1744 - acc: 0.7335 - val_loss: 0.1755 - val_acc: 0.7328\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1694 - acc: 0.7410 - val_loss: 0.1687 - val_acc: 0.7389\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 94s 987us/step - loss: 0.1643 - acc: 0.7503 - val_loss: 0.1624 - val_acc: 0.7503\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1605 - acc: 0.7575 - val_loss: 0.1620 - val_acc: 0.7515\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1564 - acc: 0.7636 - val_loss: 0.1552 - val_acc: 0.7615\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1549 - acc: 0.7678 - val_loss: 0.1528 - val_acc: 0.7708\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 94s 987us/step - loss: 0.1465 - acc: 0.7803 - val_loss: 0.1482 - val_acc: 0.7782\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.1435 - acc: 0.7842 - val_loss: 0.1458 - val_acc: 0.7819\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 93s 983us/step - loss: 0.1359 - acc: 0.7972 - val_loss: 0.1329 - val_acc: 0.7988\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 63s 661us/step - loss: 0.1336 - acc: 0.8002 - val_loss: 0.1367 - val_acc: 0.7963\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 67s 701us/step - loss: 0.1339 - acc: 0.8011 - val_loss: 0.1323 - val_acc: 0.7976\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 67s 708us/step - loss: 0.1258 - acc: 0.8133 - val_loss: 0.1231 - val_acc: 0.8126\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 69s 722us/step - loss: 0.1227 - acc: 0.8183 - val_loss: 0.1294 - val_acc: 0.8067\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 86s 906us/step - loss: 0.2276 - acc: 0.6367 - val_loss: 0.2274 - val_acc: 0.6377\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 71s 745us/step - loss: 0.2250 - acc: 0.6427 - val_loss: 0.2241 - val_acc: 0.6417\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 75s 786us/step - loss: 0.2234 - acc: 0.6456 - val_loss: 0.2244 - val_acc: 0.6419\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 77s 813us/step - loss: 0.2220 - acc: 0.6475 - val_loss: 0.2224 - val_acc: 0.6462\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 65s 684us/step - loss: 0.2205 - acc: 0.6523 - val_loss: 0.2211 - val_acc: 0.6493\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 68s 713us/step - loss: 0.2185 - acc: 0.6562 - val_loss: 0.2188 - val_acc: 0.6513\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 67s 700us/step - loss: 0.2193 - acc: 0.6531 - val_loss: 0.2227 - val_acc: 0.6502\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 63s 665us/step - loss: 0.2183 - acc: 0.6561 - val_loss: 0.2185 - val_acc: 0.6563\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 67s 703us/step - loss: 0.2164 - acc: 0.6603 - val_loss: 0.2173 - val_acc: 0.6563\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 77s 816us/step - loss: 0.2150 - acc: 0.6622 - val_loss: 0.2152 - val_acc: 0.6631\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 84s 885us/step - loss: 0.2145 - acc: 0.6638 - val_loss: 0.2155 - val_acc: 0.6571\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 93s 976us/step - loss: 0.2114 - acc: 0.6681 - val_loss: 0.2125 - val_acc: 0.6662\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2130 - acc: 0.6671 - val_loss: 0.2114 - val_acc: 0.6694\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2075 - acc: 0.6786 - val_loss: 0.2063 - val_acc: 0.6788\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 95s 997us/step - loss: 0.2030 - acc: 0.6862 - val_loss: 0.2027 - val_acc: 0.6820\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1977 - acc: 0.6965 - val_loss: 0.1937 - val_acc: 0.7003\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1931 - acc: 0.7064 - val_loss: 0.1915 - val_acc: 0.7034\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1851 - acc: 0.7193 - val_loss: 0.1862 - val_acc: 0.7147\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1810 - acc: 0.7257 - val_loss: 0.1793 - val_acc: 0.7251\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1792 - acc: 0.7295 - val_loss: 0.1762 - val_acc: 0.7362\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1740 - acc: 0.7372 - val_loss: 0.1683 - val_acc: 0.7435\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 95s 995us/step - loss: 0.1663 - acc: 0.7501 - val_loss: 0.1675 - val_acc: 0.7491\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1633 - acc: 0.7554 - val_loss: 0.1707 - val_acc: 0.7430\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1643 - acc: 0.7536 - val_loss: 0.1640 - val_acc: 0.7486\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1540 - acc: 0.7690 - val_loss: 0.1604 - val_acc: 0.7584\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1545 - acc: 0.7687 - val_loss: 0.1778 - val_acc: 0.7370\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 69s 722us/step - loss: 0.1527 - acc: 0.7719 - val_loss: 0.1547 - val_acc: 0.7659\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 89s 932us/step - loss: 0.1485 - acc: 0.7782 - val_loss: 0.1431 - val_acc: 0.7849\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 95s 1ms/step - loss: 0.1448 - acc: 0.7839 - val_loss: 0.1468 - val_acc: 0.7766\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1412 - acc: 0.7899 - val_loss: 0.1436 - val_acc: 0.7855\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 135s 1ms/step - loss: 0.2273 - acc: 0.6380 - val_loss: 0.2256 - val_acc: 0.6381\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 94s 987us/step - loss: 0.2246 - acc: 0.6427 - val_loss: 0.2245 - val_acc: 0.6414\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2226 - acc: 0.6473 - val_loss: 0.2226 - val_acc: 0.6443\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2214 - acc: 0.6489 - val_loss: 0.2215 - val_acc: 0.6452\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2184 - acc: 0.6571 - val_loss: 0.2198 - val_acc: 0.6532\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2173 - acc: 0.6589 - val_loss: 0.2183 - val_acc: 0.6549\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2149 - acc: 0.6640 - val_loss: 0.2188 - val_acc: 0.6578\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2135 - acc: 0.6660 - val_loss: 0.2160 - val_acc: 0.6610\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 94s 993us/step - loss: 0.2100 - acc: 0.6717 - val_loss: 0.2099 - val_acc: 0.6659\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2082 - acc: 0.6752 - val_loss: 0.2074 - val_acc: 0.6701\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2085 - acc: 0.6742 - val_loss: 0.2107 - val_acc: 0.6653\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 94s 992us/step - loss: 0.2055 - acc: 0.6802 - val_loss: 0.2044 - val_acc: 0.6820\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 93s 983us/step - loss: 0.2018 - acc: 0.6866 - val_loss: 0.1969 - val_acc: 0.6951\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1977 - acc: 0.6967 - val_loss: 0.1954 - val_acc: 0.7012\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 106s 1ms/step - loss: 0.1903 - acc: 0.7084 - val_loss: 0.1927 - val_acc: 0.7021\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1843 - acc: 0.7191 - val_loss: 0.1841 - val_acc: 0.7131\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1834 - acc: 0.7209 - val_loss: 0.1852 - val_acc: 0.7149\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1781 - acc: 0.7285 - val_loss: 0.1738 - val_acc: 0.7339\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1661 - acc: 0.7499 - val_loss: 0.1757 - val_acc: 0.7328\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 94s 992us/step - loss: 0.1610 - acc: 0.7572 - val_loss: 0.1668 - val_acc: 0.7434\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1557 - acc: 0.7649 - val_loss: 0.1553 - val_acc: 0.7571\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1649 - acc: 0.7525 - val_loss: 0.1565 - val_acc: 0.7613\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1496 - acc: 0.7748 - val_loss: 0.1535 - val_acc: 0.7643\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 93s 980us/step - loss: 0.1440 - acc: 0.7837 - val_loss: 0.1409 - val_acc: 0.7833\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1365 - acc: 0.7951 - val_loss: 0.1438 - val_acc: 0.7782\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1360 - acc: 0.7980 - val_loss: 0.1447 - val_acc: 0.7816\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 93s 984us/step - loss: 0.1297 - acc: 0.8074 - val_loss: 0.1334 - val_acc: 0.7997\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 94s 988us/step - loss: 0.1224 - acc: 0.8187 - val_loss: 0.1301 - val_acc: 0.8068\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1194 - acc: 0.8238 - val_loss: 0.1307 - val_acc: 0.8120\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1190 - acc: 0.8255 - val_loss: 0.1174 - val_acc: 0.8231\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 124s 1ms/step - loss: 0.2274 - acc: 0.6384 - val_loss: 0.2260 - val_acc: 0.6385\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2245 - acc: 0.6437 - val_loss: 0.2244 - val_acc: 0.6393\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2227 - acc: 0.6472 - val_loss: 0.2230 - val_acc: 0.6480\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2209 - acc: 0.6514 - val_loss: 0.2221 - val_acc: 0.6473\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2197 - acc: 0.6539 - val_loss: 0.2197 - val_acc: 0.6521\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2173 - acc: 0.6590 - val_loss: 0.2205 - val_acc: 0.6524\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2162 - acc: 0.6608 - val_loss: 0.2162 - val_acc: 0.6594\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 95s 1ms/step - loss: 0.2180 - acc: 0.6568 - val_loss: 0.2198 - val_acc: 0.6512\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2162 - acc: 0.6604 - val_loss: 0.2177 - val_acc: 0.6579\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2146 - acc: 0.6622 - val_loss: 0.2165 - val_acc: 0.6580\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2124 - acc: 0.6662 - val_loss: 0.2135 - val_acc: 0.6630\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 90s 943us/step - loss: 0.2094 - acc: 0.6735 - val_loss: 0.2127 - val_acc: 0.6672\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2078 - acc: 0.6767 - val_loss: 0.2115 - val_acc: 0.6693\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2034 - acc: 0.6849 - val_loss: 0.2024 - val_acc: 0.6852\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1975 - acc: 0.6962 - val_loss: 0.2009 - val_acc: 0.6907\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 92s 966us/step - loss: 0.1919 - acc: 0.7060 - val_loss: 0.1926 - val_acc: 0.6991\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1884 - acc: 0.7115 - val_loss: 0.1875 - val_acc: 0.7119\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1829 - acc: 0.7198 - val_loss: 0.1811 - val_acc: 0.7184\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1804 - acc: 0.7257 - val_loss: 0.1821 - val_acc: 0.7204\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 92s 973us/step - loss: 0.1715 - acc: 0.7391 - val_loss: 0.1696 - val_acc: 0.7394\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1661 - acc: 0.7461 - val_loss: 0.1671 - val_acc: 0.7433\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1632 - acc: 0.7529 - val_loss: 0.1625 - val_acc: 0.7530\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1611 - acc: 0.7564 - val_loss: 0.1702 - val_acc: 0.7479\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 94s 988us/step - loss: 0.1561 - acc: 0.7637 - val_loss: 0.1557 - val_acc: 0.7623\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1495 - acc: 0.7750 - val_loss: 0.1538 - val_acc: 0.7710\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1496 - acc: 0.7776 - val_loss: 0.1510 - val_acc: 0.7716\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 91s 961us/step - loss: 0.1460 - acc: 0.7831 - val_loss: 0.1421 - val_acc: 0.7847\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 94s 994us/step - loss: 0.1362 - acc: 0.7982 - val_loss: 0.1410 - val_acc: 0.7874\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1339 - acc: 0.8002 - val_loss: 0.1323 - val_acc: 0.8007\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1333 - acc: 0.8028 - val_loss: 0.1312 - val_acc: 0.8031\n",
      "0.8231331794146302\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 123s 1ms/step - loss: 0.2283 - acc: 0.6351 - val_loss: 0.2254 - val_acc: 0.6390\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2247 - acc: 0.6429 - val_loss: 0.2244 - val_acc: 0.6445\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2232 - acc: 0.6465 - val_loss: 0.2237 - val_acc: 0.6446\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2223 - acc: 0.6487 - val_loss: 0.2230 - val_acc: 0.6433\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 93s 975us/step - loss: 0.2212 - acc: 0.6505 - val_loss: 0.2212 - val_acc: 0.6467\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2205 - acc: 0.6518 - val_loss: 0.2208 - val_acc: 0.6518\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2194 - acc: 0.6538 - val_loss: 0.2237 - val_acc: 0.6447\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2181 - acc: 0.6563 - val_loss: 0.2192 - val_acc: 0.6544\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 93s 977us/step - loss: 0.2172 - acc: 0.6581 - val_loss: 0.2213 - val_acc: 0.6469\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2186 - acc: 0.6535 - val_loss: 0.2177 - val_acc: 0.6538\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 93s 983us/step - loss: 0.2171 - acc: 0.6580 - val_loss: 0.2187 - val_acc: 0.6518\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2143 - acc: 0.6630 - val_loss: 0.2145 - val_acc: 0.6606\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 93s 974us/step - loss: 0.2127 - acc: 0.6648 - val_loss: 0.2139 - val_acc: 0.6609\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2118 - acc: 0.6661 - val_loss: 0.2117 - val_acc: 0.6617\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2090 - acc: 0.6726 - val_loss: 0.2093 - val_acc: 0.6695\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2067 - acc: 0.6741 - val_loss: 0.2097 - val_acc: 0.6658\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 91s 961us/step - loss: 0.2066 - acc: 0.6753 - val_loss: 0.2047 - val_acc: 0.6785\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2028 - acc: 0.6834 - val_loss: 0.2033 - val_acc: 0.6822\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1977 - acc: 0.6922 - val_loss: 0.1947 - val_acc: 0.6925\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1908 - acc: 0.7042 - val_loss: 0.1896 - val_acc: 0.7048\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 94s 987us/step - loss: 0.1857 - acc: 0.7142 - val_loss: 0.1834 - val_acc: 0.7184\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1815 - acc: 0.7222 - val_loss: 0.1830 - val_acc: 0.7195\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1819 - acc: 0.7213 - val_loss: 0.2016 - val_acc: 0.6822\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1851 - acc: 0.7160 - val_loss: 0.1767 - val_acc: 0.7300\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1884 - acc: 0.7116 - val_loss: 0.1824 - val_acc: 0.7219\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 88s 923us/step - loss: 0.1738 - acc: 0.7350 - val_loss: 0.1791 - val_acc: 0.7273\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1679 - acc: 0.7439 - val_loss: 0.1670 - val_acc: 0.7452\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1591 - acc: 0.7591 - val_loss: 0.1551 - val_acc: 0.7666\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1531 - acc: 0.7708 - val_loss: 0.1490 - val_acc: 0.7744\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 93s 984us/step - loss: 0.1516 - acc: 0.7726 - val_loss: 0.1749 - val_acc: 0.7361\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 144s 2ms/step - loss: 0.2272 - acc: 0.6399 - val_loss: 0.2256 - val_acc: 0.6370\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 98s 1ms/step - loss: 0.2247 - acc: 0.6432 - val_loss: 0.2252 - val_acc: 0.6398\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 92s 972us/step - loss: 0.2224 - acc: 0.6474 - val_loss: 0.2223 - val_acc: 0.6471\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2214 - acc: 0.6493 - val_loss: 0.2223 - val_acc: 0.6447\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2198 - acc: 0.6531 - val_loss: 0.2195 - val_acc: 0.6513\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2191 - acc: 0.6537 - val_loss: 0.2200 - val_acc: 0.6465\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 92s 966us/step - loss: 0.2167 - acc: 0.6577 - val_loss: 0.2167 - val_acc: 0.6544\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.2145 - acc: 0.6618 - val_loss: 0.2146 - val_acc: 0.6621\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2107 - acc: 0.6690 - val_loss: 0.2123 - val_acc: 0.6630\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2078 - acc: 0.6738 - val_loss: 0.2129 - val_acc: 0.6690\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 85s 895us/step - loss: 0.2056 - acc: 0.6788 - val_loss: 0.2054 - val_acc: 0.6767\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1984 - acc: 0.6937 - val_loss: 0.1960 - val_acc: 0.6935\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1926 - acc: 0.7025 - val_loss: 0.1925 - val_acc: 0.7040\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1880 - acc: 0.7128 - val_loss: 0.1890 - val_acc: 0.7041\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1820 - acc: 0.7234 - val_loss: 0.1873 - val_acc: 0.7113\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 93s 982us/step - loss: 0.1763 - acc: 0.7323 - val_loss: 0.1751 - val_acc: 0.7385\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1745 - acc: 0.7373 - val_loss: 0.1783 - val_acc: 0.7258\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1648 - acc: 0.7518 - val_loss: 0.1677 - val_acc: 0.7470\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1633 - acc: 0.7548 - val_loss: 0.1624 - val_acc: 0.7553\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 91s 955us/step - loss: 0.1564 - acc: 0.7663 - val_loss: 0.1549 - val_acc: 0.7633\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1581 - acc: 0.7636 - val_loss: 0.1534 - val_acc: 0.7661\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1477 - acc: 0.7777 - val_loss: 0.1439 - val_acc: 0.7837\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 103s 1ms/step - loss: 0.1391 - acc: 0.7903 - val_loss: 0.1453 - val_acc: 0.7818\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 91s 954us/step - loss: 0.1372 - acc: 0.7921 - val_loss: 0.1391 - val_acc: 0.7840\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 97s 1ms/step - loss: 0.1351 - acc: 0.7963 - val_loss: 0.1314 - val_acc: 0.8006\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 93s 976us/step - loss: 0.1272 - acc: 0.8082 - val_loss: 0.1278 - val_acc: 0.8079\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1326 - acc: 0.8023 - val_loss: 0.1373 - val_acc: 0.7924\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 95s 999us/step - loss: 0.1375 - acc: 0.7947 - val_loss: 0.1387 - val_acc: 0.7915\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 94s 989us/step - loss: 0.1294 - acc: 0.8049 - val_loss: 0.1381 - val_acc: 0.7924\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.1230 - acc: 0.8133 - val_loss: 0.1255 - val_acc: 0.8080\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 140s 1ms/step - loss: 0.2275 - acc: 0.6376 - val_loss: 0.2257 - val_acc: 0.6394\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 90s 949us/step - loss: 0.2246 - acc: 0.6429 - val_loss: 0.2250 - val_acc: 0.6407\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2235 - acc: 0.6456 - val_loss: 0.2242 - val_acc: 0.6419\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2216 - acc: 0.6492 - val_loss: 0.2224 - val_acc: 0.6441\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2199 - acc: 0.6535 - val_loss: 0.2202 - val_acc: 0.6511\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 92s 965us/step - loss: 0.2182 - acc: 0.6558 - val_loss: 0.2196 - val_acc: 0.6454\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.2174 - acc: 0.6576 - val_loss: 0.2197 - val_acc: 0.6492\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2161 - acc: 0.6597 - val_loss: 0.2179 - val_acc: 0.6567\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2146 - acc: 0.6632 - val_loss: 0.2165 - val_acc: 0.6622\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 89s 932us/step - loss: 0.2122 - acc: 0.6680 - val_loss: 0.2132 - val_acc: 0.6665\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 88s 925us/step - loss: 0.2106 - acc: 0.6713 - val_loss: 0.2179 - val_acc: 0.6563\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.2094 - acc: 0.6742 - val_loss: 0.2099 - val_acc: 0.6721\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.2056 - acc: 0.6801 - val_loss: 0.2054 - val_acc: 0.6773\n",
      "Epoch 14/30\n",
      "95000/95000 [==============================] - 101s 1ms/step - loss: 0.2022 - acc: 0.6866 - val_loss: 0.2021 - val_acc: 0.6824\n",
      "Epoch 15/30\n",
      "95000/95000 [==============================] - 91s 955us/step - loss: 0.1992 - acc: 0.6937 - val_loss: 0.1984 - val_acc: 0.6929\n",
      "Epoch 16/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1939 - acc: 0.7020 - val_loss: 0.1963 - val_acc: 0.6916\n",
      "Epoch 17/30\n",
      "95000/95000 [==============================] - 100s 1ms/step - loss: 0.1877 - acc: 0.7133 - val_loss: 0.1900 - val_acc: 0.7051\n",
      "Epoch 18/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1837 - acc: 0.7214 - val_loss: 0.1797 - val_acc: 0.7222\n",
      "Epoch 19/30\n",
      "95000/95000 [==============================] - 96s 1ms/step - loss: 0.1761 - acc: 0.7340 - val_loss: 0.1758 - val_acc: 0.7329\n",
      "Epoch 20/30\n",
      "95000/95000 [==============================] - 91s 954us/step - loss: 0.1740 - acc: 0.7361 - val_loss: 0.1781 - val_acc: 0.7274\n",
      "Epoch 21/30\n",
      "95000/95000 [==============================] - 99s 1ms/step - loss: 0.1697 - acc: 0.7429 - val_loss: 0.1782 - val_acc: 0.7301\n",
      "Epoch 22/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1730 - acc: 0.7379 - val_loss: 0.1636 - val_acc: 0.7509\n",
      "Epoch 23/30\n",
      "95000/95000 [==============================] - 102s 1ms/step - loss: 0.1603 - acc: 0.7567 - val_loss: 0.1746 - val_acc: 0.7335\n",
      "Epoch 24/30\n",
      "95000/95000 [==============================] - 92s 970us/step - loss: 0.1587 - acc: 0.7605 - val_loss: 0.1560 - val_acc: 0.7598\n",
      "Epoch 25/30\n",
      "95000/95000 [==============================] - 94s 994us/step - loss: 0.1614 - acc: 0.7585 - val_loss: 0.2353 - val_acc: 0.6665\n",
      "Epoch 26/30\n",
      "95000/95000 [==============================] - 73s 769us/step - loss: 0.2102 - acc: 0.6788 - val_loss: 0.1941 - val_acc: 0.7009\n",
      "Epoch 27/30\n",
      "95000/95000 [==============================] - 66s 698us/step - loss: 0.1743 - acc: 0.7371 - val_loss: 0.1629 - val_acc: 0.7537\n",
      "Epoch 28/30\n",
      "95000/95000 [==============================] - 64s 670us/step - loss: 0.1605 - acc: 0.7596 - val_loss: 0.1627 - val_acc: 0.7548\n",
      "Epoch 29/30\n",
      "95000/95000 [==============================] - 60s 627us/step - loss: 0.1530 - acc: 0.7715 - val_loss: 0.1535 - val_acc: 0.7692\n",
      "Epoch 30/30\n",
      "95000/95000 [==============================] - 67s 702us/step - loss: 0.1456 - acc: 0.7817 - val_loss: 0.1440 - val_acc: 0.7833\n",
      "Train on 95000 samples, validate on 10392 samples\n",
      "Epoch 1/30\n",
      "95000/95000 [==============================] - 93s 979us/step - loss: 0.2280 - acc: 0.6356 - val_loss: 0.2257 - val_acc: 0.6369\n",
      "Epoch 2/30\n",
      "95000/95000 [==============================] - 62s 653us/step - loss: 0.2245 - acc: 0.6435 - val_loss: 0.2236 - val_acc: 0.6430\n",
      "Epoch 3/30\n",
      "95000/95000 [==============================] - 59s 620us/step - loss: 0.2226 - acc: 0.6480 - val_loss: 0.2220 - val_acc: 0.6468\n",
      "Epoch 4/30\n",
      "95000/95000 [==============================] - 60s 632us/step - loss: 0.2210 - acc: 0.6512 - val_loss: 0.2224 - val_acc: 0.6481\n",
      "Epoch 5/30\n",
      "95000/95000 [==============================] - 66s 695us/step - loss: 0.2192 - acc: 0.6553 - val_loss: 0.2204 - val_acc: 0.6485\n",
      "Epoch 6/30\n",
      "95000/95000 [==============================] - 91s 954us/step - loss: 0.2174 - acc: 0.6591 - val_loss: 0.2175 - val_acc: 0.6552\n",
      "Epoch 7/30\n",
      "95000/95000 [==============================] - 139s 1ms/step - loss: 0.2163 - acc: 0.6619 - val_loss: 0.2188 - val_acc: 0.6515\n",
      "Epoch 8/30\n",
      "95000/95000 [==============================] - 195s 2ms/step - loss: 0.2148 - acc: 0.6625 - val_loss: 0.2148 - val_acc: 0.6629\n",
      "Epoch 9/30\n",
      "95000/95000 [==============================] - 170s 2ms/step - loss: 0.2126 - acc: 0.6675 - val_loss: 0.2128 - val_acc: 0.6638\n",
      "Epoch 10/30\n",
      "95000/95000 [==============================] - 164s 2ms/step - loss: 0.2115 - acc: 0.6681 - val_loss: 0.2131 - val_acc: 0.6555\n",
      "Epoch 11/30\n",
      "95000/95000 [==============================] - 166s 2ms/step - loss: 0.2094 - acc: 0.6743 - val_loss: 0.2126 - val_acc: 0.6664\n",
      "Epoch 12/30\n",
      "95000/95000 [==============================] - 160s 2ms/step - loss: 0.2063 - acc: 0.6793 - val_loss: 0.2023 - val_acc: 0.6856\n",
      "Epoch 13/30\n",
      "95000/95000 [==============================] - 167s 2ms/step - loss: 0.2019 - acc: 0.6882 - val_loss: 0.2001 - val_acc: 0.6841\n",
      "Epoch 14/30\n",
      "49000/95000 [==============>...............] - ETA: 1:07 - loss: 0.1968 - acc: 0.6944"
     ]
    }
   ],
   "source": [
    "from NiaPy.algorithms.basic import GreyWolfOptimizer\n",
    "for i in range(5):\n",
    "    algorithm = GreyWolfOptimizer(D=30, NP=370, nFES=10, benchmark=LSTM())\n",
    "    best = algorithm.run()\n",
    "    print(-1*best[1])\n",
    "    ans.append(-1*best[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b0e96a6ac391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ans' is not defined"
     ]
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7f7dd29b225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'anss' is not defined"
     ]
    }
   ],
   "source": [
    "anss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 49, 100)           55200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25)                100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 73,296\n",
      "Trainable params: 73,246\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable= False\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 49, 100)           55200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25)                100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 128,496\n",
      "Trainable params: 73,246\n",
      "Non-trainable params: 55,250\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7238 samples, validate on 1810 samples\n",
      "Epoch 1/30\n",
      "7238/7238 [==============================] - 85s 12ms/step - loss: 0.6630 - acc: 0.5793 - val_loss: 0.6752 - val_acc: 0.5773\n",
      "Epoch 2/30\n",
      "7238/7238 [==============================] - 11s 1ms/step - loss: 0.6586 - acc: 0.5863 - val_loss: 0.6775 - val_acc: 0.5696\n",
      "Epoch 3/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6602 - acc: 0.5811 - val_loss: 0.6782 - val_acc: 0.5729\n",
      "Epoch 4/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6616 - acc: 0.5858 - val_loss: 0.6793 - val_acc: 0.5702\n",
      "Epoch 5/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6588 - acc: 0.5814 - val_loss: 0.6816 - val_acc: 0.5724\n",
      "Epoch 6/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6587 - acc: 0.5836 - val_loss: 0.6819 - val_acc: 0.5635\n",
      "Epoch 7/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6574 - acc: 0.5912 - val_loss: 0.6822 - val_acc: 0.5724\n",
      "Epoch 8/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6576 - acc: 0.5847 - val_loss: 0.6860 - val_acc: 0.5431\n",
      "Epoch 9/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6736 - acc: 0.5667 - val_loss: 0.6795 - val_acc: 0.5481\n",
      "Epoch 10/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6771 - acc: 0.5521 - val_loss: 0.6805 - val_acc: 0.5508\n",
      "Epoch 11/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6704 - acc: 0.5645 - val_loss: 0.6839 - val_acc: 0.5613\n",
      "Epoch 12/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6648 - acc: 0.5783 - val_loss: 0.6839 - val_acc: 0.5613\n",
      "Epoch 13/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6631 - acc: 0.5775 - val_loss: 0.6823 - val_acc: 0.5497\n",
      "Epoch 14/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6738 - acc: 0.5575 - val_loss: 0.8514 - val_acc: 0.5663\n",
      "Epoch 15/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6787 - acc: 0.5503 - val_loss: 0.6870 - val_acc: 0.5066\n",
      "Epoch 16/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6763 - acc: 0.5522 - val_loss: 0.6874 - val_acc: 0.5166\n",
      "Epoch 17/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6754 - acc: 0.5575 - val_loss: 0.7047 - val_acc: 0.4696\n",
      "Epoch 18/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6771 - acc: 0.5564 - val_loss: 0.6827 - val_acc: 0.5530\n",
      "Epoch 19/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6776 - acc: 0.5542 - val_loss: 0.6794 - val_acc: 0.5646\n",
      "Epoch 20/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6733 - acc: 0.5651 - val_loss: 0.6783 - val_acc: 0.5608\n",
      "Epoch 21/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6719 - acc: 0.5653 - val_loss: 0.6802 - val_acc: 0.5586\n",
      "Epoch 22/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6715 - acc: 0.5678 - val_loss: 0.6792 - val_acc: 0.5492\n",
      "Epoch 23/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6719 - acc: 0.5665 - val_loss: 0.6794 - val_acc: 0.5558\n",
      "Epoch 24/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6725 - acc: 0.5622 - val_loss: 0.6815 - val_acc: 0.5525\n",
      "Epoch 25/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6723 - acc: 0.5642 - val_loss: 0.6771 - val_acc: 0.5646\n",
      "Epoch 26/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6699 - acc: 0.5678 - val_loss: 0.6776 - val_acc: 0.5729\n",
      "Epoch 27/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6704 - acc: 0.5712 - val_loss: 0.6802 - val_acc: 0.5541\n",
      "Epoch 28/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6743 - acc: 0.5604 - val_loss: 0.6769 - val_acc: 0.5729\n",
      "Epoch 29/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6759 - acc: 0.5502 - val_loss: 0.6776 - val_acc: 0.5702\n",
      "Epoch 30/30\n",
      "7238/7238 [==============================] - 12s 2ms/step - loss: 0.6738 - acc: 0.5589 - val_loss: 0.6762 - val_acc: 0.5707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8cdbb1b00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810/1810 [==============================] - 1s 716us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.07182323076449"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test)\n",
    "scores[1]*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
